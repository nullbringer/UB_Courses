{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Based FizzBuzz Function [Software 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fizzbuzz(n):\n",
    "    \n",
    "    # Logic Explanation\n",
    "    if n % 3 == 0 and n % 5 == 0:\n",
    "        return 'FizzBuzz'\n",
    "    elif n % 3 == 0:\n",
    "        return 'Fizz'\n",
    "    elif n % 5 == 0:\n",
    "        return 'Buzz'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Datasets in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInputCSV(start,end,filename):\n",
    "    \n",
    "    # Why list in Python?\n",
    "    # We have a number of data entries which must be fed to machine for training the model so that it can predict\n",
    "    # unseen data as correctly as possible. To hold muliple data entries we are using list.\n",
    "    inputData   = []\n",
    "    outputData  = []\n",
    "    \n",
    "    # Why do we need training Data?\n",
    "    # Just like human, machine must be fed enough data to formulate an understanding. In case of human child, only\n",
    "    # after seeing a fruit multiple times, they can recognize the fruit. Same is true for a machine, a model must be\n",
    "    # supplied multiple data points(i.e. pictures of fruit) to draw conclusion. This is what we call training.\n",
    "    \n",
    "    for i in range(start,end):\n",
    "        inputData.append(i)\n",
    "        outputData.append(fizzbuzz(i))\n",
    "    \n",
    "    # Why Dataframe?\n",
    "    # Pandas Dataframe is a n-dimentional datastructure, an in-memory data storage tool. \n",
    "    # This allows user to do rapid calculations over large amounts of data very quickly.\n",
    "    # Below example demonstrated an important feature in pandas, that all the data columns are labeled. So we do not\n",
    "    # need to access them by index. This is important, when the data set and dimention is large.\n",
    "    \n",
    "    dataset = {}\n",
    "    dataset[\"input\"]  = inputData\n",
    "    dataset[\"label\"] = outputData\n",
    "    \n",
    "    # Writing to csv\n",
    "    pd.DataFrame(dataset).to_csv(filename)\n",
    "    \n",
    "    print(filename, \"Created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Input and Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(dataset):\n",
    "    \n",
    "    # Why do we have to process?\n",
    "    data   = dataset['input'].values\n",
    "    labels = dataset['label'].values\n",
    "    \n",
    "    processedData  = encodeData(data)\n",
    "    processedLabel = encodeLabel(labels)\n",
    "    \n",
    "    return processedData, processedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeData(data):\n",
    "    \n",
    "    processedData = []\n",
    "    \n",
    "    for dataInstance in data:\n",
    "        \n",
    "        # Why do we have number 10?\n",
    "        processedData.append([dataInstance >> d & 1 for d in range(10)])\n",
    "    \n",
    "    return np.array(processedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def encodeLabel(labels):\n",
    "    \n",
    "    processedLabel = []\n",
    "    \n",
    "    for labelInstance in labels:\n",
    "        if(labelInstance == \"FizzBuzz\"):\n",
    "            # Fizzbuzz\n",
    "            processedLabel.append([3])\n",
    "        elif(labelInstance == \"Fizz\"):\n",
    "            # Fizz\n",
    "            processedLabel.append([1])\n",
    "        elif(labelInstance == \"Buzz\"):\n",
    "            # Buzz\n",
    "            processedLabel.append([2])\n",
    "        else:\n",
    "            # Other\n",
    "            processedLabel.append([0])\n",
    "\n",
    "    return np_utils.to_categorical(np.array(processedLabel),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_size = 10\n",
    "drop_out = 0.2\n",
    "first_dense_layer_nodes  = 256\n",
    "second_dense_layer_nodes = 4\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    # Why do we need a model?\n",
    "    # -> In abstruct sense, a model tries to imitate human nervous system. It tries to emulate the way human\n",
    "    # understand it's surrounding. It takes information given to it, processes it, tries to add layers of\n",
    "    # understanding with time and come up with more accurate prediction system.\n",
    "    \n",
    "    # Why use Dense layer and then activation?\n",
    "    # -> A dense layer is just a regular layer of neurons in a neural network. \n",
    "    # Each neuron recieves input from all the neurons in the previous layer, thus densely connected.\n",
    "    # -> Activation is used to add non-linearity in the neural network. If we do not add activation after every\n",
    "    # dense layer, then multiple dense practically becomes one single layer of complex linear function.\n",
    "    \n",
    "    # Why use sequential model with layers?\n",
    "    # -> The sequential API allows users to create models layer-by-layer for most problems. It is limited in that,\n",
    "    # it does not allow us create models that share layers or have multiple inputs or outputs. In this model, one\n",
    "    # layer uses the previous layer's output as input and it's output feeds next layer only. This is great model for\n",
    "    # comparatively less complex problems.\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Why dropout?\n",
    "    # -> Dropout is a regularization technique, which aims to reduce the complexity of the model with the goal to \n",
    "    # prevent overfitting. The key idea is to randomly drop units (along with their connections) from the neural \n",
    "    # network during training. This prevents units from co-adapting too much. Another side effect is that \n",
    "    # training will be faster.\n",
    "\n",
    "        \n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    model.add(Activation('softmax'))\n",
    "    # Why Softmax?\n",
    "    # -> Softmax activation is basically the normalized exponential probability of class observations \n",
    "    # represented as neuron activations. The softmax function is often used in the final layer of a neural \n",
    "    # network-based classifier. It is uded because of the ease of differentiation and being in the range 0-1. \n",
    "    # The output of the function is also between 0 and 1 and therefore naturally a suitable choice for \n",
    "    # representing probability\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Why use categorical_crossentropy?\n",
    "    # -> Since the problem we are solving is a multi-classification problem (i.e. 4 expected output classes) we have\n",
    "    # to use categorical_crossentropy as the loss function. \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Creating Training and Testing Datafiles</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.csv Created!\n",
      "testing.csv Created!\n"
     ]
    }
   ],
   "source": [
    "# Create datafiles\n",
    "createInputCSV(101,1001,'training.csv')\n",
    "createInputCSV(1,101,'testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Creating Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               2816      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 3,844\n",
      "Trainable params: 3,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Run Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 675 samples, validate on 225 samples\n",
      "Epoch 1/10000\n",
      "675/675 [==============================] - 0s 655us/step - loss: 1.2606 - acc: 0.4519 - val_loss: 1.1528 - val_acc: 0.5333\n",
      "Epoch 2/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 1.1782 - acc: 0.5333 - val_loss: 1.1475 - val_acc: 0.5333\n",
      "Epoch 3/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.1640 - acc: 0.5333 - val_loss: 1.1449 - val_acc: 0.5333\n",
      "Epoch 4/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.1524 - acc: 0.5289 - val_loss: 1.1453 - val_acc: 0.5333\n",
      "Epoch 5/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 1.1518 - acc: 0.5333 - val_loss: 1.1482 - val_acc: 0.5333\n",
      "Epoch 6/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 1.1535 - acc: 0.5333 - val_loss: 1.1502 - val_acc: 0.5333\n",
      "Epoch 7/10000\n",
      "675/675 [==============================] - 0s 123us/step - loss: 1.1522 - acc: 0.5333 - val_loss: 1.1513 - val_acc: 0.5333\n",
      "Epoch 8/10000\n",
      "675/675 [==============================] - 0s 139us/step - loss: 1.1540 - acc: 0.5333 - val_loss: 1.1463 - val_acc: 0.5333\n",
      "Epoch 9/10000\n",
      "675/675 [==============================] - 0s 100us/step - loss: 1.1419 - acc: 0.5333 - val_loss: 1.1483 - val_acc: 0.5333\n",
      "Epoch 10/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 1.1425 - acc: 0.5333 - val_loss: 1.1452 - val_acc: 0.5333\n",
      "Epoch 11/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.1420 - acc: 0.5333 - val_loss: 1.1482 - val_acc: 0.5333\n",
      "Epoch 12/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 1.1407 - acc: 0.5333 - val_loss: 1.1472 - val_acc: 0.5333\n",
      "Epoch 13/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 1.1449 - acc: 0.5319 - val_loss: 1.1499 - val_acc: 0.5333\n",
      "Epoch 14/10000\n",
      "675/675 [==============================] - 0s 133us/step - loss: 1.1465 - acc: 0.5333 - val_loss: 1.1475 - val_acc: 0.5333\n",
      "Epoch 15/10000\n",
      "675/675 [==============================] - 0s 127us/step - loss: 1.1348 - acc: 0.5333 - val_loss: 1.1457 - val_acc: 0.5333\n",
      "Epoch 16/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 1.1376 - acc: 0.5333 - val_loss: 1.1454 - val_acc: 0.5333\n",
      "Epoch 17/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 1.1325 - acc: 0.5333 - val_loss: 1.1492 - val_acc: 0.5333\n",
      "Epoch 18/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.1357 - acc: 0.5333 - val_loss: 1.1477 - val_acc: 0.5333\n",
      "Epoch 19/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 1.1360 - acc: 0.5333 - val_loss: 1.1485 - val_acc: 0.5333\n",
      "Epoch 20/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.1276 - acc: 0.5333 - val_loss: 1.1441 - val_acc: 0.5333\n",
      "Epoch 21/10000\n",
      "675/675 [==============================] - 0s 125us/step - loss: 1.1371 - acc: 0.5333 - val_loss: 1.1484 - val_acc: 0.5333\n",
      "Epoch 22/10000\n",
      "675/675 [==============================] - 0s 110us/step - loss: 1.1311 - acc: 0.5333 - val_loss: 1.1460 - val_acc: 0.5333\n",
      "Epoch 23/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.1317 - acc: 0.5333 - val_loss: 1.1430 - val_acc: 0.5333\n",
      "Epoch 24/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 1.1228 - acc: 0.5333 - val_loss: 1.1449 - val_acc: 0.5333\n",
      "Epoch 25/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 1.1264 - acc: 0.5333 - val_loss: 1.1481 - val_acc: 0.5333\n",
      "Epoch 26/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 1.1264 - acc: 0.5333 - val_loss: 1.1495 - val_acc: 0.5333\n",
      "Epoch 27/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.1253 - acc: 0.5333 - val_loss: 1.1431 - val_acc: 0.5333\n",
      "Epoch 28/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.1247 - acc: 0.5333 - val_loss: 1.1442 - val_acc: 0.5333\n",
      "Epoch 29/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.1243 - acc: 0.5319 - val_loss: 1.1443 - val_acc: 0.5333\n",
      "Epoch 30/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.1209 - acc: 0.5333 - val_loss: 1.1408 - val_acc: 0.5333\n",
      "Epoch 31/10000\n",
      "675/675 [==============================] - 0s 100us/step - loss: 1.1252 - acc: 0.5333 - val_loss: 1.1443 - val_acc: 0.5333\n",
      "Epoch 32/10000\n",
      "675/675 [==============================] - 0s 98us/step - loss: 1.1145 - acc: 0.5333 - val_loss: 1.1437 - val_acc: 0.5333\n",
      "Epoch 33/10000\n",
      "675/675 [==============================] - 0s 100us/step - loss: 1.1218 - acc: 0.5333 - val_loss: 1.1448 - val_acc: 0.5333\n",
      "Epoch 34/10000\n",
      "675/675 [==============================] - 0s 99us/step - loss: 1.1124 - acc: 0.5333 - val_loss: 1.1448 - val_acc: 0.5333\n",
      "Epoch 35/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 1.1122 - acc: 0.5333 - val_loss: 1.1505 - val_acc: 0.5333\n",
      "Epoch 36/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 1.1128 - acc: 0.5333 - val_loss: 1.1439 - val_acc: 0.5333\n",
      "Epoch 37/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 1.1131 - acc: 0.5333 - val_loss: 1.1391 - val_acc: 0.5333\n",
      "Epoch 38/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.1089 - acc: 0.5348 - val_loss: 1.1406 - val_acc: 0.5333\n",
      "Epoch 39/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.1092 - acc: 0.5348 - val_loss: 1.1386 - val_acc: 0.5333\n",
      "Epoch 40/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.1063 - acc: 0.5333 - val_loss: 1.1391 - val_acc: 0.5333\n",
      "Epoch 41/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.1102 - acc: 0.5333 - val_loss: 1.1390 - val_acc: 0.5333\n",
      "Epoch 42/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 1.1060 - acc: 0.5333 - val_loss: 1.1422 - val_acc: 0.5333\n",
      "Epoch 43/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.1016 - acc: 0.5333 - val_loss: 1.1372 - val_acc: 0.5333\n",
      "Epoch 44/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 1.1067 - acc: 0.5333 - val_loss: 1.1355 - val_acc: 0.5333\n",
      "Epoch 45/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.1041 - acc: 0.5348 - val_loss: 1.1423 - val_acc: 0.5333\n",
      "Epoch 46/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0965 - acc: 0.5333 - val_loss: 1.1391 - val_acc: 0.5333\n",
      "Epoch 47/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0977 - acc: 0.5333 - val_loss: 1.1383 - val_acc: 0.5333\n",
      "Epoch 48/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0909 - acc: 0.5393 - val_loss: 1.1343 - val_acc: 0.5333\n",
      "Epoch 49/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.0936 - acc: 0.5333 - val_loss: 1.1321 - val_acc: 0.5333\n",
      "Epoch 50/10000\n",
      "675/675 [==============================] - 0s 94us/step - loss: 1.0907 - acc: 0.5333 - val_loss: 1.1300 - val_acc: 0.5333\n",
      "Epoch 51/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 1.0889 - acc: 0.5348 - val_loss: 1.1341 - val_acc: 0.5333\n",
      "Epoch 52/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0867 - acc: 0.5363 - val_loss: 1.1345 - val_acc: 0.5333\n",
      "Epoch 53/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 1.0869 - acc: 0.5333 - val_loss: 1.1328 - val_acc: 0.5378\n",
      "Epoch 54/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.0859 - acc: 0.5348 - val_loss: 1.1334 - val_acc: 0.5422\n",
      "Epoch 55/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0833 - acc: 0.5304 - val_loss: 1.1272 - val_acc: 0.5333\n",
      "Epoch 56/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0846 - acc: 0.5393 - val_loss: 1.1279 - val_acc: 0.5333\n",
      "Epoch 57/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 1.0831 - acc: 0.5348 - val_loss: 1.1283 - val_acc: 0.5378\n",
      "Epoch 58/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0807 - acc: 0.5363 - val_loss: 1.1305 - val_acc: 0.5333\n",
      "Epoch 59/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 1.0740 - acc: 0.5348 - val_loss: 1.1270 - val_acc: 0.5333\n",
      "Epoch 60/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 79us/step - loss: 1.0778 - acc: 0.5363 - val_loss: 1.1282 - val_acc: 0.5333\n",
      "Epoch 61/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 1.0708 - acc: 0.5348 - val_loss: 1.1230 - val_acc: 0.5333\n",
      "Epoch 62/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 1.0668 - acc: 0.5363 - val_loss: 1.1229 - val_acc: 0.5378\n",
      "Epoch 63/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0653 - acc: 0.5348 - val_loss: 1.1192 - val_acc: 0.5333\n",
      "Epoch 64/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 1.0674 - acc: 0.5363 - val_loss: 1.1195 - val_acc: 0.5333\n",
      "Epoch 65/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 1.0611 - acc: 0.5319 - val_loss: 1.1184 - val_acc: 0.5378\n",
      "Epoch 66/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 1.0616 - acc: 0.5378 - val_loss: 1.1218 - val_acc: 0.5422\n",
      "Epoch 67/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0585 - acc: 0.5422 - val_loss: 1.1211 - val_acc: 0.5422\n",
      "Epoch 68/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0564 - acc: 0.5437 - val_loss: 1.1175 - val_acc: 0.5378\n",
      "Epoch 69/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0548 - acc: 0.5348 - val_loss: 1.1205 - val_acc: 0.5422\n",
      "Epoch 70/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0538 - acc: 0.5393 - val_loss: 1.1236 - val_acc: 0.5378\n",
      "Epoch 71/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0488 - acc: 0.5437 - val_loss: 1.1155 - val_acc: 0.5422\n",
      "Epoch 72/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 1.0514 - acc: 0.5348 - val_loss: 1.1152 - val_acc: 0.5333\n",
      "Epoch 73/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0428 - acc: 0.5407 - val_loss: 1.1120 - val_acc: 0.5333\n",
      "Epoch 74/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0408 - acc: 0.5363 - val_loss: 1.1107 - val_acc: 0.5422\n",
      "Epoch 75/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0527 - acc: 0.5422 - val_loss: 1.1092 - val_acc: 0.5333\n",
      "Epoch 76/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0436 - acc: 0.5437 - val_loss: 1.1134 - val_acc: 0.5467\n",
      "Epoch 77/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0395 - acc: 0.5422 - val_loss: 1.1082 - val_acc: 0.5422\n",
      "Epoch 78/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0335 - acc: 0.5481 - val_loss: 1.1090 - val_acc: 0.5378\n",
      "Epoch 79/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0371 - acc: 0.5393 - val_loss: 1.1093 - val_acc: 0.5422\n",
      "Epoch 80/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0336 - acc: 0.5422 - val_loss: 1.1117 - val_acc: 0.5467\n",
      "Epoch 81/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 1.0321 - acc: 0.5496 - val_loss: 1.1122 - val_acc: 0.5422\n",
      "Epoch 82/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0269 - acc: 0.5481 - val_loss: 1.1055 - val_acc: 0.5422\n",
      "Epoch 83/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0282 - acc: 0.5481 - val_loss: 1.1118 - val_acc: 0.5378\n",
      "Epoch 84/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0266 - acc: 0.5585 - val_loss: 1.1040 - val_acc: 0.5378\n",
      "Epoch 85/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.0246 - acc: 0.5422 - val_loss: 1.1037 - val_acc: 0.5422\n",
      "Epoch 86/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.0210 - acc: 0.5526 - val_loss: 1.1038 - val_acc: 0.5511\n",
      "Epoch 87/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0216 - acc: 0.5481 - val_loss: 1.1020 - val_acc: 0.5511\n",
      "Epoch 88/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0158 - acc: 0.5511 - val_loss: 1.1052 - val_acc: 0.5378\n",
      "Epoch 89/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 1.0141 - acc: 0.5600 - val_loss: 1.0990 - val_acc: 0.5556\n",
      "Epoch 90/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 1.0078 - acc: 0.5541 - val_loss: 1.1024 - val_acc: 0.5378\n",
      "Epoch 91/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 1.0108 - acc: 0.5689 - val_loss: 1.1010 - val_acc: 0.5556\n",
      "Epoch 92/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0072 - acc: 0.5526 - val_loss: 1.0948 - val_acc: 0.5511\n",
      "Epoch 93/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0026 - acc: 0.5585 - val_loss: 1.0946 - val_acc: 0.5600\n",
      "Epoch 94/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0071 - acc: 0.5615 - val_loss: 1.0932 - val_acc: 0.5556\n",
      "Epoch 95/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9957 - acc: 0.5541 - val_loss: 1.0946 - val_acc: 0.5511\n",
      "Epoch 96/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 1.0049 - acc: 0.5526 - val_loss: 1.0914 - val_acc: 0.5422\n",
      "Epoch 97/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9957 - acc: 0.5585 - val_loss: 1.0938 - val_acc: 0.5600\n",
      "Epoch 98/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9986 - acc: 0.5689 - val_loss: 1.0922 - val_acc: 0.5600\n",
      "Epoch 99/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 1.0009 - acc: 0.5600 - val_loss: 1.0871 - val_acc: 0.5467\n",
      "Epoch 100/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.9977 - acc: 0.5600 - val_loss: 1.0876 - val_acc: 0.5600\n",
      "Epoch 101/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9813 - acc: 0.5719 - val_loss: 1.0864 - val_acc: 0.5511\n",
      "Epoch 102/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9861 - acc: 0.5644 - val_loss: 1.0906 - val_acc: 0.5467\n",
      "Epoch 103/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.9749 - acc: 0.5674 - val_loss: 1.0932 - val_acc: 0.5422\n",
      "Epoch 104/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9896 - acc: 0.5511 - val_loss: 1.0861 - val_acc: 0.5556\n",
      "Epoch 105/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9792 - acc: 0.5630 - val_loss: 1.0860 - val_acc: 0.5511\n",
      "Epoch 106/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9775 - acc: 0.5689 - val_loss: 1.0841 - val_acc: 0.5511\n",
      "Epoch 107/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9783 - acc: 0.5689 - val_loss: 1.0824 - val_acc: 0.5511\n",
      "Epoch 108/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.9673 - acc: 0.5674 - val_loss: 1.0794 - val_acc: 0.5644\n",
      "Epoch 109/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9719 - acc: 0.5793 - val_loss: 1.0775 - val_acc: 0.5600\n",
      "Epoch 110/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.9690 - acc: 0.5748 - val_loss: 1.0839 - val_acc: 0.5467\n",
      "Epoch 111/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9641 - acc: 0.5837 - val_loss: 1.0776 - val_acc: 0.5556\n",
      "Epoch 112/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9616 - acc: 0.5822 - val_loss: 1.0749 - val_acc: 0.5467\n",
      "Epoch 113/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9623 - acc: 0.5748 - val_loss: 1.0765 - val_acc: 0.5556\n",
      "Epoch 114/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9658 - acc: 0.5615 - val_loss: 1.0709 - val_acc: 0.5644\n",
      "Epoch 115/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9548 - acc: 0.5704 - val_loss: 1.0710 - val_acc: 0.5689\n",
      "Epoch 116/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9564 - acc: 0.5778 - val_loss: 1.0775 - val_acc: 0.5600\n",
      "Epoch 117/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9521 - acc: 0.5748 - val_loss: 1.0751 - val_acc: 0.5511\n",
      "Epoch 118/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9522 - acc: 0.5822 - val_loss: 1.0707 - val_acc: 0.5511\n",
      "Epoch 119/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.9546 - acc: 0.5659 - val_loss: 1.0692 - val_acc: 0.5644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.9509 - acc: 0.5822 - val_loss: 1.0661 - val_acc: 0.5511\n",
      "Epoch 121/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9514 - acc: 0.5733 - val_loss: 1.0651 - val_acc: 0.5644\n",
      "Epoch 122/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9422 - acc: 0.5867 - val_loss: 1.0631 - val_acc: 0.5600\n",
      "Epoch 123/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9375 - acc: 0.5733 - val_loss: 1.0693 - val_acc: 0.5511\n",
      "Epoch 124/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9334 - acc: 0.5926 - val_loss: 1.0727 - val_acc: 0.5511\n",
      "Epoch 125/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9426 - acc: 0.5985 - val_loss: 1.0671 - val_acc: 0.5600\n",
      "Epoch 126/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.9245 - acc: 0.5941 - val_loss: 1.0643 - val_acc: 0.5556\n",
      "Epoch 127/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9312 - acc: 0.5896 - val_loss: 1.0637 - val_acc: 0.5600\n",
      "Epoch 128/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9236 - acc: 0.6000 - val_loss: 1.0572 - val_acc: 0.5733\n",
      "Epoch 129/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.9105 - acc: 0.5837 - val_loss: 1.0596 - val_acc: 0.5644\n",
      "Epoch 130/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.9261 - acc: 0.5896 - val_loss: 1.0615 - val_acc: 0.5467\n",
      "Epoch 131/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9215 - acc: 0.6044 - val_loss: 1.0579 - val_acc: 0.5556\n",
      "Epoch 132/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9260 - acc: 0.5911 - val_loss: 1.0559 - val_acc: 0.5556\n",
      "Epoch 133/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.9222 - acc: 0.5911 - val_loss: 1.0557 - val_acc: 0.5600\n",
      "Epoch 134/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9182 - acc: 0.6148 - val_loss: 1.0529 - val_acc: 0.5511\n",
      "Epoch 135/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9177 - acc: 0.5926 - val_loss: 1.0527 - val_acc: 0.5644\n",
      "Epoch 136/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9175 - acc: 0.5704 - val_loss: 1.0490 - val_acc: 0.5511\n",
      "Epoch 137/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.9123 - acc: 0.5867 - val_loss: 1.0478 - val_acc: 0.5600\n",
      "Epoch 138/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.9093 - acc: 0.6000 - val_loss: 1.0482 - val_acc: 0.5644\n",
      "Epoch 139/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.9068 - acc: 0.5896 - val_loss: 1.0480 - val_acc: 0.5600\n",
      "Epoch 140/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8965 - acc: 0.5852 - val_loss: 1.0460 - val_acc: 0.5600\n",
      "Epoch 141/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.9039 - acc: 0.5970 - val_loss: 1.0428 - val_acc: 0.5644\n",
      "Epoch 142/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.9080 - acc: 0.5822 - val_loss: 1.0428 - val_acc: 0.5644\n",
      "Epoch 143/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.8913 - acc: 0.5956 - val_loss: 1.0393 - val_acc: 0.5600\n",
      "Epoch 144/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.8985 - acc: 0.6119 - val_loss: 1.0493 - val_acc: 0.5556\n",
      "Epoch 145/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.8929 - acc: 0.6148 - val_loss: 1.0465 - val_acc: 0.5644\n",
      "Epoch 146/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8977 - acc: 0.6044 - val_loss: 1.0415 - val_acc: 0.5600\n",
      "Epoch 147/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.8888 - acc: 0.6089 - val_loss: 1.0443 - val_acc: 0.5600\n",
      "Epoch 148/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.8925 - acc: 0.5985 - val_loss: 1.0391 - val_acc: 0.5600\n",
      "Epoch 149/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8774 - acc: 0.6089 - val_loss: 1.0425 - val_acc: 0.5733\n",
      "Epoch 150/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.8826 - acc: 0.6281 - val_loss: 1.0350 - val_acc: 0.5689\n",
      "Epoch 151/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.8651 - acc: 0.6207 - val_loss: 1.0348 - val_acc: 0.5600\n",
      "Epoch 152/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.8714 - acc: 0.6104 - val_loss: 1.0333 - val_acc: 0.5600\n",
      "Epoch 153/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8773 - acc: 0.6133 - val_loss: 1.0372 - val_acc: 0.5689\n",
      "Epoch 154/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8843 - acc: 0.6222 - val_loss: 1.0355 - val_acc: 0.5733\n",
      "Epoch 155/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.8713 - acc: 0.6133 - val_loss: 1.0340 - val_acc: 0.5644\n",
      "Epoch 156/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8733 - acc: 0.6178 - val_loss: 1.0262 - val_acc: 0.5644\n",
      "Epoch 157/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.8814 - acc: 0.6104 - val_loss: 1.0245 - val_acc: 0.5689\n",
      "Epoch 158/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8719 - acc: 0.6030 - val_loss: 1.0247 - val_acc: 0.5689\n",
      "Epoch 159/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8703 - acc: 0.6133 - val_loss: 1.0279 - val_acc: 0.5644\n",
      "Epoch 160/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.8567 - acc: 0.6193 - val_loss: 1.0216 - val_acc: 0.5644\n",
      "Epoch 161/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8667 - acc: 0.6133 - val_loss: 1.0197 - val_acc: 0.5644\n",
      "Epoch 162/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8491 - acc: 0.6207 - val_loss: 1.0203 - val_acc: 0.5689\n",
      "Epoch 163/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8518 - acc: 0.6119 - val_loss: 1.0198 - val_acc: 0.5733\n",
      "Epoch 164/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8569 - acc: 0.6193 - val_loss: 1.0340 - val_acc: 0.5733\n",
      "Epoch 165/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8617 - acc: 0.6444 - val_loss: 1.0193 - val_acc: 0.5733\n",
      "Epoch 166/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8505 - acc: 0.6370 - val_loss: 1.0149 - val_acc: 0.5733\n",
      "Epoch 167/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.8553 - acc: 0.6385 - val_loss: 1.0142 - val_acc: 0.5644\n",
      "Epoch 168/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8518 - acc: 0.6356 - val_loss: 1.0189 - val_acc: 0.5644\n",
      "Epoch 169/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8317 - acc: 0.6326 - val_loss: 1.0169 - val_acc: 0.5822\n",
      "Epoch 170/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 0.8477 - acc: 0.6474 - val_loss: 1.0132 - val_acc: 0.5778\n",
      "Epoch 171/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8278 - acc: 0.6400 - val_loss: 1.0075 - val_acc: 0.5689\n",
      "Epoch 172/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8346 - acc: 0.6370 - val_loss: 1.0136 - val_acc: 0.5733\n",
      "Epoch 173/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.8348 - acc: 0.6341 - val_loss: 1.0027 - val_acc: 0.5822\n",
      "Epoch 174/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.8267 - acc: 0.6578 - val_loss: 1.0009 - val_acc: 0.5733\n",
      "Epoch 175/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 0.8324 - acc: 0.6444 - val_loss: 1.0025 - val_acc: 0.5867\n",
      "Epoch 176/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.8283 - acc: 0.6326 - val_loss: 1.0146 - val_acc: 0.5778\n",
      "Epoch 177/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8153 - acc: 0.6681 - val_loss: 1.0005 - val_acc: 0.5867\n",
      "Epoch 178/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.8232 - acc: 0.6504 - val_loss: 1.0041 - val_acc: 0.5822\n",
      "Epoch 179/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 82us/step - loss: 0.8231 - acc: 0.6593 - val_loss: 0.9963 - val_acc: 0.5867\n",
      "Epoch 180/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8066 - acc: 0.6696 - val_loss: 0.9924 - val_acc: 0.5867\n",
      "Epoch 181/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.8120 - acc: 0.6637 - val_loss: 1.0023 - val_acc: 0.5956\n",
      "Epoch 182/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.8179 - acc: 0.6756 - val_loss: 0.9962 - val_acc: 0.5778\n",
      "Epoch 183/10000\n",
      "675/675 [==============================] - 0s 98us/step - loss: 0.8150 - acc: 0.6696 - val_loss: 0.9890 - val_acc: 0.5733\n",
      "Epoch 184/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.8124 - acc: 0.6385 - val_loss: 0.9880 - val_acc: 0.5733\n",
      "Epoch 185/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.8019 - acc: 0.6696 - val_loss: 0.9865 - val_acc: 0.5867\n",
      "Epoch 186/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 0.8049 - acc: 0.6622 - val_loss: 0.9852 - val_acc: 0.5733\n",
      "Epoch 187/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.8112 - acc: 0.6548 - val_loss: 0.9866 - val_acc: 0.5822\n",
      "Epoch 188/10000\n",
      "675/675 [==============================] - 0s 158us/step - loss: 0.7972 - acc: 0.6696 - val_loss: 0.9815 - val_acc: 0.6044\n",
      "Epoch 189/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.7965 - acc: 0.6726 - val_loss: 0.9797 - val_acc: 0.6000\n",
      "Epoch 190/10000\n",
      "675/675 [==============================] - 0s 159us/step - loss: 0.7879 - acc: 0.6874 - val_loss: 0.9797 - val_acc: 0.5822\n",
      "Epoch 191/10000\n",
      "675/675 [==============================] - 0s 140us/step - loss: 0.7977 - acc: 0.6711 - val_loss: 0.9800 - val_acc: 0.5911\n",
      "Epoch 192/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 0.7942 - acc: 0.6726 - val_loss: 0.9730 - val_acc: 0.5822\n",
      "Epoch 193/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.7955 - acc: 0.6800 - val_loss: 0.9704 - val_acc: 0.5911\n",
      "Epoch 194/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.7926 - acc: 0.6711 - val_loss: 0.9726 - val_acc: 0.5911\n",
      "Epoch 195/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.7944 - acc: 0.6667 - val_loss: 0.9725 - val_acc: 0.6000\n",
      "Epoch 196/10000\n",
      "675/675 [==============================] - 0s 165us/step - loss: 0.7904 - acc: 0.6681 - val_loss: 0.9722 - val_acc: 0.5956\n",
      "Epoch 197/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.7959 - acc: 0.6844 - val_loss: 0.9648 - val_acc: 0.5911\n",
      "Epoch 198/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.7795 - acc: 0.6800 - val_loss: 0.9686 - val_acc: 0.5956\n",
      "Epoch 199/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.7903 - acc: 0.6919 - val_loss: 0.9639 - val_acc: 0.5911\n",
      "Epoch 200/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.7790 - acc: 0.6889 - val_loss: 0.9623 - val_acc: 0.5956\n",
      "Epoch 201/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.7766 - acc: 0.6770 - val_loss: 0.9758 - val_acc: 0.6000\n",
      "Epoch 202/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.7775 - acc: 0.6844 - val_loss: 0.9679 - val_acc: 0.6000\n",
      "Epoch 203/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.7767 - acc: 0.6830 - val_loss: 0.9585 - val_acc: 0.6000\n",
      "Epoch 204/10000\n",
      "675/675 [==============================] - 0s 166us/step - loss: 0.7664 - acc: 0.7141 - val_loss: 0.9657 - val_acc: 0.5733\n",
      "Epoch 205/10000\n",
      "675/675 [==============================] - 0s 141us/step - loss: 0.7637 - acc: 0.6904 - val_loss: 0.9565 - val_acc: 0.6133\n",
      "Epoch 206/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.7675 - acc: 0.6859 - val_loss: 0.9601 - val_acc: 0.6133\n",
      "Epoch 207/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.7813 - acc: 0.6741 - val_loss: 0.9484 - val_acc: 0.6178\n",
      "Epoch 208/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.7677 - acc: 0.6756 - val_loss: 0.9513 - val_acc: 0.6044\n",
      "Epoch 209/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.7520 - acc: 0.7052 - val_loss: 0.9500 - val_acc: 0.6133\n",
      "Epoch 210/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 0.7350 - acc: 0.6948 - val_loss: 0.9508 - val_acc: 0.5778\n",
      "Epoch 211/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 0.7703 - acc: 0.6844 - val_loss: 0.9460 - val_acc: 0.6089\n",
      "Epoch 212/10000\n",
      "675/675 [==============================] - 0s 101us/step - loss: 0.7669 - acc: 0.6815 - val_loss: 0.9443 - val_acc: 0.6089\n",
      "Epoch 213/10000\n",
      "675/675 [==============================] - 0s 94us/step - loss: 0.7431 - acc: 0.7156 - val_loss: 0.9488 - val_acc: 0.6133\n",
      "Epoch 214/10000\n",
      "675/675 [==============================] - 0s 132us/step - loss: 0.7608 - acc: 0.7067 - val_loss: 0.9508 - val_acc: 0.6000\n",
      "Epoch 215/10000\n",
      "675/675 [==============================] - 0s 155us/step - loss: 0.7578 - acc: 0.6889 - val_loss: 0.9521 - val_acc: 0.6267\n",
      "Epoch 216/10000\n",
      "675/675 [==============================] - 0s 164us/step - loss: 0.7536 - acc: 0.7141 - val_loss: 0.9392 - val_acc: 0.6178\n",
      "Epoch 217/10000\n",
      "675/675 [==============================] - 0s 102us/step - loss: 0.7436 - acc: 0.6978 - val_loss: 0.9416 - val_acc: 0.6222\n",
      "Epoch 218/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.7695 - acc: 0.6844 - val_loss: 0.9433 - val_acc: 0.6222\n",
      "Epoch 219/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.7516 - acc: 0.7185 - val_loss: 0.9387 - val_acc: 0.6000\n",
      "Epoch 220/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.7372 - acc: 0.7215 - val_loss: 0.9318 - val_acc: 0.6044\n",
      "Epoch 221/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.7398 - acc: 0.6874 - val_loss: 0.9345 - val_acc: 0.6133\n",
      "Epoch 222/10000\n",
      "675/675 [==============================] - 0s 118us/step - loss: 0.7412 - acc: 0.7274 - val_loss: 0.9306 - val_acc: 0.6267\n",
      "Epoch 223/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.7301 - acc: 0.7111 - val_loss: 0.9301 - val_acc: 0.6222\n",
      "Epoch 224/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.7437 - acc: 0.7096 - val_loss: 0.9324 - val_acc: 0.5867\n",
      "Epoch 225/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.7289 - acc: 0.7111 - val_loss: 0.9314 - val_acc: 0.6178\n",
      "Epoch 226/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.7162 - acc: 0.7244 - val_loss: 0.9320 - val_acc: 0.5911\n",
      "Epoch 227/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.7199 - acc: 0.7141 - val_loss: 0.9300 - val_acc: 0.6089\n",
      "Epoch 228/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.7301 - acc: 0.7215 - val_loss: 0.9277 - val_acc: 0.6178\n",
      "Epoch 229/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.7171 - acc: 0.7244 - val_loss: 0.9210 - val_acc: 0.6311\n",
      "Epoch 230/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.7297 - acc: 0.7081 - val_loss: 0.9172 - val_acc: 0.6356\n",
      "Epoch 231/10000\n",
      "675/675 [==============================] - 0s 153us/step - loss: 0.7282 - acc: 0.7363 - val_loss: 0.9197 - val_acc: 0.6311\n",
      "Epoch 232/10000\n",
      "675/675 [==============================] - 0s 167us/step - loss: 0.7111 - acc: 0.7363 - val_loss: 0.9153 - val_acc: 0.6267\n",
      "Epoch 233/10000\n",
      "675/675 [==============================] - 0s 116us/step - loss: 0.7383 - acc: 0.7185 - val_loss: 0.9170 - val_acc: 0.6222\n",
      "Epoch 234/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.7369 - acc: 0.7037 - val_loss: 0.9116 - val_acc: 0.6222\n",
      "Epoch 235/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.7148 - acc: 0.7289 - val_loss: 0.9172 - val_acc: 0.6400\n",
      "Epoch 236/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.7091 - acc: 0.7230 - val_loss: 0.9104 - val_acc: 0.6222\n",
      "Epoch 237/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.7138 - acc: 0.7304 - val_loss: 0.9131 - val_acc: 0.6356\n",
      "Epoch 238/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 87us/step - loss: 0.7125 - acc: 0.7156 - val_loss: 0.9066 - val_acc: 0.6222\n",
      "Epoch 239/10000\n",
      "675/675 [==============================] - 0s 148us/step - loss: 0.7147 - acc: 0.7244 - val_loss: 0.9061 - val_acc: 0.6356\n",
      "Epoch 240/10000\n",
      "675/675 [==============================] - 0s 141us/step - loss: 0.6969 - acc: 0.7304 - val_loss: 0.9058 - val_acc: 0.6311\n",
      "Epoch 241/10000\n",
      "675/675 [==============================] - 0s 106us/step - loss: 0.7023 - acc: 0.7393 - val_loss: 0.9094 - val_acc: 0.6356\n",
      "Epoch 242/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.7150 - acc: 0.7230 - val_loss: 0.9070 - val_acc: 0.6444\n",
      "Epoch 243/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6951 - acc: 0.7422 - val_loss: 0.9076 - val_acc: 0.6311\n",
      "Epoch 244/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 0.7060 - acc: 0.7170 - val_loss: 0.8994 - val_acc: 0.6622\n",
      "Epoch 245/10000\n",
      "675/675 [==============================] - 0s 131us/step - loss: 0.7071 - acc: 0.7200 - val_loss: 0.9044 - val_acc: 0.6489\n",
      "Epoch 246/10000\n",
      "675/675 [==============================] - 0s 127us/step - loss: 0.7089 - acc: 0.7348 - val_loss: 0.9037 - val_acc: 0.6489\n",
      "Epoch 247/10000\n",
      "675/675 [==============================] - 0s 140us/step - loss: 0.6909 - acc: 0.7407 - val_loss: 0.8923 - val_acc: 0.6711\n",
      "Epoch 248/10000\n",
      "675/675 [==============================] - 0s 141us/step - loss: 0.6947 - acc: 0.7407 - val_loss: 0.8984 - val_acc: 0.6533\n",
      "Epoch 249/10000\n",
      "675/675 [==============================] - 0s 139us/step - loss: 0.6872 - acc: 0.7348 - val_loss: 0.8973 - val_acc: 0.6311\n",
      "Epoch 250/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.6922 - acc: 0.7452 - val_loss: 0.9038 - val_acc: 0.6533\n",
      "Epoch 251/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.6840 - acc: 0.7467 - val_loss: 0.8971 - val_acc: 0.6178\n",
      "Epoch 252/10000\n",
      "675/675 [==============================] - 0s 143us/step - loss: 0.7117 - acc: 0.7244 - val_loss: 0.8911 - val_acc: 0.6311\n",
      "Epoch 253/10000\n",
      "675/675 [==============================] - 0s 147us/step - loss: 0.6977 - acc: 0.7393 - val_loss: 0.8907 - val_acc: 0.6267\n",
      "Epoch 254/10000\n",
      "675/675 [==============================] - 0s 100us/step - loss: 0.6874 - acc: 0.7378 - val_loss: 0.8951 - val_acc: 0.6578\n",
      "Epoch 255/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.6939 - acc: 0.7496 - val_loss: 0.8926 - val_acc: 0.6578\n",
      "Epoch 256/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.6800 - acc: 0.7600 - val_loss: 0.8887 - val_acc: 0.6356\n",
      "Epoch 257/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6834 - acc: 0.7511 - val_loss: 0.8990 - val_acc: 0.6178\n",
      "Epoch 258/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.6915 - acc: 0.7363 - val_loss: 0.8964 - val_acc: 0.6622\n",
      "Epoch 259/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.6842 - acc: 0.7407 - val_loss: 0.8811 - val_acc: 0.6444\n",
      "Epoch 260/10000\n",
      "675/675 [==============================] - 0s 117us/step - loss: 0.6810 - acc: 0.7570 - val_loss: 0.8809 - val_acc: 0.6533\n",
      "Epoch 261/10000\n",
      "675/675 [==============================] - 0s 119us/step - loss: 0.6711 - acc: 0.7600 - val_loss: 0.8860 - val_acc: 0.6400\n",
      "Epoch 262/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.6761 - acc: 0.7422 - val_loss: 0.8787 - val_acc: 0.6578\n",
      "Epoch 263/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 0.6575 - acc: 0.7526 - val_loss: 0.8766 - val_acc: 0.6489\n",
      "Epoch 264/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.6746 - acc: 0.7526 - val_loss: 0.8711 - val_acc: 0.6489\n",
      "Epoch 265/10000\n",
      "675/675 [==============================] - 0s 120us/step - loss: 0.6729 - acc: 0.7600 - val_loss: 0.8730 - val_acc: 0.6533\n",
      "Epoch 266/10000\n",
      "675/675 [==============================] - 0s 151us/step - loss: 0.6661 - acc: 0.7437 - val_loss: 0.8728 - val_acc: 0.6578\n",
      "Epoch 267/10000\n",
      "675/675 [==============================] - 0s 180us/step - loss: 0.6607 - acc: 0.7733 - val_loss: 0.8816 - val_acc: 0.6489\n",
      "Epoch 268/10000\n",
      "675/675 [==============================] - 0s 150us/step - loss: 0.6764 - acc: 0.7556 - val_loss: 0.8776 - val_acc: 0.6400\n",
      "Epoch 269/10000\n",
      "675/675 [==============================] - 0s 140us/step - loss: 0.6721 - acc: 0.7452 - val_loss: 0.8757 - val_acc: 0.6444\n",
      "Epoch 270/10000\n",
      "675/675 [==============================] - 0s 152us/step - loss: 0.6743 - acc: 0.7333 - val_loss: 0.8743 - val_acc: 0.6711\n",
      "Epoch 271/10000\n",
      "675/675 [==============================] - 0s 182us/step - loss: 0.6852 - acc: 0.7378 - val_loss: 0.8752 - val_acc: 0.6444\n",
      "Epoch 272/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.6697 - acc: 0.7570 - val_loss: 0.8787 - val_acc: 0.6622\n",
      "Epoch 273/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 0.6545 - acc: 0.7630 - val_loss: 0.8667 - val_acc: 0.6578\n",
      "Epoch 274/10000\n",
      "675/675 [==============================] - 0s 122us/step - loss: 0.6411 - acc: 0.7733 - val_loss: 0.8701 - val_acc: 0.6711\n",
      "Epoch 275/10000\n",
      "675/675 [==============================] - 0s 122us/step - loss: 0.6744 - acc: 0.7259 - val_loss: 0.8629 - val_acc: 0.6800\n",
      "Epoch 276/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.6701 - acc: 0.7496 - val_loss: 0.8604 - val_acc: 0.6756\n",
      "Epoch 277/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6411 - acc: 0.7719 - val_loss: 0.8598 - val_acc: 0.6622\n",
      "Epoch 278/10000\n",
      "675/675 [==============================] - 0s 103us/step - loss: 0.6604 - acc: 0.7511 - val_loss: 0.8613 - val_acc: 0.6756\n",
      "Epoch 279/10000\n",
      "675/675 [==============================] - 0s 125us/step - loss: 0.6480 - acc: 0.7659 - val_loss: 0.8673 - val_acc: 0.6711\n",
      "Epoch 280/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.6537 - acc: 0.7674 - val_loss: 0.8577 - val_acc: 0.6756\n",
      "Epoch 281/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 0.6602 - acc: 0.7570 - val_loss: 0.8658 - val_acc: 0.6711\n",
      "Epoch 282/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.6665 - acc: 0.7481 - val_loss: 0.8622 - val_acc: 0.6756\n",
      "Epoch 283/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.6556 - acc: 0.7541 - val_loss: 0.8730 - val_acc: 0.6756\n",
      "Epoch 284/10000\n",
      "675/675 [==============================] - 0s 168us/step - loss: 0.6493 - acc: 0.7556 - val_loss: 0.8545 - val_acc: 0.6578\n",
      "Epoch 285/10000\n",
      "675/675 [==============================] - 0s 169us/step - loss: 0.6470 - acc: 0.7689 - val_loss: 0.8584 - val_acc: 0.6756\n",
      "Epoch 286/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 0.6593 - acc: 0.7526 - val_loss: 0.8515 - val_acc: 0.6756\n",
      "Epoch 287/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.6462 - acc: 0.7570 - val_loss: 0.8634 - val_acc: 0.6533\n",
      "Epoch 288/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6315 - acc: 0.7570 - val_loss: 0.8581 - val_acc: 0.6622\n",
      "Epoch 289/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.6429 - acc: 0.7630 - val_loss: 0.8639 - val_acc: 0.6756\n",
      "Epoch 290/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6360 - acc: 0.7748 - val_loss: 0.8498 - val_acc: 0.6622\n",
      "Epoch 291/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.6425 - acc: 0.7585 - val_loss: 0.8534 - val_acc: 0.6800\n",
      "Epoch 292/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6293 - acc: 0.7941 - val_loss: 0.8468 - val_acc: 0.6622\n",
      "Epoch 293/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.6216 - acc: 0.7719 - val_loss: 0.8519 - val_acc: 0.6667\n",
      "Epoch 294/10000\n",
      "675/675 [==============================] - 0s 138us/step - loss: 0.6256 - acc: 0.7763 - val_loss: 0.8522 - val_acc: 0.6622\n",
      "Epoch 295/10000\n",
      "675/675 [==============================] - 0s 136us/step - loss: 0.6069 - acc: 0.7896 - val_loss: 0.8607 - val_acc: 0.6311\n",
      "Epoch 296/10000\n",
      "675/675 [==============================] - 0s 118us/step - loss: 0.6264 - acc: 0.7748 - val_loss: 0.8474 - val_acc: 0.6711\n",
      "Epoch 297/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 87us/step - loss: 0.6279 - acc: 0.7763 - val_loss: 0.8467 - val_acc: 0.6756\n",
      "Epoch 298/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.6185 - acc: 0.7881 - val_loss: 0.8445 - val_acc: 0.6800\n",
      "Epoch 299/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.6105 - acc: 0.7852 - val_loss: 0.8388 - val_acc: 0.6622\n",
      "Epoch 300/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.6208 - acc: 0.7704 - val_loss: 0.8453 - val_acc: 0.6533\n",
      "Epoch 301/10000\n",
      "675/675 [==============================] - 0s 154us/step - loss: 0.6299 - acc: 0.7541 - val_loss: 0.8358 - val_acc: 0.6711\n",
      "Epoch 302/10000\n",
      "675/675 [==============================] - 0s 150us/step - loss: 0.6204 - acc: 0.7704 - val_loss: 0.8470 - val_acc: 0.6667\n",
      "Epoch 303/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 0.6247 - acc: 0.7644 - val_loss: 0.8336 - val_acc: 0.6756\n",
      "Epoch 304/10000\n",
      "675/675 [==============================] - 0s 125us/step - loss: 0.6269 - acc: 0.7778 - val_loss: 0.8383 - val_acc: 0.6711\n",
      "Epoch 305/10000\n",
      "675/675 [==============================] - 0s 155us/step - loss: 0.6208 - acc: 0.7733 - val_loss: 0.8510 - val_acc: 0.6444\n",
      "Epoch 306/10000\n",
      "675/675 [==============================] - 0s 142us/step - loss: 0.6348 - acc: 0.7615 - val_loss: 0.8546 - val_acc: 0.6311\n",
      "Epoch 307/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6117 - acc: 0.7630 - val_loss: 0.8320 - val_acc: 0.6844\n",
      "Epoch 308/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6255 - acc: 0.7793 - val_loss: 0.8384 - val_acc: 0.6667\n",
      "Epoch 309/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.6100 - acc: 0.8000 - val_loss: 0.8293 - val_acc: 0.7022\n",
      "Epoch 310/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 0.6175 - acc: 0.7867 - val_loss: 0.8318 - val_acc: 0.6711\n",
      "Epoch 311/10000\n",
      "675/675 [==============================] - 0s 99us/step - loss: 0.6012 - acc: 0.7881 - val_loss: 0.8364 - val_acc: 0.6489\n",
      "Epoch 312/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.5994 - acc: 0.7793 - val_loss: 0.8267 - val_acc: 0.6889\n",
      "Epoch 313/10000\n",
      "675/675 [==============================] - 0s 129us/step - loss: 0.6051 - acc: 0.7881 - val_loss: 0.8267 - val_acc: 0.6711\n",
      "Epoch 314/10000\n",
      "675/675 [==============================] - 0s 95us/step - loss: 0.6229 - acc: 0.7748 - val_loss: 0.8323 - val_acc: 0.6578\n",
      "Epoch 315/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.5971 - acc: 0.7837 - val_loss: 0.8309 - val_acc: 0.6756\n",
      "Epoch 316/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.6074 - acc: 0.7867 - val_loss: 0.8227 - val_acc: 0.6711\n",
      "Epoch 317/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.6139 - acc: 0.7881 - val_loss: 0.8181 - val_acc: 0.6889\n",
      "Epoch 318/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.5762 - acc: 0.7970 - val_loss: 0.8176 - val_acc: 0.6889\n",
      "Epoch 319/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.6157 - acc: 0.7615 - val_loss: 0.8169 - val_acc: 0.6933\n",
      "Epoch 320/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 0.5959 - acc: 0.7985 - val_loss: 0.8349 - val_acc: 0.6800\n",
      "Epoch 321/10000\n",
      "675/675 [==============================] - 0s 120us/step - loss: 0.5993 - acc: 0.7956 - val_loss: 0.8141 - val_acc: 0.6978\n",
      "Epoch 322/10000\n",
      "675/675 [==============================] - 0s 131us/step - loss: 0.6046 - acc: 0.7837 - val_loss: 0.8248 - val_acc: 0.6667\n",
      "Epoch 323/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.6075 - acc: 0.7867 - val_loss: 0.8330 - val_acc: 0.6667\n",
      "Epoch 324/10000\n",
      "675/675 [==============================] - 0s 124us/step - loss: 0.5955 - acc: 0.7807 - val_loss: 0.8346 - val_acc: 0.6800\n",
      "Epoch 325/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 0.6078 - acc: 0.7704 - val_loss: 0.8240 - val_acc: 0.6711\n",
      "Epoch 326/10000\n",
      "675/675 [==============================] - 0s 168us/step - loss: 0.5902 - acc: 0.7807 - val_loss: 0.8152 - val_acc: 0.6889\n",
      "Epoch 327/10000\n",
      "675/675 [==============================] - 0s 153us/step - loss: 0.6041 - acc: 0.7793 - val_loss: 0.8301 - val_acc: 0.6844\n",
      "Epoch 328/10000\n",
      "675/675 [==============================] - 0s 140us/step - loss: 0.5947 - acc: 0.7926 - val_loss: 0.8136 - val_acc: 0.6933\n",
      "Epoch 329/10000\n",
      "675/675 [==============================] - 0s 95us/step - loss: 0.5962 - acc: 0.7970 - val_loss: 0.8125 - val_acc: 0.6711\n",
      "Epoch 330/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 0.6047 - acc: 0.7763 - val_loss: 0.8250 - val_acc: 0.6889\n",
      "Epoch 331/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5973 - acc: 0.7881 - val_loss: 0.8172 - val_acc: 0.6844\n",
      "Epoch 332/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5840 - acc: 0.7956 - val_loss: 0.8100 - val_acc: 0.7067\n",
      "Epoch 333/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5878 - acc: 0.8163 - val_loss: 0.8116 - val_acc: 0.6889\n",
      "Epoch 334/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.6030 - acc: 0.7956 - val_loss: 0.8111 - val_acc: 0.6978\n",
      "Epoch 335/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.6014 - acc: 0.7793 - val_loss: 0.8259 - val_acc: 0.6800\n",
      "Epoch 336/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5655 - acc: 0.8207 - val_loss: 0.8151 - val_acc: 0.6978\n",
      "Epoch 337/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.5642 - acc: 0.8000 - val_loss: 0.8016 - val_acc: 0.7111\n",
      "Epoch 338/10000\n",
      "675/675 [==============================] - 0s 107us/step - loss: 0.5731 - acc: 0.7985 - val_loss: 0.8024 - val_acc: 0.7067\n",
      "Epoch 339/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 0.5531 - acc: 0.8311 - val_loss: 0.8063 - val_acc: 0.7022\n",
      "Epoch 340/10000\n",
      "675/675 [==============================] - 0s 108us/step - loss: 0.5736 - acc: 0.8000 - val_loss: 0.7971 - val_acc: 0.7111\n",
      "Epoch 341/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 0.5837 - acc: 0.7881 - val_loss: 0.8141 - val_acc: 0.6756\n",
      "Epoch 342/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 0.5743 - acc: 0.7867 - val_loss: 0.8027 - val_acc: 0.6844\n",
      "Epoch 343/10000\n",
      "675/675 [==============================] - 0s 168us/step - loss: 0.5853 - acc: 0.7867 - val_loss: 0.8067 - val_acc: 0.6978\n",
      "Epoch 344/10000\n",
      "675/675 [==============================] - 0s 120us/step - loss: 0.5823 - acc: 0.7926 - val_loss: 0.8079 - val_acc: 0.6844\n",
      "Epoch 345/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.5614 - acc: 0.8015 - val_loss: 0.8034 - val_acc: 0.6889\n",
      "Epoch 346/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5695 - acc: 0.7985 - val_loss: 0.7952 - val_acc: 0.7067\n",
      "Epoch 347/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5632 - acc: 0.8015 - val_loss: 0.8039 - val_acc: 0.6933\n",
      "Epoch 348/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.5615 - acc: 0.8119 - val_loss: 0.7932 - val_acc: 0.6933\n",
      "Epoch 349/10000\n",
      "675/675 [==============================] - 0s 94us/step - loss: 0.5525 - acc: 0.8030 - val_loss: 0.7886 - val_acc: 0.7111\n",
      "Epoch 350/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.5891 - acc: 0.7881 - val_loss: 0.7982 - val_acc: 0.6978\n",
      "Epoch 351/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5742 - acc: 0.7852 - val_loss: 0.7974 - val_acc: 0.6889\n",
      "Epoch 352/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5657 - acc: 0.7867 - val_loss: 0.8074 - val_acc: 0.7022\n",
      "Epoch 353/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5538 - acc: 0.8163 - val_loss: 0.7929 - val_acc: 0.7111\n",
      "Epoch 354/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5547 - acc: 0.8222 - val_loss: 0.8017 - val_acc: 0.6844\n",
      "Epoch 355/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.5715 - acc: 0.7911 - val_loss: 0.7987 - val_acc: 0.6933\n",
      "Epoch 356/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 78us/step - loss: 0.5540 - acc: 0.8163 - val_loss: 0.8051 - val_acc: 0.6889\n",
      "Epoch 357/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5825 - acc: 0.7852 - val_loss: 0.8015 - val_acc: 0.6800\n",
      "Epoch 358/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5425 - acc: 0.8163 - val_loss: 0.7993 - val_acc: 0.6844\n",
      "Epoch 359/10000\n",
      "675/675 [==============================] - 0s 103us/step - loss: 0.5751 - acc: 0.7807 - val_loss: 0.7978 - val_acc: 0.6978\n",
      "Epoch 360/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 0.5529 - acc: 0.7881 - val_loss: 0.7994 - val_acc: 0.7111\n",
      "Epoch 361/10000\n",
      "675/675 [==============================] - 0s 112us/step - loss: 0.5320 - acc: 0.8104 - val_loss: 0.7876 - val_acc: 0.7111\n",
      "Epoch 362/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 0.5367 - acc: 0.8148 - val_loss: 0.7834 - val_acc: 0.7111\n",
      "Epoch 363/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.5497 - acc: 0.8000 - val_loss: 0.7799 - val_acc: 0.7156\n",
      "Epoch 364/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.5620 - acc: 0.8074 - val_loss: 0.8060 - val_acc: 0.6711\n",
      "Epoch 365/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5441 - acc: 0.8089 - val_loss: 0.7873 - val_acc: 0.6889\n",
      "Epoch 366/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.5613 - acc: 0.7941 - val_loss: 0.7985 - val_acc: 0.7022\n",
      "Epoch 367/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.5361 - acc: 0.8059 - val_loss: 0.7950 - val_acc: 0.6889\n",
      "Epoch 368/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5280 - acc: 0.8119 - val_loss: 0.7803 - val_acc: 0.7200\n",
      "Epoch 369/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5554 - acc: 0.7985 - val_loss: 0.7919 - val_acc: 0.6844\n",
      "Epoch 370/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.5218 - acc: 0.8281 - val_loss: 0.7758 - val_acc: 0.7067\n",
      "Epoch 371/10000\n",
      "675/675 [==============================] - 0s 108us/step - loss: 0.5381 - acc: 0.8030 - val_loss: 0.7804 - val_acc: 0.7111\n",
      "Epoch 372/10000\n",
      "675/675 [==============================] - 0s 109us/step - loss: 0.5508 - acc: 0.7881 - val_loss: 0.7862 - val_acc: 0.6978\n",
      "Epoch 373/10000\n",
      "675/675 [==============================] - 0s 108us/step - loss: 0.5264 - acc: 0.8178 - val_loss: 0.7927 - val_acc: 0.6933\n",
      "Epoch 374/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 0.5377 - acc: 0.8015 - val_loss: 0.7765 - val_acc: 0.7200\n",
      "Epoch 375/10000\n",
      "675/675 [==============================] - 0s 118us/step - loss: 0.5530 - acc: 0.8044 - val_loss: 0.7849 - val_acc: 0.7022\n",
      "Epoch 376/10000\n",
      "675/675 [==============================] - 0s 143us/step - loss: 0.5268 - acc: 0.8252 - val_loss: 0.7969 - val_acc: 0.7111\n",
      "Epoch 377/10000\n",
      "675/675 [==============================] - 0s 154us/step - loss: 0.5579 - acc: 0.7970 - val_loss: 0.7831 - val_acc: 0.6978\n",
      "Epoch 378/10000\n",
      "675/675 [==============================] - 0s 146us/step - loss: 0.5608 - acc: 0.8089 - val_loss: 0.7754 - val_acc: 0.7067\n",
      "Epoch 379/10000\n",
      "675/675 [==============================] - 0s 118us/step - loss: 0.5491 - acc: 0.8178 - val_loss: 0.7745 - val_acc: 0.6889\n",
      "Epoch 380/10000\n",
      "675/675 [==============================] - 0s 149us/step - loss: 0.5516 - acc: 0.8030 - val_loss: 0.7868 - val_acc: 0.6978\n",
      "Epoch 381/10000\n",
      "675/675 [==============================] - 0s 138us/step - loss: 0.5530 - acc: 0.7881 - val_loss: 0.7791 - val_acc: 0.6800\n",
      "Epoch 382/10000\n",
      "675/675 [==============================] - 0s 151us/step - loss: 0.5233 - acc: 0.8074 - val_loss: 0.7852 - val_acc: 0.6889\n",
      "Epoch 383/10000\n",
      "675/675 [==============================] - 0s 151us/step - loss: 0.5232 - acc: 0.8119 - val_loss: 0.7758 - val_acc: 0.6889\n",
      "Epoch 384/10000\n",
      "675/675 [==============================] - 0s 144us/step - loss: 0.5403 - acc: 0.8089 - val_loss: 0.7783 - val_acc: 0.7022\n",
      "Epoch 385/10000\n",
      "675/675 [==============================] - 0s 145us/step - loss: 0.5334 - acc: 0.8074 - val_loss: 0.7764 - val_acc: 0.7067\n",
      "Epoch 386/10000\n",
      "675/675 [==============================] - 0s 149us/step - loss: 0.5178 - acc: 0.8281 - val_loss: 0.7696 - val_acc: 0.7111\n",
      "Epoch 387/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.5060 - acc: 0.8356 - val_loss: 0.7666 - val_acc: 0.7156\n",
      "Epoch 388/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 0.5326 - acc: 0.8296 - val_loss: 0.7653 - val_acc: 0.7067\n",
      "Epoch 389/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 0.5498 - acc: 0.8133 - val_loss: 0.7702 - val_acc: 0.7156\n",
      "Epoch 390/10000\n",
      "675/675 [==============================] - 0s 99us/step - loss: 0.5074 - acc: 0.8326 - val_loss: 0.7697 - val_acc: 0.6978\n",
      "Epoch 391/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 0.5255 - acc: 0.8163 - val_loss: 0.7768 - val_acc: 0.7022\n",
      "Epoch 392/10000\n",
      "675/675 [==============================] - 0s 100us/step - loss: 0.5325 - acc: 0.8089 - val_loss: 0.7758 - val_acc: 0.7022\n",
      "Epoch 393/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 0.5089 - acc: 0.8207 - val_loss: 0.7707 - val_acc: 0.7067\n",
      "Epoch 394/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 0.5641 - acc: 0.7807 - val_loss: 0.7778 - val_acc: 0.7156\n",
      "Epoch 395/10000\n",
      "675/675 [==============================] - 0s 131us/step - loss: 0.5117 - acc: 0.8296 - val_loss: 0.7560 - val_acc: 0.7156\n",
      "Epoch 396/10000\n",
      "675/675 [==============================] - 0s 141us/step - loss: 0.5358 - acc: 0.8385 - val_loss: 0.7623 - val_acc: 0.7244\n",
      "Epoch 397/10000\n",
      "675/675 [==============================] - 0s 185us/step - loss: 0.5324 - acc: 0.8281 - val_loss: 0.7679 - val_acc: 0.7067\n",
      "Epoch 398/10000\n",
      "675/675 [==============================] - 0s 123us/step - loss: 0.5206 - acc: 0.8326 - val_loss: 0.7597 - val_acc: 0.7067\n",
      "Epoch 399/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5337 - acc: 0.7970 - val_loss: 0.7660 - val_acc: 0.7156\n",
      "Epoch 400/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5282 - acc: 0.8133 - val_loss: 0.7830 - val_acc: 0.7022\n",
      "Epoch 401/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.5086 - acc: 0.8311 - val_loss: 0.7786 - val_acc: 0.7111\n",
      "Epoch 402/10000\n",
      "675/675 [==============================] - 0s 175us/step - loss: 0.5260 - acc: 0.8000 - val_loss: 0.7704 - val_acc: 0.6978\n",
      "Epoch 403/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 0.5264 - acc: 0.8178 - val_loss: 0.7764 - val_acc: 0.6978\n",
      "Epoch 404/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.5089 - acc: 0.8089 - val_loss: 0.7686 - val_acc: 0.7022\n",
      "Epoch 405/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4976 - acc: 0.8222 - val_loss: 0.7770 - val_acc: 0.7156\n",
      "Epoch 406/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5061 - acc: 0.8296 - val_loss: 0.7867 - val_acc: 0.6844\n",
      "Epoch 407/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.5098 - acc: 0.8193 - val_loss: 0.7605 - val_acc: 0.7022\n",
      "Epoch 408/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 0.5065 - acc: 0.8178 - val_loss: 0.7538 - val_acc: 0.7111\n",
      "Epoch 409/10000\n",
      "675/675 [==============================] - 0s 182us/step - loss: 0.5257 - acc: 0.8281 - val_loss: 0.7689 - val_acc: 0.7067\n",
      "Epoch 410/10000\n",
      "675/675 [==============================] - 0s 143us/step - loss: 0.5307 - acc: 0.8163 - val_loss: 0.7656 - val_acc: 0.6933\n",
      "Epoch 411/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.5098 - acc: 0.8148 - val_loss: 0.7681 - val_acc: 0.6933\n",
      "Epoch 412/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4907 - acc: 0.8311 - val_loss: 0.7683 - val_acc: 0.7200\n",
      "Epoch 413/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4867 - acc: 0.8504 - val_loss: 0.7795 - val_acc: 0.6978\n",
      "Epoch 414/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.5091 - acc: 0.8000 - val_loss: 0.7663 - val_acc: 0.6889\n",
      "Epoch 415/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 109us/step - loss: 0.5355 - acc: 0.8089 - val_loss: 0.7738 - val_acc: 0.6889\n",
      "Epoch 416/10000\n",
      "675/675 [==============================] - 0s 151us/step - loss: 0.5051 - acc: 0.8252 - val_loss: 0.7719 - val_acc: 0.6933\n",
      "Epoch 417/10000\n",
      "675/675 [==============================] - 0s 132us/step - loss: 0.5017 - acc: 0.8104 - val_loss: 0.7715 - val_acc: 0.7156\n",
      "Epoch 418/10000\n",
      "675/675 [==============================] - 0s 119us/step - loss: 0.5173 - acc: 0.8207 - val_loss: 0.7669 - val_acc: 0.6889\n",
      "Epoch 419/10000\n",
      "675/675 [==============================] - 0s 149us/step - loss: 0.4858 - acc: 0.8444 - val_loss: 0.7496 - val_acc: 0.7022\n",
      "Epoch 420/10000\n",
      "675/675 [==============================] - 0s 148us/step - loss: 0.5004 - acc: 0.8267 - val_loss: 0.7739 - val_acc: 0.6978\n",
      "Epoch 421/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.4912 - acc: 0.8222 - val_loss: 0.7432 - val_acc: 0.7200\n",
      "Epoch 422/10000\n",
      "675/675 [==============================] - 0s 131us/step - loss: 0.5159 - acc: 0.8237 - val_loss: 0.7495 - val_acc: 0.7244\n",
      "Epoch 423/10000\n",
      "675/675 [==============================] - 0s 139us/step - loss: 0.4777 - acc: 0.8400 - val_loss: 0.7628 - val_acc: 0.6933\n",
      "Epoch 424/10000\n",
      "675/675 [==============================] - 0s 142us/step - loss: 0.4976 - acc: 0.8133 - val_loss: 0.7550 - val_acc: 0.7067\n",
      "Epoch 425/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.5426 - acc: 0.8074 - val_loss: 0.7491 - val_acc: 0.7156\n",
      "Epoch 426/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4948 - acc: 0.8207 - val_loss: 0.7488 - val_acc: 0.6978\n",
      "Epoch 427/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.5094 - acc: 0.8237 - val_loss: 0.7519 - val_acc: 0.6978\n",
      "Epoch 428/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.5078 - acc: 0.8267 - val_loss: 0.7634 - val_acc: 0.6889\n",
      "Epoch 429/10000\n",
      "675/675 [==============================] - 0s 101us/step - loss: 0.5032 - acc: 0.8341 - val_loss: 0.7586 - val_acc: 0.7111\n",
      "Epoch 430/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.5141 - acc: 0.8326 - val_loss: 0.7427 - val_acc: 0.7244\n",
      "Epoch 431/10000\n",
      "675/675 [==============================] - 0s 121us/step - loss: 0.4807 - acc: 0.8385 - val_loss: 0.7525 - val_acc: 0.6978\n",
      "Epoch 432/10000\n",
      "675/675 [==============================] - 0s 116us/step - loss: 0.4907 - acc: 0.8178 - val_loss: 0.7709 - val_acc: 0.6889\n",
      "Epoch 433/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 0.5014 - acc: 0.8341 - val_loss: 0.7479 - val_acc: 0.7244\n",
      "Epoch 434/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.4943 - acc: 0.8163 - val_loss: 0.7462 - val_acc: 0.7067\n",
      "Epoch 435/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.5133 - acc: 0.8089 - val_loss: 0.7557 - val_acc: 0.7022\n",
      "Epoch 436/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4940 - acc: 0.8222 - val_loss: 0.7467 - val_acc: 0.7156\n",
      "Epoch 437/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4702 - acc: 0.8385 - val_loss: 0.7589 - val_acc: 0.7333\n",
      "Epoch 438/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.5063 - acc: 0.8193 - val_loss: 0.7588 - val_acc: 0.7067\n",
      "Epoch 439/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.5049 - acc: 0.8296 - val_loss: 0.7481 - val_acc: 0.7200\n",
      "Epoch 440/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4785 - acc: 0.8459 - val_loss: 0.7547 - val_acc: 0.6978\n",
      "Epoch 441/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4804 - acc: 0.8444 - val_loss: 0.7716 - val_acc: 0.6889\n",
      "Epoch 442/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.4841 - acc: 0.8296 - val_loss: 0.7595 - val_acc: 0.7022\n",
      "Epoch 443/10000\n",
      "675/675 [==============================] - 0s 104us/step - loss: 0.5049 - acc: 0.8178 - val_loss: 0.7532 - val_acc: 0.7111\n",
      "Epoch 444/10000\n",
      "675/675 [==============================] - 0s 102us/step - loss: 0.4959 - acc: 0.8341 - val_loss: 0.7713 - val_acc: 0.6844\n",
      "Epoch 445/10000\n",
      "675/675 [==============================] - 0s 102us/step - loss: 0.4903 - acc: 0.8148 - val_loss: 0.7649 - val_acc: 0.7067\n",
      "Epoch 446/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 0.4929 - acc: 0.8089 - val_loss: 0.7514 - val_acc: 0.7156\n",
      "Epoch 447/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4800 - acc: 0.8326 - val_loss: 0.7505 - val_acc: 0.7067\n",
      "Epoch 448/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4909 - acc: 0.8370 - val_loss: 0.7620 - val_acc: 0.7067\n",
      "Epoch 449/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4828 - acc: 0.8415 - val_loss: 0.7454 - val_acc: 0.7022\n",
      "Epoch 450/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 0.5071 - acc: 0.8281 - val_loss: 0.7485 - val_acc: 0.6978\n",
      "Epoch 451/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 0.4845 - acc: 0.8267 - val_loss: 0.7503 - val_acc: 0.7022\n",
      "Epoch 452/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4774 - acc: 0.8341 - val_loss: 0.7687 - val_acc: 0.7022\n",
      "Epoch 453/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4792 - acc: 0.8311 - val_loss: 0.7417 - val_acc: 0.7111\n",
      "Epoch 454/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.4863 - acc: 0.8400 - val_loss: 0.7431 - val_acc: 0.7156\n",
      "Epoch 455/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4787 - acc: 0.8326 - val_loss: 0.7757 - val_acc: 0.6889\n",
      "Epoch 456/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.4739 - acc: 0.8311 - val_loss: 0.7462 - val_acc: 0.7200\n",
      "Epoch 457/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4523 - acc: 0.8622 - val_loss: 0.7418 - val_acc: 0.7111\n",
      "Epoch 458/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.5183 - acc: 0.8267 - val_loss: 0.7470 - val_acc: 0.7200\n",
      "Epoch 459/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4734 - acc: 0.8459 - val_loss: 0.7263 - val_acc: 0.7156\n",
      "Epoch 460/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4924 - acc: 0.8326 - val_loss: 0.7373 - val_acc: 0.7067\n",
      "Epoch 461/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4791 - acc: 0.8415 - val_loss: 0.7460 - val_acc: 0.7156\n",
      "Epoch 462/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4774 - acc: 0.8326 - val_loss: 0.7429 - val_acc: 0.7111\n",
      "Epoch 463/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4748 - acc: 0.8311 - val_loss: 0.7492 - val_acc: 0.7022\n",
      "Epoch 464/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4515 - acc: 0.8489 - val_loss: 0.7350 - val_acc: 0.7156\n",
      "Epoch 465/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.4660 - acc: 0.8504 - val_loss: 0.7664 - val_acc: 0.7111\n",
      "Epoch 466/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4573 - acc: 0.8533 - val_loss: 0.7462 - val_acc: 0.7067\n",
      "Epoch 467/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4695 - acc: 0.8415 - val_loss: 0.7571 - val_acc: 0.7289\n",
      "Epoch 468/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4698 - acc: 0.8385 - val_loss: 0.7472 - val_acc: 0.6978\n",
      "Epoch 469/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4708 - acc: 0.8267 - val_loss: 0.7349 - val_acc: 0.7111\n",
      "Epoch 470/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4849 - acc: 0.8148 - val_loss: 0.7428 - val_acc: 0.7333\n",
      "Epoch 471/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 0.4665 - acc: 0.8415 - val_loss: 0.7400 - val_acc: 0.7289\n",
      "Epoch 472/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4679 - acc: 0.8311 - val_loss: 0.7565 - val_acc: 0.7111\n",
      "Epoch 473/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4661 - acc: 0.8504 - val_loss: 0.7399 - val_acc: 0.7111\n",
      "Epoch 474/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 80us/step - loss: 0.4728 - acc: 0.8341 - val_loss: 0.7508 - val_acc: 0.7111\n",
      "Epoch 475/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4663 - acc: 0.8370 - val_loss: 0.7649 - val_acc: 0.6933\n",
      "Epoch 476/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4661 - acc: 0.8237 - val_loss: 0.7391 - val_acc: 0.7022\n",
      "Epoch 477/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4790 - acc: 0.8370 - val_loss: 0.7423 - val_acc: 0.6978\n",
      "Epoch 478/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4578 - acc: 0.8548 - val_loss: 0.7507 - val_acc: 0.7022\n",
      "Epoch 479/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4802 - acc: 0.8267 - val_loss: 0.7371 - val_acc: 0.7022\n",
      "Epoch 480/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4668 - acc: 0.8207 - val_loss: 0.7514 - val_acc: 0.7156\n",
      "Epoch 481/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4625 - acc: 0.8430 - val_loss: 0.7495 - val_acc: 0.6978\n",
      "Epoch 482/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4802 - acc: 0.8341 - val_loss: 0.7438 - val_acc: 0.7156\n",
      "Epoch 483/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4696 - acc: 0.8400 - val_loss: 0.7372 - val_acc: 0.7111\n",
      "Epoch 484/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4616 - acc: 0.8400 - val_loss: 0.7308 - val_acc: 0.7111\n",
      "Epoch 485/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4888 - acc: 0.8207 - val_loss: 0.7657 - val_acc: 0.7022\n",
      "Epoch 486/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4712 - acc: 0.8400 - val_loss: 0.7769 - val_acc: 0.7022\n",
      "Epoch 487/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4596 - acc: 0.8341 - val_loss: 0.7464 - val_acc: 0.7156\n",
      "Epoch 488/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4574 - acc: 0.8430 - val_loss: 0.7407 - val_acc: 0.7156\n",
      "Epoch 489/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4485 - acc: 0.8415 - val_loss: 0.7351 - val_acc: 0.7200\n",
      "Epoch 490/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4910 - acc: 0.8193 - val_loss: 0.7399 - val_acc: 0.7022\n",
      "Epoch 491/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4736 - acc: 0.8356 - val_loss: 0.7277 - val_acc: 0.7156\n",
      "Epoch 492/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4517 - acc: 0.8533 - val_loss: 0.7389 - val_acc: 0.7200\n",
      "Epoch 493/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4691 - acc: 0.8311 - val_loss: 0.7369 - val_acc: 0.7022\n",
      "Epoch 494/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.4499 - acc: 0.8593 - val_loss: 0.7306 - val_acc: 0.7200\n",
      "Epoch 495/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4727 - acc: 0.8119 - val_loss: 0.7344 - val_acc: 0.7022\n",
      "Epoch 496/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4823 - acc: 0.8311 - val_loss: 0.7218 - val_acc: 0.7289\n",
      "Epoch 497/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4637 - acc: 0.8489 - val_loss: 0.7980 - val_acc: 0.6889\n",
      "Epoch 498/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4678 - acc: 0.8133 - val_loss: 0.7362 - val_acc: 0.7111\n",
      "Epoch 499/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4589 - acc: 0.8415 - val_loss: 0.7647 - val_acc: 0.7022\n",
      "Epoch 500/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4689 - acc: 0.8385 - val_loss: 0.7368 - val_acc: 0.7022\n",
      "Epoch 501/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4693 - acc: 0.8341 - val_loss: 0.7357 - val_acc: 0.6978\n",
      "Epoch 502/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4439 - acc: 0.8548 - val_loss: 0.7551 - val_acc: 0.7111\n",
      "Epoch 503/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.4466 - acc: 0.8519 - val_loss: 0.7249 - val_acc: 0.7289\n",
      "Epoch 504/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4534 - acc: 0.8519 - val_loss: 0.7343 - val_acc: 0.7289\n",
      "Epoch 505/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4322 - acc: 0.8548 - val_loss: 0.7383 - val_acc: 0.7022\n",
      "Epoch 506/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4447 - acc: 0.8563 - val_loss: 0.7266 - val_acc: 0.7200\n",
      "Epoch 507/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4632 - acc: 0.8385 - val_loss: 0.7514 - val_acc: 0.7067\n",
      "Epoch 508/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4620 - acc: 0.8311 - val_loss: 0.7393 - val_acc: 0.7067\n",
      "Epoch 509/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4696 - acc: 0.8311 - val_loss: 0.7306 - val_acc: 0.7111\n",
      "Epoch 510/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4195 - acc: 0.8548 - val_loss: 0.7296 - val_acc: 0.7200\n",
      "Epoch 511/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4656 - acc: 0.8326 - val_loss: 0.7281 - val_acc: 0.7156\n",
      "Epoch 512/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4580 - acc: 0.8415 - val_loss: 0.7380 - val_acc: 0.7111\n",
      "Epoch 513/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4130 - acc: 0.8533 - val_loss: 0.7203 - val_acc: 0.7244\n",
      "Epoch 514/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4433 - acc: 0.8563 - val_loss: 0.7401 - val_acc: 0.7156\n",
      "Epoch 515/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4537 - acc: 0.8444 - val_loss: 0.7123 - val_acc: 0.7156\n",
      "Epoch 516/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4437 - acc: 0.8652 - val_loss: 0.7583 - val_acc: 0.7156\n",
      "Epoch 517/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4657 - acc: 0.8326 - val_loss: 0.7316 - val_acc: 0.7200\n",
      "Epoch 518/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4592 - acc: 0.8311 - val_loss: 0.7099 - val_acc: 0.7289\n",
      "Epoch 519/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4530 - acc: 0.8385 - val_loss: 0.7261 - val_acc: 0.7200\n",
      "Epoch 520/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4626 - acc: 0.8400 - val_loss: 0.7132 - val_acc: 0.7244\n",
      "Epoch 521/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4475 - acc: 0.8415 - val_loss: 0.7154 - val_acc: 0.7244\n",
      "Epoch 522/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4552 - acc: 0.8474 - val_loss: 0.7319 - val_acc: 0.7156\n",
      "Epoch 523/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4525 - acc: 0.8474 - val_loss: 0.7440 - val_acc: 0.7200\n",
      "Epoch 524/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4230 - acc: 0.8489 - val_loss: 0.7373 - val_acc: 0.7200\n",
      "Epoch 525/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4345 - acc: 0.8548 - val_loss: 0.7411 - val_acc: 0.7200\n",
      "Epoch 526/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4302 - acc: 0.8519 - val_loss: 0.7449 - val_acc: 0.7156\n",
      "Epoch 527/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4410 - acc: 0.8356 - val_loss: 0.7134 - val_acc: 0.7289\n",
      "Epoch 528/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4386 - acc: 0.8563 - val_loss: 0.7558 - val_acc: 0.7156\n",
      "Epoch 529/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4481 - acc: 0.8489 - val_loss: 0.7285 - val_acc: 0.7244\n",
      "Epoch 530/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4387 - acc: 0.8459 - val_loss: 0.7109 - val_acc: 0.7333\n",
      "Epoch 531/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4539 - acc: 0.8444 - val_loss: 0.7185 - val_acc: 0.7200\n",
      "Epoch 532/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4295 - acc: 0.8504 - val_loss: 0.7489 - val_acc: 0.7156\n",
      "Epoch 533/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 82us/step - loss: 0.4172 - acc: 0.8637 - val_loss: 0.7398 - val_acc: 0.7111\n",
      "Epoch 534/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4317 - acc: 0.8459 - val_loss: 0.7531 - val_acc: 0.7067\n",
      "Epoch 535/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4307 - acc: 0.8430 - val_loss: 0.7450 - val_acc: 0.7156\n",
      "Epoch 536/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4419 - acc: 0.8370 - val_loss: 0.7385 - val_acc: 0.7111\n",
      "Epoch 537/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4579 - acc: 0.8415 - val_loss: 0.7620 - val_acc: 0.7111\n",
      "Epoch 538/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4254 - acc: 0.8430 - val_loss: 0.7228 - val_acc: 0.7111\n",
      "Epoch 539/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4070 - acc: 0.8711 - val_loss: 0.7200 - val_acc: 0.7111\n",
      "Epoch 540/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4531 - acc: 0.8504 - val_loss: 0.7590 - val_acc: 0.7244\n",
      "Epoch 541/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4297 - acc: 0.8607 - val_loss: 0.7481 - val_acc: 0.7022\n",
      "Epoch 542/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4253 - acc: 0.8415 - val_loss: 0.7277 - val_acc: 0.7244\n",
      "Epoch 543/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4401 - acc: 0.8400 - val_loss: 0.7438 - val_acc: 0.7156\n",
      "Epoch 544/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4216 - acc: 0.8533 - val_loss: 0.7275 - val_acc: 0.7244\n",
      "Epoch 545/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4563 - acc: 0.8430 - val_loss: 0.7162 - val_acc: 0.7333\n",
      "Epoch 546/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4186 - acc: 0.8637 - val_loss: 0.7311 - val_acc: 0.7200\n",
      "Epoch 547/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4294 - acc: 0.8681 - val_loss: 0.7168 - val_acc: 0.7244\n",
      "Epoch 548/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4347 - acc: 0.8459 - val_loss: 0.7199 - val_acc: 0.7244\n",
      "Epoch 549/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4269 - acc: 0.8578 - val_loss: 0.7438 - val_acc: 0.7067\n",
      "Epoch 550/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4328 - acc: 0.8415 - val_loss: 0.7119 - val_acc: 0.7289\n",
      "Epoch 551/10000\n",
      "675/675 [==============================] - 0s 133us/step - loss: 0.4401 - acc: 0.8311 - val_loss: 0.7339 - val_acc: 0.7156\n",
      "Epoch 552/10000\n",
      "675/675 [==============================] - 0s 128us/step - loss: 0.4031 - acc: 0.8563 - val_loss: 0.7262 - val_acc: 0.7111\n",
      "Epoch 553/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.4218 - acc: 0.8474 - val_loss: 0.7334 - val_acc: 0.7333\n",
      "Epoch 554/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 0.4427 - acc: 0.8370 - val_loss: 0.7046 - val_acc: 0.7378\n",
      "Epoch 555/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4433 - acc: 0.8444 - val_loss: 0.7189 - val_acc: 0.7111\n",
      "Epoch 556/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4014 - acc: 0.8578 - val_loss: 0.7149 - val_acc: 0.7111\n",
      "Epoch 557/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 0.4141 - acc: 0.8681 - val_loss: 0.7179 - val_acc: 0.7200\n",
      "Epoch 558/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4247 - acc: 0.8548 - val_loss: 0.7148 - val_acc: 0.7422\n",
      "Epoch 559/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4460 - acc: 0.8370 - val_loss: 0.7097 - val_acc: 0.7333\n",
      "Epoch 560/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.4376 - acc: 0.8400 - val_loss: 0.7358 - val_acc: 0.7156\n",
      "Epoch 561/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4429 - acc: 0.8267 - val_loss: 0.7263 - val_acc: 0.7156\n",
      "Epoch 562/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4069 - acc: 0.8578 - val_loss: 0.7154 - val_acc: 0.7333\n",
      "Epoch 563/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4266 - acc: 0.8519 - val_loss: 0.7139 - val_acc: 0.7111\n",
      "Epoch 564/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4231 - acc: 0.8430 - val_loss: 0.7316 - val_acc: 0.7200\n",
      "Epoch 565/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 0.4226 - acc: 0.8459 - val_loss: 0.7522 - val_acc: 0.7022\n",
      "Epoch 566/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4135 - acc: 0.8548 - val_loss: 0.7499 - val_acc: 0.7200\n",
      "Epoch 567/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4346 - acc: 0.8400 - val_loss: 0.7249 - val_acc: 0.7200\n",
      "Epoch 568/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4116 - acc: 0.8578 - val_loss: 0.7529 - val_acc: 0.6978\n",
      "Epoch 569/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4295 - acc: 0.8430 - val_loss: 0.7239 - val_acc: 0.7156\n",
      "Epoch 570/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4435 - acc: 0.8533 - val_loss: 0.7119 - val_acc: 0.7200\n",
      "Epoch 571/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4295 - acc: 0.8430 - val_loss: 0.7354 - val_acc: 0.7200\n",
      "Epoch 572/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4192 - acc: 0.8519 - val_loss: 0.7266 - val_acc: 0.7156\n",
      "Epoch 573/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4753 - acc: 0.8326 - val_loss: 0.7315 - val_acc: 0.7156\n",
      "Epoch 574/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4484 - acc: 0.8430 - val_loss: 0.7218 - val_acc: 0.7200\n",
      "Epoch 575/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4433 - acc: 0.8326 - val_loss: 0.7335 - val_acc: 0.7200\n",
      "Epoch 576/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4101 - acc: 0.8667 - val_loss: 0.7451 - val_acc: 0.7244\n",
      "Epoch 577/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4391 - acc: 0.8444 - val_loss: 0.7467 - val_acc: 0.7200\n",
      "Epoch 578/10000\n",
      "675/675 [==============================] - 0s 165us/step - loss: 0.4460 - acc: 0.8489 - val_loss: 0.7598 - val_acc: 0.7156\n",
      "Epoch 579/10000\n",
      "675/675 [==============================] - 0s 139us/step - loss: 0.4443 - acc: 0.8519 - val_loss: 0.7463 - val_acc: 0.7111\n",
      "Epoch 580/10000\n",
      "675/675 [==============================] - 0s 105us/step - loss: 0.4618 - acc: 0.8415 - val_loss: 0.7384 - val_acc: 0.7244\n",
      "Epoch 581/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.4204 - acc: 0.8385 - val_loss: 0.7263 - val_acc: 0.7156\n",
      "Epoch 582/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4402 - acc: 0.8519 - val_loss: 0.7282 - val_acc: 0.7244\n",
      "Epoch 583/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.4269 - acc: 0.8444 - val_loss: 0.7413 - val_acc: 0.7067\n",
      "Epoch 584/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4235 - acc: 0.8578 - val_loss: 0.7398 - val_acc: 0.7111\n",
      "Epoch 585/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4260 - acc: 0.8548 - val_loss: 0.7303 - val_acc: 0.7200\n",
      "Epoch 586/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4202 - acc: 0.8696 - val_loss: 0.7395 - val_acc: 0.7156\n",
      "Epoch 587/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.4455 - acc: 0.8356 - val_loss: 0.7258 - val_acc: 0.7244\n",
      "Epoch 588/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4245 - acc: 0.8385 - val_loss: 0.7382 - val_acc: 0.7200\n",
      "Epoch 589/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.4145 - acc: 0.8593 - val_loss: 0.7144 - val_acc: 0.7244\n",
      "Epoch 590/10000\n",
      "675/675 [==============================] - 0s 126us/step - loss: 0.4194 - acc: 0.8578 - val_loss: 0.7147 - val_acc: 0.7244\n",
      "Epoch 591/10000\n",
      "675/675 [==============================] - 0s 122us/step - loss: 0.4237 - acc: 0.8489 - val_loss: 0.7038 - val_acc: 0.7333\n",
      "Epoch 592/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 120us/step - loss: 0.4105 - acc: 0.8607 - val_loss: 0.7056 - val_acc: 0.7378\n",
      "Epoch 593/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 0.4208 - acc: 0.8504 - val_loss: 0.7163 - val_acc: 0.7422\n",
      "Epoch 594/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4167 - acc: 0.8563 - val_loss: 0.7460 - val_acc: 0.7289\n",
      "Epoch 595/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.3988 - acc: 0.8652 - val_loss: 0.7146 - val_acc: 0.7289\n",
      "Epoch 596/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 0.4104 - acc: 0.8622 - val_loss: 0.7388 - val_acc: 0.7333\n",
      "Epoch 597/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3988 - acc: 0.8607 - val_loss: 0.7498 - val_acc: 0.7244\n",
      "Epoch 598/10000\n",
      "675/675 [==============================] - 0s 109us/step - loss: 0.4017 - acc: 0.8637 - val_loss: 0.7648 - val_acc: 0.7111\n",
      "Epoch 599/10000\n",
      "675/675 [==============================] - 0s 153us/step - loss: 0.4055 - acc: 0.8519 - val_loss: 0.7270 - val_acc: 0.7111\n",
      "Epoch 600/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.4064 - acc: 0.8607 - val_loss: 0.7082 - val_acc: 0.7467\n",
      "Epoch 601/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.4140 - acc: 0.8593 - val_loss: 0.7565 - val_acc: 0.7022\n",
      "Epoch 602/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4327 - acc: 0.8400 - val_loss: 0.7026 - val_acc: 0.7244\n",
      "Epoch 603/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.4137 - acc: 0.8607 - val_loss: 0.7030 - val_acc: 0.7333\n",
      "Epoch 604/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3955 - acc: 0.8667 - val_loss: 0.7138 - val_acc: 0.7111\n",
      "Epoch 605/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4377 - acc: 0.8593 - val_loss: 0.7030 - val_acc: 0.7333\n",
      "Epoch 606/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 0.4047 - acc: 0.8578 - val_loss: 0.7009 - val_acc: 0.7289\n",
      "Epoch 607/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.4474 - acc: 0.8415 - val_loss: 0.7144 - val_acc: 0.7200\n",
      "Epoch 608/10000\n",
      "675/675 [==============================] - 0s 120us/step - loss: 0.3802 - acc: 0.8785 - val_loss: 0.7041 - val_acc: 0.7467\n",
      "Epoch 609/10000\n",
      "675/675 [==============================] - 0s 122us/step - loss: 0.4307 - acc: 0.8504 - val_loss: 0.7687 - val_acc: 0.7022\n",
      "Epoch 610/10000\n",
      "675/675 [==============================] - 0s 140us/step - loss: 0.4059 - acc: 0.8548 - val_loss: 0.7071 - val_acc: 0.7511\n",
      "Epoch 611/10000\n",
      "675/675 [==============================] - 0s 147us/step - loss: 0.3835 - acc: 0.8637 - val_loss: 0.7046 - val_acc: 0.7289\n",
      "Epoch 612/10000\n",
      "675/675 [==============================] - 0s 176us/step - loss: 0.3733 - acc: 0.8800 - val_loss: 0.7348 - val_acc: 0.7200\n",
      "Epoch 613/10000\n",
      "675/675 [==============================] - 0s 155us/step - loss: 0.4128 - acc: 0.8356 - val_loss: 0.7329 - val_acc: 0.7200\n",
      "Epoch 614/10000\n",
      "675/675 [==============================] - 0s 149us/step - loss: 0.4504 - acc: 0.8311 - val_loss: 0.7094 - val_acc: 0.7378\n",
      "Epoch 615/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 0.4064 - acc: 0.8696 - val_loss: 0.7321 - val_acc: 0.7333\n",
      "Epoch 616/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 0.4405 - acc: 0.8474 - val_loss: 0.7074 - val_acc: 0.7467\n",
      "Epoch 617/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4027 - acc: 0.8770 - val_loss: 0.7229 - val_acc: 0.7200\n",
      "Epoch 618/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3696 - acc: 0.8800 - val_loss: 0.7221 - val_acc: 0.7422\n",
      "Epoch 619/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 0.4151 - acc: 0.8578 - val_loss: 0.7189 - val_acc: 0.7200\n",
      "Epoch 620/10000\n",
      "675/675 [==============================] - 0s 145us/step - loss: 0.3972 - acc: 0.8667 - val_loss: 0.7131 - val_acc: 0.7511\n",
      "Epoch 621/10000\n",
      "675/675 [==============================] - 0s 92us/step - loss: 0.4003 - acc: 0.8667 - val_loss: 0.7162 - val_acc: 0.7333\n",
      "Epoch 622/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3852 - acc: 0.8756 - val_loss: 0.7306 - val_acc: 0.7244\n",
      "Epoch 623/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 0.4034 - acc: 0.8726 - val_loss: 0.7034 - val_acc: 0.7422\n",
      "Epoch 624/10000\n",
      "675/675 [==============================] - 0s 97us/step - loss: 0.3861 - acc: 0.8741 - val_loss: 0.7056 - val_acc: 0.7378\n",
      "Epoch 625/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3926 - acc: 0.8622 - val_loss: 0.7137 - val_acc: 0.7200\n",
      "Epoch 626/10000\n",
      "675/675 [==============================] - 0s 164us/step - loss: 0.4108 - acc: 0.8607 - val_loss: 0.7377 - val_acc: 0.7244\n",
      "Epoch 627/10000\n",
      "675/675 [==============================] - 0s 161us/step - loss: 0.3610 - acc: 0.8830 - val_loss: 0.7192 - val_acc: 0.7289\n",
      "Epoch 628/10000\n",
      "675/675 [==============================] - 0s 102us/step - loss: 0.4021 - acc: 0.8563 - val_loss: 0.7080 - val_acc: 0.7422\n",
      "Epoch 629/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3945 - acc: 0.8548 - val_loss: 0.7099 - val_acc: 0.7289\n",
      "Epoch 630/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4270 - acc: 0.8533 - val_loss: 0.6994 - val_acc: 0.7467\n",
      "Epoch 631/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4168 - acc: 0.8415 - val_loss: 0.7127 - val_acc: 0.7200\n",
      "Epoch 632/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3692 - acc: 0.8652 - val_loss: 0.7260 - val_acc: 0.7200\n",
      "Epoch 633/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3935 - acc: 0.8667 - val_loss: 0.7317 - val_acc: 0.7200\n",
      "Epoch 634/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4002 - acc: 0.8548 - val_loss: 0.7561 - val_acc: 0.7111\n",
      "Epoch 635/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4070 - acc: 0.8578 - val_loss: 0.7219 - val_acc: 0.7244\n",
      "Epoch 636/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4250 - acc: 0.8474 - val_loss: 0.7168 - val_acc: 0.7289\n",
      "Epoch 637/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4102 - acc: 0.8548 - val_loss: 0.7123 - val_acc: 0.7378\n",
      "Epoch 638/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4145 - acc: 0.8607 - val_loss: 0.7137 - val_acc: 0.7244\n",
      "Epoch 639/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3859 - acc: 0.8815 - val_loss: 0.7046 - val_acc: 0.7333\n",
      "Epoch 640/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4016 - acc: 0.8652 - val_loss: 0.7372 - val_acc: 0.7289\n",
      "Epoch 641/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4014 - acc: 0.8637 - val_loss: 0.6955 - val_acc: 0.7467\n",
      "Epoch 642/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4293 - acc: 0.8311 - val_loss: 0.7216 - val_acc: 0.7244\n",
      "Epoch 643/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3868 - acc: 0.8711 - val_loss: 0.6959 - val_acc: 0.7422\n",
      "Epoch 644/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3832 - acc: 0.8726 - val_loss: 0.6893 - val_acc: 0.7378\n",
      "Epoch 645/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3768 - acc: 0.8800 - val_loss: 0.7568 - val_acc: 0.7200\n",
      "Epoch 646/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3935 - acc: 0.8637 - val_loss: 0.7304 - val_acc: 0.7244\n",
      "Epoch 647/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4154 - acc: 0.8726 - val_loss: 0.6947 - val_acc: 0.7422\n",
      "Epoch 648/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4068 - acc: 0.8563 - val_loss: 0.7081 - val_acc: 0.7244\n",
      "Epoch 649/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3905 - acc: 0.8667 - val_loss: 0.7724 - val_acc: 0.7067\n",
      "Epoch 650/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4216 - acc: 0.8504 - val_loss: 0.7429 - val_acc: 0.7156\n",
      "Epoch 651/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 80us/step - loss: 0.3918 - acc: 0.8726 - val_loss: 0.7396 - val_acc: 0.7156\n",
      "Epoch 652/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.3924 - acc: 0.8519 - val_loss: 0.7125 - val_acc: 0.7511\n",
      "Epoch 653/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3803 - acc: 0.8741 - val_loss: 0.7068 - val_acc: 0.7289\n",
      "Epoch 654/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3924 - acc: 0.8593 - val_loss: 0.7526 - val_acc: 0.7156\n",
      "Epoch 655/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3977 - acc: 0.8474 - val_loss: 0.7211 - val_acc: 0.7244\n",
      "Epoch 656/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3749 - acc: 0.8652 - val_loss: 0.7168 - val_acc: 0.7422\n",
      "Epoch 657/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3973 - acc: 0.8622 - val_loss: 0.7350 - val_acc: 0.7156\n",
      "Epoch 658/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3874 - acc: 0.8548 - val_loss: 0.7176 - val_acc: 0.7200\n",
      "Epoch 659/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4113 - acc: 0.8607 - val_loss: 0.7183 - val_acc: 0.7378\n",
      "Epoch 660/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3865 - acc: 0.8711 - val_loss: 0.7123 - val_acc: 0.7333\n",
      "Epoch 661/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3973 - acc: 0.8607 - val_loss: 0.7114 - val_acc: 0.7333\n",
      "Epoch 662/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3492 - acc: 0.8815 - val_loss: 0.7139 - val_acc: 0.7333\n",
      "Epoch 663/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3804 - acc: 0.8711 - val_loss: 0.7201 - val_acc: 0.7244\n",
      "Epoch 664/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4117 - acc: 0.8519 - val_loss: 0.7297 - val_acc: 0.7244\n",
      "Epoch 665/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3856 - acc: 0.8711 - val_loss: 0.7088 - val_acc: 0.7556\n",
      "Epoch 666/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4072 - acc: 0.8652 - val_loss: 0.7164 - val_acc: 0.7244\n",
      "Epoch 667/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3955 - acc: 0.8548 - val_loss: 0.7265 - val_acc: 0.7156\n",
      "Epoch 668/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4074 - acc: 0.8519 - val_loss: 0.7389 - val_acc: 0.7333\n",
      "Epoch 669/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3830 - acc: 0.8770 - val_loss: 0.7168 - val_acc: 0.7422\n",
      "Epoch 670/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3709 - acc: 0.8681 - val_loss: 0.7200 - val_acc: 0.7378\n",
      "Epoch 671/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3938 - acc: 0.8726 - val_loss: 0.7628 - val_acc: 0.7111\n",
      "Epoch 672/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3939 - acc: 0.8681 - val_loss: 0.7037 - val_acc: 0.7467\n",
      "Epoch 673/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3851 - acc: 0.8637 - val_loss: 0.7052 - val_acc: 0.7244\n",
      "Epoch 674/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3756 - acc: 0.8889 - val_loss: 0.6928 - val_acc: 0.7467\n",
      "Epoch 675/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3694 - acc: 0.8844 - val_loss: 0.7024 - val_acc: 0.7422\n",
      "Epoch 676/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3687 - acc: 0.8844 - val_loss: 0.7103 - val_acc: 0.7333\n",
      "Epoch 677/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3852 - acc: 0.8637 - val_loss: 0.7287 - val_acc: 0.7333\n",
      "Epoch 678/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4100 - acc: 0.8459 - val_loss: 0.7174 - val_acc: 0.7422\n",
      "Epoch 679/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3785 - acc: 0.8741 - val_loss: 0.7277 - val_acc: 0.7378\n",
      "Epoch 680/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3825 - acc: 0.8622 - val_loss: 0.7125 - val_acc: 0.7333\n",
      "Epoch 681/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3997 - acc: 0.8459 - val_loss: 0.7289 - val_acc: 0.7333\n",
      "Epoch 682/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3837 - acc: 0.8637 - val_loss: 0.7046 - val_acc: 0.7467\n",
      "Epoch 683/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3928 - acc: 0.8726 - val_loss: 0.7078 - val_acc: 0.7467\n",
      "Epoch 684/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3922 - acc: 0.8548 - val_loss: 0.7596 - val_acc: 0.7289\n",
      "Epoch 685/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3816 - acc: 0.8681 - val_loss: 0.7213 - val_acc: 0.7289\n",
      "Epoch 686/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4111 - acc: 0.8815 - val_loss: 0.6977 - val_acc: 0.7467\n",
      "Epoch 687/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3855 - acc: 0.8607 - val_loss: 0.7090 - val_acc: 0.7289\n",
      "Epoch 688/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3750 - acc: 0.8904 - val_loss: 0.6878 - val_acc: 0.7556\n",
      "Epoch 689/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3831 - acc: 0.8770 - val_loss: 0.7172 - val_acc: 0.7289\n",
      "Epoch 690/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3861 - acc: 0.8652 - val_loss: 0.7239 - val_acc: 0.7244\n",
      "Epoch 691/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4302 - acc: 0.8474 - val_loss: 0.7225 - val_acc: 0.7378\n",
      "Epoch 692/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.4110 - acc: 0.8578 - val_loss: 0.7096 - val_acc: 0.7422\n",
      "Epoch 693/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3905 - acc: 0.8607 - val_loss: 0.7562 - val_acc: 0.7200\n",
      "Epoch 694/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3973 - acc: 0.8622 - val_loss: 0.7101 - val_acc: 0.7689\n",
      "Epoch 695/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3360 - acc: 0.8919 - val_loss: 0.7228 - val_acc: 0.7467\n",
      "Epoch 696/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3748 - acc: 0.8681 - val_loss: 0.7137 - val_acc: 0.7467\n",
      "Epoch 697/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3763 - acc: 0.8844 - val_loss: 0.7010 - val_acc: 0.7733\n",
      "Epoch 698/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3801 - acc: 0.8681 - val_loss: 0.7461 - val_acc: 0.7378\n",
      "Epoch 699/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3781 - acc: 0.8726 - val_loss: 0.7269 - val_acc: 0.7289\n",
      "Epoch 700/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4129 - acc: 0.8444 - val_loss: 0.7328 - val_acc: 0.7378\n",
      "Epoch 701/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4012 - acc: 0.8667 - val_loss: 0.7295 - val_acc: 0.7467\n",
      "Epoch 702/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3846 - acc: 0.8711 - val_loss: 0.7352 - val_acc: 0.7200\n",
      "Epoch 703/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3580 - acc: 0.8756 - val_loss: 0.7008 - val_acc: 0.7556\n",
      "Epoch 704/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3747 - acc: 0.8593 - val_loss: 0.7249 - val_acc: 0.7244\n",
      "Epoch 705/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.4190 - acc: 0.8311 - val_loss: 0.7147 - val_acc: 0.7467\n",
      "Epoch 706/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4034 - acc: 0.8548 - val_loss: 0.7207 - val_acc: 0.7333\n",
      "Epoch 707/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3930 - acc: 0.8607 - val_loss: 0.7279 - val_acc: 0.7244\n",
      "Epoch 708/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.4094 - acc: 0.8548 - val_loss: 0.7243 - val_acc: 0.7467\n",
      "Epoch 709/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4034 - acc: 0.8593 - val_loss: 0.6932 - val_acc: 0.7600\n",
      "Epoch 710/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 80us/step - loss: 0.3641 - acc: 0.8800 - val_loss: 0.7292 - val_acc: 0.7378\n",
      "Epoch 711/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3682 - acc: 0.8652 - val_loss: 0.7286 - val_acc: 0.7333\n",
      "Epoch 712/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3618 - acc: 0.8815 - val_loss: 0.7059 - val_acc: 0.7467\n",
      "Epoch 713/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4030 - acc: 0.8563 - val_loss: 0.7239 - val_acc: 0.7378\n",
      "Epoch 714/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3547 - acc: 0.8681 - val_loss: 0.7285 - val_acc: 0.7378\n",
      "Epoch 715/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3352 - acc: 0.8919 - val_loss: 0.7341 - val_acc: 0.7244\n",
      "Epoch 716/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3574 - acc: 0.8637 - val_loss: 0.7001 - val_acc: 0.7644\n",
      "Epoch 717/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3656 - acc: 0.8548 - val_loss: 0.7470 - val_acc: 0.7200\n",
      "Epoch 718/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 0.3816 - acc: 0.8593 - val_loss: 0.6998 - val_acc: 0.7689\n",
      "Epoch 719/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3815 - acc: 0.8696 - val_loss: 0.7171 - val_acc: 0.7422\n",
      "Epoch 720/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3201 - acc: 0.8963 - val_loss: 0.7287 - val_acc: 0.7378\n",
      "Epoch 721/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3438 - acc: 0.8785 - val_loss: 0.6913 - val_acc: 0.7511\n",
      "Epoch 722/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3585 - acc: 0.8933 - val_loss: 0.7089 - val_acc: 0.7422\n",
      "Epoch 723/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.4191 - acc: 0.8415 - val_loss: 0.7121 - val_acc: 0.7467\n",
      "Epoch 724/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.4193 - acc: 0.8459 - val_loss: 0.7080 - val_acc: 0.7600\n",
      "Epoch 725/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.4000 - acc: 0.8533 - val_loss: 0.7089 - val_acc: 0.7644\n",
      "Epoch 726/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3621 - acc: 0.8726 - val_loss: 0.6899 - val_acc: 0.8044\n",
      "Epoch 727/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3586 - acc: 0.9022 - val_loss: 0.6803 - val_acc: 0.7822\n",
      "Epoch 728/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3690 - acc: 0.8741 - val_loss: 0.6802 - val_acc: 0.7956\n",
      "Epoch 729/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3729 - acc: 0.8800 - val_loss: 0.7213 - val_acc: 0.7422\n",
      "Epoch 730/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3757 - acc: 0.8741 - val_loss: 0.7028 - val_acc: 0.7511\n",
      "Epoch 731/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3839 - acc: 0.8533 - val_loss: 0.7332 - val_acc: 0.7511\n",
      "Epoch 732/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3592 - acc: 0.8756 - val_loss: 0.7202 - val_acc: 0.7422\n",
      "Epoch 733/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3888 - acc: 0.8711 - val_loss: 0.7077 - val_acc: 0.7644\n",
      "Epoch 734/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3889 - acc: 0.8533 - val_loss: 0.7126 - val_acc: 0.7556\n",
      "Epoch 735/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3862 - acc: 0.8593 - val_loss: 0.7142 - val_acc: 0.7689\n",
      "Epoch 736/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3539 - acc: 0.8770 - val_loss: 0.7037 - val_acc: 0.7467\n",
      "Epoch 737/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3647 - acc: 0.8844 - val_loss: 0.7061 - val_acc: 0.7467\n",
      "Epoch 738/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.3886 - acc: 0.8489 - val_loss: 0.6912 - val_acc: 0.7644\n",
      "Epoch 739/10000\n",
      "675/675 [==============================] - 0s 96us/step - loss: 0.3841 - acc: 0.8696 - val_loss: 0.7043 - val_acc: 0.7600\n",
      "Epoch 740/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 0.3957 - acc: 0.8504 - val_loss: 0.7008 - val_acc: 0.7511\n",
      "Epoch 741/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 0.3771 - acc: 0.8652 - val_loss: 0.6971 - val_acc: 0.7511\n",
      "Epoch 742/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 0.3492 - acc: 0.8830 - val_loss: 0.7357 - val_acc: 0.7511\n",
      "Epoch 743/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 0.3739 - acc: 0.8756 - val_loss: 0.7084 - val_acc: 0.7511\n",
      "Epoch 744/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 0.3800 - acc: 0.8489 - val_loss: 0.7097 - val_acc: 0.7644\n",
      "Epoch 745/10000\n",
      "675/675 [==============================] - 0s 99us/step - loss: 0.3679 - acc: 0.8741 - val_loss: 0.6955 - val_acc: 0.7733\n",
      "Epoch 746/10000\n",
      "675/675 [==============================] - 0s 93us/step - loss: 0.3722 - acc: 0.8667 - val_loss: 0.7132 - val_acc: 0.7600\n",
      "Epoch 747/10000\n",
      "675/675 [==============================] - 0s 87us/step - loss: 0.3814 - acc: 0.8696 - val_loss: 0.7100 - val_acc: 0.7644\n",
      "Epoch 748/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3433 - acc: 0.8785 - val_loss: 0.6998 - val_acc: 0.7689\n",
      "Epoch 749/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3921 - acc: 0.8637 - val_loss: 0.7208 - val_acc: 0.7333\n",
      "Epoch 750/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3723 - acc: 0.8593 - val_loss: 0.7263 - val_acc: 0.7333\n",
      "Epoch 751/10000\n",
      "675/675 [==============================] - 0s 77us/step - loss: 0.3457 - acc: 0.8770 - val_loss: 0.7190 - val_acc: 0.7644\n",
      "Epoch 752/10000\n",
      "675/675 [==============================] - 0s 76us/step - loss: 0.3617 - acc: 0.8978 - val_loss: 0.7449 - val_acc: 0.7289\n",
      "Epoch 753/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.3562 - acc: 0.8652 - val_loss: 0.7177 - val_acc: 0.7600\n",
      "Epoch 754/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3716 - acc: 0.8637 - val_loss: 0.7084 - val_acc: 0.7600\n",
      "Epoch 755/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.3701 - acc: 0.8726 - val_loss: 0.7340 - val_acc: 0.7378\n",
      "Epoch 756/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3662 - acc: 0.8770 - val_loss: 0.7273 - val_acc: 0.7467\n",
      "Epoch 757/10000\n",
      "675/675 [==============================] - 0s 91us/step - loss: 0.3517 - acc: 0.8919 - val_loss: 0.7151 - val_acc: 0.7600\n",
      "Epoch 758/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3985 - acc: 0.8563 - val_loss: 0.7371 - val_acc: 0.7422\n",
      "Epoch 759/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3950 - acc: 0.8548 - val_loss: 0.7207 - val_acc: 0.7422\n",
      "Epoch 760/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3337 - acc: 0.8815 - val_loss: 0.7041 - val_acc: 0.7644\n",
      "Epoch 761/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3520 - acc: 0.8785 - val_loss: 0.7303 - val_acc: 0.7244\n",
      "Epoch 762/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 0.4103 - acc: 0.8430 - val_loss: 0.6987 - val_acc: 0.7556\n",
      "Epoch 763/10000\n",
      "675/675 [==============================] - 0s 88us/step - loss: 0.3558 - acc: 0.8756 - val_loss: 0.7509 - val_acc: 0.7511\n",
      "Epoch 764/10000\n",
      "675/675 [==============================] - 0s 89us/step - loss: 0.3941 - acc: 0.8593 - val_loss: 0.7523 - val_acc: 0.7378\n",
      "Epoch 765/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.3377 - acc: 0.8978 - val_loss: 0.7222 - val_acc: 0.7378\n",
      "Epoch 766/10000\n",
      "675/675 [==============================] - 0s 90us/step - loss: 0.3757 - acc: 0.8756 - val_loss: 0.7507 - val_acc: 0.7333\n",
      "Epoch 767/10000\n",
      "675/675 [==============================] - 0s 86us/step - loss: 0.3505 - acc: 0.8756 - val_loss: 0.7098 - val_acc: 0.7600\n",
      "Epoch 768/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3890 - acc: 0.8696 - val_loss: 0.7422 - val_acc: 0.7378\n",
      "Epoch 769/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 80us/step - loss: 0.4119 - acc: 0.8519 - val_loss: 0.6980 - val_acc: 0.7556\n",
      "Epoch 770/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3379 - acc: 0.8800 - val_loss: 0.7156 - val_acc: 0.7511\n",
      "Epoch 771/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3847 - acc: 0.8770 - val_loss: 0.7199 - val_acc: 0.7467\n",
      "Epoch 772/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3825 - acc: 0.8726 - val_loss: 0.7011 - val_acc: 0.7511\n",
      "Epoch 773/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3628 - acc: 0.8874 - val_loss: 0.6937 - val_acc: 0.7644\n",
      "Epoch 774/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3585 - acc: 0.8830 - val_loss: 0.7237 - val_acc: 0.7378\n",
      "Epoch 775/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.3828 - acc: 0.8622 - val_loss: 0.7178 - val_acc: 0.7511\n",
      "Epoch 776/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3703 - acc: 0.8711 - val_loss: 0.7150 - val_acc: 0.7556\n",
      "Epoch 777/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3633 - acc: 0.8696 - val_loss: 0.7082 - val_acc: 0.7422\n",
      "Epoch 778/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3340 - acc: 0.8770 - val_loss: 0.6981 - val_acc: 0.7644\n",
      "Epoch 779/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3828 - acc: 0.8770 - val_loss: 0.6886 - val_acc: 0.7733\n",
      "Epoch 780/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3715 - acc: 0.8756 - val_loss: 0.7011 - val_acc: 0.7556\n",
      "Epoch 781/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3946 - acc: 0.8578 - val_loss: 0.7232 - val_acc: 0.7422\n",
      "Epoch 782/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3335 - acc: 0.8830 - val_loss: 0.6987 - val_acc: 0.7556\n",
      "Epoch 783/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3921 - acc: 0.8578 - val_loss: 0.7033 - val_acc: 0.7689\n",
      "Epoch 784/10000\n",
      "675/675 [==============================] - 0s 85us/step - loss: 0.3490 - acc: 0.8815 - val_loss: 0.7275 - val_acc: 0.7511\n",
      "Epoch 785/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3635 - acc: 0.8696 - val_loss: 0.6971 - val_acc: 0.7822\n",
      "Epoch 786/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3614 - acc: 0.8800 - val_loss: 0.7250 - val_acc: 0.7511\n",
      "Epoch 787/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3687 - acc: 0.8593 - val_loss: 0.7210 - val_acc: 0.7600\n",
      "Epoch 788/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3578 - acc: 0.8889 - val_loss: 0.7614 - val_acc: 0.7422\n",
      "Epoch 789/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3912 - acc: 0.8667 - val_loss: 0.7120 - val_acc: 0.7644\n",
      "Epoch 790/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3697 - acc: 0.8696 - val_loss: 0.7197 - val_acc: 0.7422\n",
      "Epoch 791/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3539 - acc: 0.8756 - val_loss: 0.7251 - val_acc: 0.7378\n",
      "Epoch 792/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3656 - acc: 0.8637 - val_loss: 0.7302 - val_acc: 0.7333\n",
      "Epoch 793/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3577 - acc: 0.8726 - val_loss: 0.7211 - val_acc: 0.7556\n",
      "Epoch 794/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3634 - acc: 0.8830 - val_loss: 0.6969 - val_acc: 0.7600\n",
      "Epoch 795/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3540 - acc: 0.8785 - val_loss: 0.7473 - val_acc: 0.7333\n",
      "Epoch 796/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3746 - acc: 0.8593 - val_loss: 0.7251 - val_acc: 0.7467\n",
      "Epoch 797/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3767 - acc: 0.8696 - val_loss: 0.7189 - val_acc: 0.7689\n",
      "Epoch 798/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3941 - acc: 0.8504 - val_loss: 0.7211 - val_acc: 0.7556\n",
      "Epoch 799/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3680 - acc: 0.8696 - val_loss: 0.7058 - val_acc: 0.7822\n",
      "Epoch 800/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3449 - acc: 0.8726 - val_loss: 0.7209 - val_acc: 0.7600\n",
      "Epoch 801/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3717 - acc: 0.8681 - val_loss: 0.7299 - val_acc: 0.7511\n",
      "Epoch 802/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3767 - acc: 0.8711 - val_loss: 0.7417 - val_acc: 0.7378\n",
      "Epoch 803/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3429 - acc: 0.8785 - val_loss: 0.7186 - val_acc: 0.7511\n",
      "Epoch 804/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3519 - acc: 0.8696 - val_loss: 0.7277 - val_acc: 0.7422\n",
      "Epoch 805/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3500 - acc: 0.8859 - val_loss: 0.7004 - val_acc: 0.7644\n",
      "Epoch 806/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3938 - acc: 0.8489 - val_loss: 0.7109 - val_acc: 0.7511\n",
      "Epoch 807/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3854 - acc: 0.8637 - val_loss: 0.7048 - val_acc: 0.7556\n",
      "Epoch 808/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3621 - acc: 0.8785 - val_loss: 0.7046 - val_acc: 0.7511\n",
      "Epoch 809/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3696 - acc: 0.8830 - val_loss: 0.7193 - val_acc: 0.7422\n",
      "Epoch 810/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3813 - acc: 0.8726 - val_loss: 0.7260 - val_acc: 0.7511\n",
      "Epoch 811/10000\n",
      "675/675 [==============================] - 0s 83us/step - loss: 0.3628 - acc: 0.8652 - val_loss: 0.7309 - val_acc: 0.7378\n",
      "Epoch 812/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3472 - acc: 0.8770 - val_loss: 0.7225 - val_acc: 0.7556\n",
      "Epoch 813/10000\n",
      "675/675 [==============================] - 0s 84us/step - loss: 0.3525 - acc: 0.8830 - val_loss: 0.7552 - val_acc: 0.7556\n",
      "Epoch 814/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3664 - acc: 0.8681 - val_loss: 0.7504 - val_acc: 0.7422\n",
      "Epoch 815/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.3889 - acc: 0.8741 - val_loss: 0.7654 - val_acc: 0.7111\n",
      "Epoch 816/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3507 - acc: 0.8652 - val_loss: 0.7184 - val_acc: 0.7556\n",
      "Epoch 817/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3794 - acc: 0.8607 - val_loss: 0.7494 - val_acc: 0.7511\n",
      "Epoch 818/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3832 - acc: 0.8563 - val_loss: 0.7274 - val_acc: 0.7556\n",
      "Epoch 819/10000\n",
      "675/675 [==============================] - 0s 79us/step - loss: 0.3337 - acc: 0.8978 - val_loss: 0.7254 - val_acc: 0.7689\n",
      "Epoch 820/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3489 - acc: 0.8681 - val_loss: 0.7361 - val_acc: 0.7644\n",
      "Epoch 821/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3571 - acc: 0.8667 - val_loss: 0.7364 - val_acc: 0.7600\n",
      "Epoch 822/10000\n",
      "675/675 [==============================] - 0s 82us/step - loss: 0.3225 - acc: 0.8770 - val_loss: 0.7598 - val_acc: 0.7556\n",
      "Epoch 823/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3429 - acc: 0.8726 - val_loss: 0.7018 - val_acc: 0.7689\n",
      "Epoch 824/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3401 - acc: 0.8815 - val_loss: 0.7281 - val_acc: 0.7556\n",
      "Epoch 825/10000\n",
      "675/675 [==============================] - 0s 81us/step - loss: 0.3482 - acc: 0.8859 - val_loss: 0.7214 - val_acc: 0.7467\n",
      "Epoch 826/10000\n",
      "675/675 [==============================] - 0s 78us/step - loss: 0.3572 - acc: 0.8844 - val_loss: 0.7381 - val_acc: 0.7556\n",
      "Epoch 827/10000\n",
      "675/675 [==============================] - 0s 80us/step - loss: 0.3862 - acc: 0.8533 - val_loss: 0.7286 - val_acc: 0.7556\n",
      "Epoch 828/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 0s 82us/step - loss: 0.3487 - acc: 0.8667 - val_loss: 0.7054 - val_acc: 0.7822\n",
      "Epoch 00828: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Validation split is the data, that will be used for validation of the model. If we set validation_data_split = 0.1,\n",
    "# it means first 90% data will be used for training and last 10% data will be used for validation.\n",
    "\n",
    "validation_data_split = 0.25\n",
    "\n",
    "# Epoch is the time duration needed for an entire dataset to be passed forward and backward \n",
    "# through the neural network once.\n",
    "\n",
    "num_epochs = 10000  \n",
    "\n",
    "# we should not pass the whole dataset in the neural network at the same time. \n",
    "#Rather we should feed it batch by batch.\n",
    "model_batch_size = 128\n",
    "\n",
    "tb_batch_size = 32\n",
    "\n",
    "# early_patience parameter will be used in EarlyStopping function. If set the value as 100, that means, system will\n",
    "# check if the monitored value has stopped imroving over last 100 epochs. If no improvemnet followed, it will stop\n",
    "# the training.\n",
    "early_patience = 100\n",
    "\n",
    "\n",
    "# TensorBoard is a visualization tool. This callback writes a log for TensorBoard, which allows the user to \n",
    "# visualize dynamic graphs of the training and test metrics, as well as activation histograms for the \n",
    "# different layers in the model.\n",
    "\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "\n",
    "# In EarlyStopping callback a monitored quantiry is specified. If it stops improving after certain time, \n",
    "# this callback stops the training.\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "# Read Dataset\n",
    "dataset = pd.read_csv('training.csv')\n",
    "\n",
    "# Process Dataset\n",
    "processedData, processedLabel = processData(dataset)\n",
    "\n",
    "# model fit trains the data according to the parameters given to it.\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Training and Validation Graphs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [1.1528225575553046, 1.1475346989101833, 1.1449391025967068, 1.145316144625346, 1.1482087167104085, 1.1501536300447253, 1.1513349183400472, 1.1463188234965007, 1.148311782942878, 1.1451573408974542, 1.1481768295500014, 1.1471802096896702, 1.1498919253879123, 1.1475100655025905, 1.1456551408767701, 1.145353038046095, 1.1491913731892904, 1.1477203845977784, 1.14848808977339, 1.1440959813859728, 1.1484267960654364, 1.1459545596440632, 1.1430140940348308, 1.1448643573125203, 1.1480898486243354, 1.1494583850436741, 1.1430596526463825, 1.1442279736200969, 1.1443362029393513, 1.140813847647773, 1.1443036052915785, 1.1437014367845324, 1.1447564199235705, 1.1447957134246827, 1.1505116277270846, 1.1439465676413643, 1.1391340435875787, 1.1406379832161797, 1.1385834556155734, 1.1391285456551445, 1.139043434990777, 1.1422366751564874, 1.1372001944647896, 1.1354533682929144, 1.1422789833280775, 1.1391033071941798, 1.1382715426550971, 1.1343330881330702, 1.132069141070048, 1.1300147883097331, 1.134065226978726, 1.134535666571723, 1.1328224515914918, 1.1333719004525078, 1.1272359699673122, 1.1278603267669678, 1.1282595348358155, 1.1304784382714166, 1.1269570170508492, 1.1282212252087063, 1.122986298137241, 1.12285416815016, 1.1191773875554403, 1.1194728512234158, 1.1183551168441772, 1.1217958074145846, 1.1210711786482068, 1.1175488217671712, 1.1204779232872857, 1.1235687753889296, 1.1154850753148398, 1.1152260981665718, 1.1120357163747152, 1.1107364336649577, 1.109213687049018, 1.1134122159745958, 1.108188485039605, 1.109037454922994, 1.1093192768096924, 1.1116634919908313, 1.112212478849623, 1.105498538017273, 1.111832571029663, 1.104042378001743, 1.103704687754313, 1.1038398223453099, 1.1019726626078288, 1.1051805353164672, 1.0989954969618057, 1.1024263339572482, 1.100979191992018, 1.0948254304462008, 1.0945717223485312, 1.093249979548984, 1.0946275308397082, 1.091378803253174, 1.0937766800986395, 1.0921763473086887, 1.0871181991365222, 1.0876139089796277, 1.0864276679356892, 1.0905567248662313, 1.0932459825939602, 1.0861233202616374, 1.0860054339302911, 1.0840725511974758, 1.0824312787585788, 1.0793944719102648, 1.0775137186050414, 1.0839032888412476, 1.0775784466001723, 1.0749007611804537, 1.0764977137247722, 1.0708969412909615, 1.0710218752755059, 1.0775464179780747, 1.0750935464435154, 1.0707213650809393, 1.0691955624686347, 1.0661001751157972, 1.0650784534878202, 1.0630924267239041, 1.069262268808153, 1.0726937029096815, 1.0670900450812446, 1.0642951281865438, 1.0636976252661812, 1.0571906979878742, 1.059551599820455, 1.0614991082085503, 1.0578753561443752, 1.055938334994846, 1.055661202536689, 1.0528996668921577, 1.0527188873291016, 1.0489769570032756, 1.0477693096796672, 1.048243661986457, 1.0479686042997571, 1.0459630590014988, 1.042838185098436, 1.0428032959832085, 1.0392814042833116, 1.0493365383148194, 1.0465485196643405, 1.0415338473849827, 1.0443440845277574, 1.0390844662984213, 1.042525520854526, 1.0350126690334744, 1.034775332874722, 1.0332762516869438, 1.0371797455681695, 1.0354829019970364, 1.0339975913365682, 1.026192421383328, 1.0244930039511786, 1.0247309239705404, 1.0279217688242595, 1.0216344616148207, 1.0196822182337444, 1.02030682987637, 1.0198483520083956, 1.0339925040139093, 1.0193084319432577, 1.0149082008997599, 1.0142083639568753, 1.0189263174268934, 1.016905795203315, 1.013153101073371, 1.0075329960717094, 1.0135699971516927, 1.0027198404735989, 1.0009436432520549, 1.0024693245357936, 1.0145656082365249, 1.0004752763112386, 1.0040603076087105, 0.9962895973523458, 0.9924016986952887, 1.0023379214604695, 0.9962401877509223, 0.9890140093697442, 0.9879621622297499, 0.9864827214346992, 0.9852243656582302, 0.9865565898683336, 0.9814958394898309, 0.9796971209843953, 0.9796836574872335, 0.9800143877665202, 0.9729746929804484, 0.9704403063986037, 0.9725534319877625, 0.972480784257253, 0.9721529467900594, 0.9647983437114291, 0.9685632877879673, 0.9638751957151624, 0.9623451646169027, 0.975802706082662, 0.9679085527526008, 0.9584997818205092, 0.9656619882583618, 0.956454053454929, 0.9601413414213392, 0.9483722496032715, 0.9513193056318495, 0.94999504354265, 0.9507608800464207, 0.9459880018234252, 0.9443300278981527, 0.9488271795378791, 0.9507798928684659, 0.9520538351270887, 0.9392033002111647, 0.9415511367056105, 0.9432849576738146, 0.938716566297743, 0.931753879653083, 0.9344966077804565, 0.9305705025460985, 0.9301372485690647, 0.9323831913206312, 0.9314196382628547, 0.9319937464925978, 0.9300004121992324, 0.9277278335889181, 0.9210364455646939, 0.9171525804201762, 0.9196941759851244, 0.9152868223190308, 0.9170388168758816, 0.9115629630618626, 0.9172373098797268, 0.910403852197859, 0.9130735341707865, 0.9065850938691034, 0.9061154601309035, 0.9058324805895488, 0.9093780443403456, 0.9070000092188517, 0.9076011072264777, 0.8994326384862265, 0.9043716491593254, 0.903745346069336, 0.8922773694992066, 0.8983876935640971, 0.89729410621855, 0.9037586543295119, 0.8971035006311204, 0.8911280194918315, 0.8907142451074388, 0.8950521400239733, 0.892586445013682, 0.8886819741461012, 0.89902811076906, 0.896384850607978, 0.8810769311587016, 0.8808741344345941, 0.8859723353385925, 0.8787226419978672, 0.8766084774335225, 0.8711107452710469, 0.8730091365178426, 0.8727591207292344, 0.881610725455814, 0.8775687975353664, 0.8756856555408902, 0.8742744580904642, 0.8752493556340536, 0.878693786462148, 0.8666949388715955, 0.8701020057996114, 0.8629113732443916, 0.8603793356153701, 0.8598040994008382, 0.8612591263982985, 0.8673194077279832, 0.8576576744185553, 0.865763934718238, 0.8621603698200649, 0.8730082429779901, 0.8545350538359748, 0.8583545827865601, 0.8514935919973585, 0.863428865008884, 0.858075556755066, 0.8638922924465603, 0.8498268596331279, 0.8533917575412326, 0.846761515405443, 0.8519161958164639, 0.8522416522767808, 0.8606888937950135, 0.8474261159367031, 0.846695552666982, 0.8445008924272326, 0.8388465099864536, 0.8453337921036614, 0.835785121652815, 0.8470176222589281, 0.8336058775583903, 0.8382588791847229, 0.8510086112552219, 0.8545864166153802, 0.8319688370492724, 0.8384075530370076, 0.829321043756273, 0.8318185806274414, 0.8364486408233642, 0.8267399213049147, 0.8267397371927897, 0.8323217198583814, 0.8308882761001587, 0.8227487585279677, 0.8180512889226278, 0.8176317124896579, 0.8169407444530064, 0.8348565639389885, 0.8141452095243666, 0.8247901370790269, 0.8329649239116245, 0.8346284641159906, 0.8239668679237365, 0.8151833052105374, 0.8300662607616849, 0.8136493327882555, 0.8125155570771959, 0.8249824449751112, 0.8172247743606568, 0.8100339200761583, 0.8116170189115737, 0.8111062065760295, 0.8258658165401883, 0.8151418781280517, 0.8016054632928636, 0.8024385878774855, 0.8062797077496846, 0.7971198513772753, 0.814122925069597, 0.8027468159463671, 0.8067100273238288, 0.8079486187299093, 0.8033675368626912, 0.7952222723431057, 0.8039375013775296, 0.7932396353615655, 0.7886378725369771, 0.7981640010409885, 0.7973784642749363, 0.807351402176751, 0.7929479686419169, 0.801690702173445, 0.7986753270361159, 0.8050523765881856, 0.8015484264161852, 0.7993376919958326, 0.7977995803621081, 0.799394490983751, 0.7876233636008368, 0.7834310658772786, 0.7798897247844272, 0.8059850774870978, 0.7872693334685431, 0.7984873827298482, 0.7950145422087775, 0.780281708240509, 0.7918924736976624, 0.7757773179478116, 0.7804147325621711, 0.7861574975649516, 0.7926682641771104, 0.7764892329110039, 0.7849393958515591, 0.7968849778175354, 0.7830951823128595, 0.7754168825679355, 0.7745436832639906, 0.7867810710271199, 0.7790733403629727, 0.7852181635962592, 0.7757845115661621, 0.7783029566870795, 0.7764153329531351, 0.7695846234427558, 0.7665994551446703, 0.7653178996509976, 0.7701851156022813, 0.7697429699367947, 0.7768390107154847, 0.7757561530007256, 0.7706713970502218, 0.7778076471222771, 0.7559684138827854, 0.7622554514143202, 0.7678622897466024, 0.7596598267555237, 0.7659593102667067, 0.7829643490579393, 0.778615276283688, 0.7704009379280938, 0.776380315621694, 0.7686347524325053, 0.7770011652840508, 0.7867220717006259, 0.7605162782139249, 0.7538328099250794, 0.7689297408527798, 0.7656254646513198, 0.768054129547543, 0.7683024197154575, 0.7795437574386597, 0.7662678718566894, 0.7737641814019945, 0.7718941020965576, 0.771487857500712, 0.7668526792526245, 0.7495624725023905, 0.7738507994016012, 0.7431968524720933, 0.7495495994885762, 0.7628209842575921, 0.7549740714497036, 0.7490826317999097, 0.7487851577334934, 0.751872874101003, 0.7633914573987325, 0.7585514426231384, 0.7426549728711446, 0.7524551849895054, 0.770911426809099, 0.7479264264636569, 0.746233213212755, 0.7556638622283935, 0.7466520783636305, 0.7588923533757528, 0.7587689860661825, 0.7480704077084859, 0.7546559185451931, 0.7716314339637756, 0.7594653206401402, 0.7531559605068631, 0.7712729504373339, 0.7648706067932977, 0.7513988955815634, 0.7505459952354431, 0.7620174227820502, 0.7453640286127726, 0.7485212937990824, 0.7502960157394409, 0.768659840159946, 0.7417233673731486, 0.7431379458639357, 0.7757190357314215, 0.7462410063213772, 0.7417853858735827, 0.7470158198144701, 0.7262965591748556, 0.7373459778891669, 0.7459627654817369, 0.7429231990708245, 0.7492138555314806, 0.7349839735031128, 0.766443248324924, 0.7462126286824544, 0.7570561718940735, 0.7472406382030911, 0.7349055388238694, 0.7427720136112637, 0.739980669286516, 0.7565478955374824, 0.7398952526516385, 0.7507661912176344, 0.7649112526575724, 0.7391042431195577, 0.7423497139083015, 0.7507130591074626, 0.7371174579196506, 0.7513714045948452, 0.7494850084516738, 0.7437792181968689, 0.7371987024943034, 0.730815474457211, 0.7657101082801819, 0.7769179855452644, 0.7464163422584533, 0.7407188910908169, 0.7350839869181315, 0.7399135319391886, 0.7276738423771328, 0.7388757673899332, 0.7369110706117418, 0.7306047005123563, 0.7344137562645806, 0.7218363846672906, 0.7979785921838548, 0.7362273462613423, 0.7646807127528721, 0.736826646592882, 0.7356572347217136, 0.7550857779714796, 0.7249298024177552, 0.734292905860477, 0.7383381250169542, 0.7265855593151517, 0.7513570046424866, 0.739321837955051, 0.7305673310491774, 0.7295566413137647, 0.728057086997562, 0.7380171261893378, 0.7203258609771729, 0.7401202143563165, 0.7122847554418775, 0.7582869537671407, 0.731568869749705, 0.7099028608534071, 0.7261157777574327, 0.7131925892829895, 0.7153964792357551, 0.7319035241338941, 0.7439873308605618, 0.737296548154619, 0.7411207670635648, 0.7448636558320787, 0.7133904009395176, 0.7557990638415019, 0.7285267053710089, 0.7108588939242892, 0.7184670035044353, 0.7488518275154962, 0.7397844815254211, 0.7530766015582614, 0.7449722690052456, 0.7384802836842007, 0.7619704916742113, 0.7227761809031169, 0.720007213221656, 0.758998450173272, 0.7481191931830512, 0.7276550751262241, 0.7437746535407173, 0.7275007581710815, 0.7162016402350532, 0.7311177553070916, 0.7168481874465943, 0.719900741842058, 0.7437973271475898, 0.7118864194552104, 0.7338922712537977, 0.726173276371426, 0.7333941226535373, 0.7045963531070285, 0.7188824899991353, 0.7149411940574646, 0.7178812005784776, 0.7148159535725912, 0.7096621113353305, 0.7357747403780619, 0.7263249929745992, 0.7154181493653191, 0.7138610821300083, 0.7315776817003886, 0.7521831170717875, 0.7498767775959438, 0.724854916996426, 0.7528664379649692, 0.7238706578148736, 0.7118945275412666, 0.7354261620839437, 0.7266072124905056, 0.7315457601017422, 0.7217904663085938, 0.7335378103786044, 0.7451219738854302, 0.7466666950119867, 0.7597822067472669, 0.7462917110655043, 0.7384030050701565, 0.7262783763143751, 0.728197120030721, 0.7412696716520522, 0.7397632755173578, 0.7302938283814324, 0.7394618007871839, 0.7258074355125427, 0.7382252287864685, 0.7143512524498834, 0.7147216486930847, 0.703831303384569, 0.7055933311250475, 0.7163140148586697, 0.7460298069318135, 0.7145991770426432, 0.7388046884536743, 0.7498185841242472, 0.7647854728168911, 0.7269922150505914, 0.7081614189677768, 0.7565308046340943, 0.7026399967405531, 0.7030080056190491, 0.7137960720062256, 0.702961130672031, 0.7008710355228848, 0.7144036552641126, 0.7040570131937662, 0.7686611559655931, 0.707055057419671, 0.7046002756224738, 0.734803528520796, 0.7328759860992432, 0.7094149075614081, 0.7320945575502183, 0.7073917084270054, 0.7228640336460538, 0.7221389887068007, 0.7188715651300218, 0.7130522264374627, 0.716240496635437, 0.7306213500764634, 0.7033669596248203, 0.7055689732233683, 0.7137109311421712, 0.7376589534017775, 0.719217287434472, 0.7080326122707791, 0.7099065102471246, 0.6994097661972046, 0.7126800566249424, 0.725991829501258, 0.7316958999633789, 0.756077959007687, 0.7218649909231398, 0.7168250947528415, 0.7122923911942376, 0.7136648193995158, 0.7045634979671902, 0.737154226038191, 0.6955417641003927, 0.7216317394044665, 0.695876002046797, 0.6892770833439297, 0.756841504573822, 0.7304351766904195, 0.6947341701719496, 0.7081149217817518, 0.7724346025784811, 0.7428891629642911, 0.7396220217810737, 0.7125210693147448, 0.7067814840210809, 0.7526319498485989, 0.7210600484742059, 0.7167845238579644, 0.7349998982747395, 0.7176293007532756, 0.7182936951849196, 0.7122787512673272, 0.7114263539844089, 0.7139137058787876, 0.7200832809342278, 0.7297074336475796, 0.7088015773561266, 0.7164498835139804, 0.726533985932668, 0.7388711553149753, 0.7167555329534743, 0.7200097433725993, 0.7628353987799751, 0.7036961889266968, 0.7051680175463358, 0.6928360708554586, 0.7024285464816623, 0.7103239941596985, 0.7287475697199504, 0.717427463001675, 0.727709306081136, 0.7125437468952602, 0.728861559761895, 0.7046482316652933, 0.7078125235769483, 0.7595607858233981, 0.7212780443827311, 0.6976657761467828, 0.7089941334724427, 0.6877670690748426, 0.7172261521551344, 0.723944521745046, 0.7225034464730157, 0.7095776293012831, 0.7561637730068631, 0.7101028206613329, 0.7228225768937004, 0.7136954090330336, 0.7009840268558926, 0.7460761819945442, 0.7268845370080737, 0.7327867966228061, 0.7295297622680664, 0.7352334965599908, 0.7007586616939968, 0.7248884129524231, 0.7147163777881198, 0.7206970784399245, 0.7278950656784905, 0.7242688213454352, 0.6931533461146885, 0.7292071069611443, 0.7286169812414381, 0.7059130636850993, 0.7238852718141344, 0.7284918138715956, 0.7340538202391731, 0.7000652715894911, 0.7470082667138841, 0.6997631865077548, 0.7170618049303691, 0.7286851427290175, 0.6913007291158041, 0.7088962745666504, 0.7120540075831943, 0.7080348738034566, 0.7088830582300822, 0.6899379915661282, 0.6802796088324653, 0.6802394713295831, 0.7213211390707228, 0.7028163888719346, 0.7332456109258864, 0.72017998430464, 0.7077410533693101, 0.7126457097795275, 0.7142048083411323, 0.7036753577656216, 0.7060820611317953, 0.6912107947137621, 0.7043464552031623, 0.7007624067200555, 0.6970602530903286, 0.7357035422325134, 0.7083916929033067, 0.7097203302383422, 0.6954544533623589, 0.71320987145106, 0.7100285728772481, 0.699763150744968, 0.7208170578214858, 0.7262926994429695, 0.7190272365676033, 0.7449448614650303, 0.7176869379149543, 0.7083918007214864, 0.7339763980441624, 0.7273145824008518, 0.7150986780060662, 0.7370574567053053, 0.7206687172253926, 0.7041009436713325, 0.730320053630405, 0.6986984221140543, 0.75088337553872, 0.752335606680976, 0.7221913274129231, 0.7506943837801615, 0.7098465625445048, 0.7421724332703484, 0.698030165831248, 0.7155645990371704, 0.7199426709281074, 0.701066591474745, 0.6937313201692369, 0.7236836327446832, 0.717783858511183, 0.715014882352617, 0.7081651168399387, 0.6980717187457615, 0.6886447230974834, 0.7010889005661011, 0.723231840663486, 0.6987023848957485, 0.7033136076397366, 0.7275267492400276, 0.69708242893219, 0.7249714928203159, 0.7209809374809265, 0.7614067305458917, 0.7120128446155124, 0.7197148558828566, 0.7250625639491611, 0.7301879247029622, 0.7210830579863654, 0.6968991327285766, 0.7472631096839905, 0.7250749320454067, 0.7189093354013231, 0.7211078633202447, 0.7058498750792609, 0.7208949123488532, 0.729942937956916, 0.7417203047540453, 0.7186417555809022, 0.7276809766557482, 0.7004313482178582, 0.7108899972173903, 0.7048309032122294, 0.7046118625005087, 0.7192734101083543, 0.7260044712490505, 0.7308609612782796, 0.7225164688958062, 0.7552058217260572, 0.7504231834411621, 0.7653848870595297, 0.7183675010999043, 0.749385507106781, 0.72736419889662, 0.725448378721873, 0.7361225016911824, 0.7363941203223334, 0.7597881086667378, 0.7017858481407165, 0.7281296012136671, 0.7214167886310153, 0.738089272181193, 0.7286229496532016, 0.7054248481326633], 'val_acc': [0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5377777655919392, 0.5422222100363837, 0.5333333211474949, 0.5333333211474949, 0.5377777655919392, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5333333211474949, 0.5377777655919392, 0.5333333211474949, 0.5333333211474949, 0.5377777655919392, 0.5422222100363837, 0.5422222100363837, 0.5377777655919392, 0.5422222100363837, 0.5377777655919392, 0.5422222100363837, 0.5333333211474949, 0.5333333211474949, 0.5422222100363837, 0.5333333211474949, 0.5466666547457377, 0.5422222100363837, 0.5377777655919392, 0.5422222100363837, 0.5466666544808282, 0.5422222100363837, 0.5422222100363837, 0.5377777655919392, 0.5377777655919392, 0.5422222100363837, 0.5511110991901822, 0.5511110991901822, 0.5377777655919392, 0.5555555436346266, 0.5377777655919392, 0.5555555438995361, 0.5511110991901822, 0.559999988079071, 0.5555555436346266, 0.5511110991901822, 0.5422222100363837, 0.5599999883439806, 0.559999988079071, 0.5466666547457377, 0.559999988079071, 0.5511110991901822, 0.5466666547457377, 0.5422222100363837, 0.5555555436346266, 0.5511110991901822, 0.5511110991901822, 0.5511110997200013, 0.564444432788425, 0.559999988079071, 0.5466666550106473, 0.5555555441644456, 0.5466666547457377, 0.5555555438995361, 0.564444432788425, 0.5688888772328695, 0.5599999886088901, 0.5511110991901822, 0.5511110991901822, 0.5644444330533346, 0.5511110991901822, 0.564444432788425, 0.559999988079071, 0.5511110994550917, 0.5511110999849107, 0.5599999886088901, 0.5555555438995361, 0.5599999888737996, 0.5733333216773139, 0.564444432788425, 0.5466666550106473, 0.5555555441644456, 0.5555555438995361, 0.5599999886088901, 0.5511110991901822, 0.5644444325235155, 0.5511110994550917, 0.5599999886088901, 0.564444432788425, 0.559999988079071, 0.5599999886088901, 0.5644444330533346, 0.5644444333182441, 0.5599999883439806, 0.5555555444293552, 0.5644444330533346, 0.5599999888737996, 0.5599999883439806, 0.5599999886088901, 0.5733333219422234, 0.5688888774977789, 0.5599999883439806, 0.5599999883439806, 0.568888878027598, 0.5733333216773139, 0.564444432788425, 0.5644444330533346, 0.5688888774977789, 0.5688888772328695, 0.564444432788425, 0.564444432788425, 0.564444432788425, 0.5688888772328695, 0.573333322207133, 0.573333322736952, 0.5733333219422234, 0.5733333219422234, 0.564444432788425, 0.564444432788425, 0.5822222113609314, 0.5777777663866679, 0.5688888774977789, 0.5733333219422234, 0.5822222113609314, 0.573333322207133, 0.5866666558053758, 0.5777777677112156, 0.5866666555404663, 0.582222211625841, 0.5866666555404663, 0.5866666558053758, 0.5955555454889934, 0.5777777671813965, 0.5733333219422234, 0.5733333219422234, 0.5866666563351949, 0.573333322207133, 0.5822222110960219, 0.6044444338480631, 0.5999999894036188, 0.5822222108311124, 0.5911110999849107, 0.5822222110960219, 0.5911110999849107, 0.5911110999849107, 0.5999999891387092, 0.5955555452240838, 0.5911111002498203, 0.5955555452240838, 0.5911111005147298, 0.5955555444293552, 0.5999999899334377, 0.5999999891387092, 0.5999999891387092, 0.5733333216773139, 0.6133333227369521, 0.6133333230018616, 0.617777767446306, 0.6044444338480631, 0.6133333230018616, 0.5777777663866679, 0.6088888782925076, 0.6088888785574171, 0.6133333230018616, 0.5999999891387092, 0.6266666566001045, 0.6177777677112155, 0.6222222118907504, 0.6222222118907504, 0.5999999891387092, 0.6044444338480631, 0.6133333230018616, 0.6266666566001045, 0.6222222118907504, 0.5866666552755567, 0.617777767446306, 0.5911110999849107, 0.6088888782925076, 0.6177777671813964, 0.6311111010445489, 0.6355555457539028, 0.6311111010445489, 0.626666656865014, 0.62222221215566, 0.6222222118907504, 0.6399999901983473, 0.62222221215566, 0.6355555457539028, 0.6222222118907504, 0.6355555452240838, 0.6311111010445489, 0.6355555452240838, 0.6444444349077013, 0.6311111010445489, 0.6622222132152982, 0.6488888793521457, 0.6488888788223267, 0.6711111023690965, 0.6533333240614997, 0.6311111007796394, 0.6533333235316806, 0.617777766916487, 0.6311111010445489, 0.6266666563351949, 0.6577777682410346, 0.6577777679761251, 0.6355555452240838, 0.617777766916487, 0.6622222126854791, 0.6444444346427918, 0.6533333235316806, 0.6399999899334378, 0.6577777682410346, 0.6488888788223267, 0.6488888790872362, 0.6533333237965901, 0.6577777677112155, 0.6488888790872362, 0.6399999896685282, 0.6444444343778822, 0.6711111018392775, 0.6444444341129727, 0.6622222126854791, 0.6577777679761251, 0.6711111018392775, 0.679999990993076, 0.6755555460188124, 0.6622222124205696, 0.6755555457539029, 0.6711111018392775, 0.6755555465486315, 0.6711111015743679, 0.6755555462837219, 0.6755555462837219, 0.6577777679761251, 0.6755555462837219, 0.6755555465486315, 0.6533333230018615, 0.6622222126854791, 0.6755555460188124, 0.66222221215566, 0.679999990993076, 0.66222221215566, 0.6666666566001045, 0.66222221215566, 0.6311111002498203, 0.6711111013094584, 0.6755555462837219, 0.6799999907281664, 0.6622222118907505, 0.653333322736952, 0.6711111015743679, 0.6666666566001045, 0.6755555454889933, 0.6711111010445489, 0.6444444338480632, 0.6311111005147299, 0.6844444346427917, 0.6666666571299235, 0.7022222132152981, 0.6711111010445489, 0.6488888782925076, 0.6888888790872362, 0.6711111015743679, 0.657777767446306, 0.6755555460188124, 0.6711111010445489, 0.6888888801468743, 0.6888888798819648, 0.6933333248562283, 0.6799999907281664, 0.6977777695655822, 0.6666666566001045, 0.666666656335195, 0.6799999904632569, 0.6711111007796393, 0.6888888798819648, 0.6844444354375203, 0.6933333245913188, 0.6711111010445489, 0.6888888801468743, 0.6844444354375203, 0.7066666584544712, 0.6888888793521457, 0.6977777690357632, 0.6799999907281664, 0.6977777693006727, 0.7111111026340061, 0.7066666584544712, 0.7022222132152981, 0.7111111028989157, 0.6755555454889933, 0.6844444346427917, 0.6977777687708536, 0.6844444343778822, 0.6888888798819648, 0.7066666584544712, 0.6933333235316806, 0.6933333237965902, 0.7111111026340061, 0.6977777687708536, 0.6888888798819648, 0.7022222140100267, 0.7111111028989157, 0.6844444346427917, 0.6933333240614997, 0.6888888796170552, 0.6799999896685283, 0.6844444341129727, 0.6977777679761251, 0.7111111026340061, 0.7111111026340061, 0.7111111023690966, 0.71555554734336, 0.6711111007796393, 0.6888888793521457, 0.702222212685479, 0.6888888785574171, 0.7199999917878045, 0.6844444346427917, 0.7066666579246521, 0.7111111026340061, 0.6977777687708536, 0.6933333237965902, 0.719999992052714, 0.7022222134802076, 0.7111111028989157, 0.6977777687708536, 0.7066666581895616, 0.6888888796170552, 0.6977777690357632, 0.6799999896685283, 0.6888888798819648, 0.6888888796170552, 0.7022222134802076, 0.7066666584544712, 0.7111111026340061, 0.7155555476082696, 0.7066666581895616, 0.71555554734336, 0.6977777690357632, 0.7022222129503886, 0.7022222129503886, 0.7066666579246521, 0.7155555476082696, 0.7155555476082696, 0.7244444359673394, 0.7066666571299235, 0.7066666576597426, 0.71555554734336, 0.7022222132152981, 0.711111102104187, 0.6977777687708536, 0.6977777685059442, 0.7022222129503886, 0.71555554734336, 0.6844444338480632, 0.7022222134802076, 0.7111111026340061, 0.7066666576597426, 0.6933333237965902, 0.6933333240614997, 0.7199999917878045, 0.6977777682410347, 0.6888888790872362, 0.6888888785574171, 0.6933333243264093, 0.71555554734336, 0.6888888788223266, 0.7022222129503886, 0.6977777679761251, 0.719999991522895, 0.724444436762068, 0.6933333232667711, 0.7066666576597426, 0.71555554734336, 0.6977777685059442, 0.6977777685059442, 0.6888888788223266, 0.711111102104187, 0.7244444357024299, 0.6977777677112156, 0.6888888785574171, 0.7244444364971585, 0.706666657394833, 0.702222212685479, 0.7155555468135409, 0.7333333251211378, 0.706666656865014, 0.719999991522895, 0.6977777682410347, 0.6888888785574171, 0.7022222124205695, 0.711111102104187, 0.6844444343778822, 0.706666657394833, 0.7155555465486314, 0.706666657394833, 0.7066666576597426, 0.7022222124205695, 0.6977777679761251, 0.7022222129503886, 0.7022222124205695, 0.7111111018392775, 0.7155555468135409, 0.6888888788223266, 0.719999991522895, 0.7111111026340061, 0.719999991522895, 0.7155555468135409, 0.706666657394833, 0.7155555460188124, 0.7111111018392775, 0.702222212685479, 0.7155555470784505, 0.7111111018392775, 0.7066666576597426, 0.7288888809416029, 0.6977777685059442, 0.711111102104187, 0.7333333256509569, 0.7288888806766934, 0.7111111023690966, 0.711111102104187, 0.711111102104187, 0.6933333235316806, 0.7022222124205695, 0.6977777682410347, 0.7022222124205695, 0.702222212685479, 0.7155555468135409, 0.6977777682410347, 0.7155555465486314, 0.711111102104187, 0.711111102104187, 0.7022222129503886, 0.702222212685479, 0.7155555468135409, 0.7155555465486314, 0.7199999909930759, 0.7022222124205695, 0.7155555470784505, 0.7199999909930759, 0.702222212685479, 0.719999991522895, 0.7022222129503886, 0.7288888809416029, 0.6888888788223266, 0.7111111018392775, 0.7022222124205695, 0.7022222124205695, 0.6977777679761251, 0.711111101574368, 0.7288888812065124, 0.728888881471422, 0.7022222124205695, 0.7199999912579854, 0.7066666571299235, 0.7066666571299235, 0.711111101574368, 0.719999991522895, 0.7155555465486314, 0.711111101574368, 0.724444436232249, 0.7155555465486314, 0.7155555468135409, 0.715555546283722, 0.7199999912579854, 0.7288888806766934, 0.7199999912579854, 0.724444436232249, 0.7244444359673394, 0.7155555465486314, 0.7199999909930759, 0.7199999907281663, 0.7199999909930759, 0.7155555460188124, 0.7288888801468744, 0.7155555465486314, 0.724444436232249, 0.7333333253860473, 0.719999991522895, 0.7155555465486314, 0.711111102104187, 0.7066666576597426, 0.7155555468135409, 0.711111102104187, 0.7111111018392775, 0.7111111023690966, 0.7111111023690966, 0.7244444354375204, 0.7022222124205695, 0.7244444354375204, 0.715555546283722, 0.7244444359673394, 0.7333333248562283, 0.719999991522895, 0.7244444359673394, 0.7244444359673394, 0.7066666576597426, 0.7288888806766934, 0.7155555468135409, 0.7111111023690966, 0.7333333248562283, 0.7377777693006727, 0.711111101574368, 0.711111102104187, 0.7199999907281663, 0.7422222140100267, 0.7333333251211378, 0.715555546283722, 0.7155555460188124, 0.7333333256509569, 0.7111111018392775, 0.7199999907281663, 0.702222212685479, 0.7199999907281663, 0.7199999912579854, 0.6977777687708536, 0.7155555468135409, 0.719999991522895, 0.7199999909930759, 0.7155555468135409, 0.7155555468135409, 0.7199999909930759, 0.7199999912579854, 0.724444436232249, 0.7199999909930759, 0.7155555468135409, 0.711111102104187, 0.7244444359673394, 0.7155555468135409, 0.7244444357024299, 0.706666657394833, 0.711111101574368, 0.7199999912579854, 0.7155555468135409, 0.7244444357024299, 0.7199999909930759, 0.7244444357024299, 0.7244444357024299, 0.7333333256509569, 0.7377777695655823, 0.7422222150696649, 0.7288888804117839, 0.7288888804117839, 0.7333333253860473, 0.7244444359673394, 0.7111111023690966, 0.7111111013094584, 0.7466666584544711, 0.7022222121556599, 0.7244444357024299, 0.7333333248562283, 0.7111111018392775, 0.7333333248562283, 0.7288888801468744, 0.7199999909930759, 0.7466666592491997, 0.7022222121556599, 0.7511111036936442, 0.7288888801468744, 0.7199999904632568, 0.7199999901983473, 0.7377777695655823, 0.7333333251211378, 0.7466666589842902, 0.7199999912579854, 0.7422222142749363, 0.7199999912579854, 0.7511111036936442, 0.7333333248562283, 0.7244444359673394, 0.7422222140100267, 0.7377777700954014, 0.7199999909930759, 0.7244444357024299, 0.7288888804117839, 0.7422222145398458, 0.7288888804117839, 0.7466666589842902, 0.7199999912579854, 0.7199999909930759, 0.7199999909930759, 0.7111111018392775, 0.7244444357024299, 0.7288888798819648, 0.7377777693006727, 0.7244444357024299, 0.7333333248562283, 0.7288888798819648, 0.7466666587193806, 0.7244444354375204, 0.7422222142749363, 0.7377777693006727, 0.7199999909930759, 0.7244444357024299, 0.7422222140100267, 0.7244444357024299, 0.7066666571299235, 0.7155555465486314, 0.715555546283722, 0.7511111031638251, 0.7288888806766934, 0.7155555465486314, 0.7244444359673394, 0.7422222140100267, 0.7155555465486314, 0.7199999907281663, 0.7377777690357632, 0.7333333248562283, 0.7333333245913187, 0.7333333245913187, 0.7244444357024299, 0.7244444359673394, 0.7555555473433601, 0.7244444357024299, 0.7155555460188124, 0.7333333248562283, 0.7422222137451172, 0.7377777695655823, 0.711111102104187, 0.7466666584544711, 0.7244444357024299, 0.7466666589842902, 0.7422222137451172, 0.7333333248562283, 0.7333333248562283, 0.7422222140100267, 0.7377777693006727, 0.7333333245913187, 0.7333333248562283, 0.7466666587193806, 0.7466666587193806, 0.7288888804117839, 0.7288888806766934, 0.7466666589842902, 0.7288888804117839, 0.7555555484029982, 0.7288888801468744, 0.7244444357024299, 0.7377777695655823, 0.7422222142749363, 0.719999991522895, 0.768888881471422, 0.7466666584544711, 0.7466666587193806, 0.773333326180776, 0.7377777698304918, 0.7288888801468744, 0.7377777695655823, 0.7466666581895617, 0.7199999907281663, 0.7555555478731791, 0.7244444354375204, 0.7466666587193806, 0.7333333245913187, 0.7244444359673394, 0.7466666587193806, 0.7599999928474426, 0.7377777693006727, 0.7333333243264092, 0.7466666587193806, 0.7377777693006727, 0.7377777693006727, 0.7244444354375204, 0.7644444370269775, 0.7199999909930759, 0.7688888817363315, 0.7422222142749363, 0.7377777693006727, 0.7511111028989156, 0.7422222140100267, 0.7466666581895617, 0.7599999920527141, 0.764444436762068, 0.8044444388813443, 0.7822222153345744, 0.7955555494626363, 0.7422222137451172, 0.7511111036936442, 0.7511111031638251, 0.7422222145398458, 0.764444437291887, 0.7555555484029982, 0.768888881471422, 0.7466666587193806, 0.7466666584544711, 0.7644444375567966, 0.7599999931123521, 0.751111102634006, 0.751111102634006, 0.7511111028989156, 0.7511111028989156, 0.764444436762068, 0.7733333264456855, 0.7599999920527141, 0.764444436762068, 0.7688888825310601, 0.7333333248562283, 0.7333333243264092, 0.7644444370269775, 0.7288888798819648, 0.7599999920527141, 0.7599999925825331, 0.7377777693006727, 0.7466666581895617, 0.7599999920527141, 0.7422222137451172, 0.7422222137451172, 0.764444437291887, 0.7244444351726108, 0.7555555473433601, 0.751111102634006, 0.7377777690357632, 0.7377777690357632, 0.7333333245913187, 0.7599999920527141, 0.7377777690357632, 0.7555555473433601, 0.7511111034287347, 0.7466666581895617, 0.7511111042234633, 0.764444437291887, 0.7377777698304918, 0.7511111028989156, 0.7555555476082696, 0.7422222134802077, 0.7644444362322489, 0.7733333264456855, 0.7555555473433601, 0.7422222142749363, 0.7555555473433601, 0.768888881471422, 0.7511111028989156, 0.7822222153345744, 0.7511111031638251, 0.7599999925825331, 0.7422222134802077, 0.764444436762068, 0.7422222132152981, 0.7377777695655823, 0.7333333243264092, 0.7555555476082696, 0.7599999928474426, 0.7333333243264092, 0.7466666581895617, 0.768888881471422, 0.7555555476082696, 0.7822222153345744, 0.7599999925825331, 0.7511111031638251, 0.7377777685059441, 0.7511111028989156, 0.7422222140100267, 0.764444437291887, 0.7511111023690965, 0.7555555470784505, 0.7511111023690965, 0.7422222129503886, 0.751111102634006, 0.7377777687708537, 0.7555555476082696, 0.7555555473433601, 0.7422222134802077, 0.711111101574368, 0.7555555473433601, 0.751111102634006, 0.7555555481380887, 0.768888881471422, 0.7644444364971584, 0.7599999920527141, 0.7555555476082696, 0.7688888822661506, 0.7555555473433601, 0.7466666589842902, 0.7555555476082696, 0.7555555476082696, 0.7822222153345744], 'loss': [1.260623601101063, 1.178200129756221, 1.1639923934583312, 1.1523679609651918, 1.1518163168871844, 1.1534528105347246, 1.1522039907949941, 1.1539807273723461, 1.1419056936546608, 1.142493672547517, 1.1420360558121292, 1.1407240772247313, 1.144876068962945, 1.1465285046895346, 1.1347583528801246, 1.1376060572376958, 1.1324936953297369, 1.135721448968958, 1.1359957982875684, 1.1276259950355247, 1.1370669442635994, 1.1310555574629042, 1.1317255954389218, 1.1227771490591543, 1.1263947901902376, 1.1263700088748225, 1.1253173759248523, 1.1247485265025385, 1.1242629243709423, 1.1209141494609691, 1.1252215924086395, 1.1145343895311708, 1.1217525909565114, 1.1124231732333147, 1.11217464349888, 1.1127672382637306, 1.1131179428100586, 1.1089107877236826, 1.109175366472315, 1.1062525189364398, 1.1101693033289026, 1.105969836446974, 1.101623505309776, 1.1066624088640566, 1.104119663591738, 1.0964668671290079, 1.097673216925727, 1.090946307182312, 1.093621410087303, 1.090680390463935, 1.0889420073120681, 1.086685824305923, 1.0869253635406495, 1.0858902795226486, 1.0832628670445195, 1.0846382201159441, 1.083105955477114, 1.0807471043975265, 1.0739593633898983, 1.0777894541952344, 1.0708294806657015, 1.0667586084648415, 1.065301983444779, 1.0673810942967732, 1.06107983606833, 1.0616086714356034, 1.0585301891962686, 1.0564033921559652, 1.0547746854358249, 1.0538178790057147, 1.0487747951790138, 1.05140585343043, 1.0428102090623643, 1.040815124158506, 1.0526651997036405, 1.0435956280319778, 1.039509287233706, 1.0334633207321167, 1.037069319795679, 1.0335825538635255, 1.032122049861484, 1.0268840304127447, 1.0282369087360523, 1.0265790541966757, 1.0245945898691813, 1.0209698315019962, 1.0215792233855636, 1.015808430954262, 1.0141446995735168, 1.007776263554891, 1.0107592462610315, 1.0071692920614173, 1.0025915926474112, 1.0071200906788862, 0.9956991167421694, 1.0049236831841646, 0.9956796626691465, 0.9986246927579244, 1.000922545503687, 0.9976936395080002, 0.9812914523371944, 0.9860826554121794, 0.9748606845184609, 0.9896068972128409, 0.9791772060924107, 0.977519463079947, 0.9783204385969374, 0.9673256428153426, 0.9718946034819992, 0.968956121162132, 0.9641078277870461, 0.9615581997235616, 0.962288414284035, 0.9658492632265444, 0.9547803415192498, 0.9564287777300234, 0.9520780150095621, 0.9522197759592974, 0.9545644957047922, 0.9508711516415631, 0.951383837947139, 0.9422376175279971, 0.9374654741640445, 0.9334211967609547, 0.9426395968154625, 0.9245371802647908, 0.9312308685867875, 0.9236464442147149, 0.9105111269597654, 0.9261426804683827, 0.9214702354537117, 0.9260429245454294, 0.9221925746070014, 0.9181777124051694, 0.9176889124623051, 0.9175099115018491, 0.9122685117191739, 0.9092872309684753, 0.9067632251315647, 0.8964766203032599, 0.9039054238354718, 0.90798395925098, 0.891278328454053, 0.8984573456093117, 0.8929276972346836, 0.8976889657091212, 0.8887954271281207, 0.8924693371631481, 0.8774060976063763, 0.8825822378087926, 0.8651318345246491, 0.8714165170104415, 0.8772698856283118, 0.8843301154949047, 0.871255991547196, 0.8733053474956088, 0.881398831208547, 0.8718645185894436, 0.8703033343067875, 0.8567348957944799, 0.8667090171354789, 0.8491435158694232, 0.851817643907335, 0.8568976945347256, 0.8616518276709098, 0.850472631101255, 0.8553382352546409, 0.8518097318543328, 0.8316639161109924, 0.8476701935132345, 0.8278020794303329, 0.8345690273355555, 0.8348468276306434, 0.8266710621339304, 0.8323564503811024, 0.8282814940699824, 0.8152636987191659, 0.8231621310445998, 0.8231276898030881, 0.8066419935226441, 0.8120341528786553, 0.8178611089565135, 0.8149651322541414, 0.8124284044018498, 0.8019222310737327, 0.8048517714606391, 0.8111997886940285, 0.7971716366873847, 0.7964937791117915, 0.7878643575421086, 0.7976870397285178, 0.7942075968671728, 0.7955302044197365, 0.7925637717600222, 0.7944144415855408, 0.7903900829067937, 0.7959216619420935, 0.7795356975661384, 0.7902724606019479, 0.7790270624337373, 0.7765500607313933, 0.7775398730348657, 0.7767369253547103, 0.7663784884523462, 0.7636564776632521, 0.7675431942056726, 0.781265975104438, 0.7677170196285954, 0.7520423484731603, 0.7350340147371646, 0.7703212018366213, 0.7669079198660674, 0.7431373066372342, 0.7608492384133515, 0.7577564731350651, 0.7536376548696447, 0.7435657023500513, 0.7695146287812127, 0.7515757853013498, 0.7372316688078421, 0.7398253771993849, 0.7411518084561384, 0.73008529248061, 0.7436903222401937, 0.7289315010883191, 0.7161828760747556, 0.7199449499448141, 0.7300850324277525, 0.7171171009982074, 0.729697667669367, 0.7282210088659216, 0.7110725634186356, 0.738270307470251, 0.7369184394235965, 0.7147929689619277, 0.709091614264029, 0.7137520514594184, 0.7125083162166455, 0.7146830443982725, 0.6969384472458451, 0.7022794327912507, 0.7149994910204852, 0.6950686622548986, 0.7060171071688334, 0.7071447533148306, 0.7088904262472082, 0.6908923359270449, 0.6946847091780769, 0.6872462367128442, 0.6921888042379308, 0.6839792435257523, 0.7117049725850423, 0.6977480103351452, 0.6873654670185513, 0.6939240534217269, 0.6799704629403573, 0.6834484859749123, 0.6915039677090115, 0.6841878643742314, 0.6809781741212916, 0.6710630641160188, 0.6760610725261547, 0.6575089850249114, 0.6746309384593258, 0.6729053756042763, 0.6661002269497625, 0.6607304233974881, 0.6763980308285465, 0.6720743182853416, 0.6742988400106077, 0.6852274599781742, 0.669720643626319, 0.6545074061994199, 0.6411158160810118, 0.6744083191730358, 0.6701035331796716, 0.6410573667067069, 0.6603933164808485, 0.6480418958487334, 0.65371275583903, 0.6602040158377753, 0.6665225573822304, 0.65562872533445, 0.649310203216694, 0.6469545383806582, 0.6593163344595168, 0.6462218885951572, 0.6314637366047612, 0.6429329482714335, 0.6360302861531576, 0.6425428499115838, 0.6292722247265004, 0.621647849701069, 0.6255719651999297, 0.6068620605821963, 0.6263532613824915, 0.6278759632287202, 0.6184830159611172, 0.6105161528234129, 0.6208403947618273, 0.6298830003208584, 0.6203771122296651, 0.6247134054148639, 0.6268814839257134, 0.6208370644957931, 0.6347529141991227, 0.6116792915485524, 0.6255216488131771, 0.610027145103172, 0.6175484670533075, 0.6012481334474352, 0.5993927604180795, 0.6050951853504888, 0.6228777090708415, 0.5970964629561812, 0.6073592778488441, 0.6138624566572684, 0.5762398620888038, 0.6157228239377339, 0.5958735735328109, 0.5992776410667985, 0.6045980932977465, 0.6074912874786942, 0.5955447234930815, 0.6077580980901365, 0.5901706223134642, 0.6040597550957291, 0.5947399489084879, 0.5961708437071906, 0.6047439906332228, 0.5973481505005448, 0.5840164261394077, 0.5878262805055688, 0.6029567901293437, 0.6013974776974431, 0.5654831511003, 0.5641597180013304, 0.5731285039583842, 0.5531174856645089, 0.5736497131983439, 0.5837412521574232, 0.574250232996764, 0.5852620150424817, 0.5822506636160392, 0.5613589768056516, 0.5694669535424974, 0.5631820314018815, 0.5614523987416867, 0.5525216658027083, 0.5890623845877471, 0.5742250951131185, 0.5657364534448694, 0.553763270996235, 0.5546908696492513, 0.5715486394917524, 0.5539861442424633, 0.5825152997617369, 0.5425115923528318, 0.5751338858074612, 0.5529071712493896, 0.531975204149882, 0.5367118968787017, 0.5497410496075948, 0.562015358960187, 0.5440731188103005, 0.5613396161132389, 0.5360953458150228, 0.5279865503311157, 0.5553904389452051, 0.5217940836482577, 0.5380907612376743, 0.5508011534478929, 0.5264470936633923, 0.5376913419476261, 0.5529712278754623, 0.526804070296111, 0.5579336043640419, 0.5608086098564996, 0.5490731366916939, 0.5516116692401745, 0.5530030216111077, 0.5233066588860971, 0.5231934040564078, 0.540276161785479, 0.5334099594751994, 0.5177639708695588, 0.5060289502585376, 0.5326291535518788, 0.5497847648020144, 0.5073776468524226, 0.5254948863276729, 0.5325231042173174, 0.5088532681818362, 0.5640654064107824, 0.5116750443423236, 0.5357631725735135, 0.5323820470880579, 0.5206189171473186, 0.5336984322689198, 0.5282131644531532, 0.5086440481962982, 0.5260181417288604, 0.5264283957746294, 0.5088677549362183, 0.49758398206145676, 0.5060830906143895, 0.5097912638275711, 0.5064942240715027, 0.5257360871191378, 0.5307459941616764, 0.5097840916668928, 0.4907342941672714, 0.4867199646102058, 0.5090501742009763, 0.5354854302053098, 0.5050580722314341, 0.501657901958183, 0.517335534007461, 0.48584961582113195, 0.5004203148682912, 0.49123333189222546, 0.5159133203382845, 0.47768877144213073, 0.49759818399394, 0.5425829066612102, 0.4948320961440051, 0.509380854677271, 0.507835999418188, 0.5031825061197635, 0.5140891820413095, 0.4806651509249652, 0.49069356741728604, 0.5014222538029706, 0.49429699217831646, 0.5132665046056112, 0.49404557991910864, 0.4702384309415464, 0.5062846408508442, 0.5048539922414003, 0.4785401048925188, 0.48040581901868185, 0.48408269030076484, 0.5048898718533692, 0.4959230346149868, 0.49034428274189984, 0.49289758214244134, 0.4800181288189358, 0.49094351830305877, 0.48275676254872923, 0.5071262709741239, 0.4845241723237214, 0.47742646323310006, 0.4791963745046545, 0.4862795836837203, 0.47869889078316863, 0.47390039925222044, 0.4522768332340099, 0.5183008030608848, 0.4734416891910412, 0.49237033896976046, 0.4790784818154794, 0.4773523732467934, 0.4748053808123977, 0.45146405352486507, 0.46597633719444276, 0.4572935915434802, 0.46952101839913263, 0.4698126368610947, 0.470837203308388, 0.4848866285218133, 0.46647396679277775, 0.4679177474092554, 0.4660534100179319, 0.47279351627385174, 0.4663348756013093, 0.46605400571116695, 0.47896049159544485, 0.4577817580876527, 0.4802479782810918, 0.4668484029063472, 0.46251502399091365, 0.48022957342642325, 0.4696386294894748, 0.46161357954696375, 0.48876523053204574, 0.4712135660648346, 0.4595942402769018, 0.45743716774163423, 0.4485012699939587, 0.49098112282929596, 0.47359216235302115, 0.4516934479607476, 0.46911487446890937, 0.4499456166338038, 0.47269965480875087, 0.4823250911853932, 0.4636515808546985, 0.46784928498444733, 0.4588733024508865, 0.4688861073387994, 0.46928215349162067, 0.4439115947264212, 0.4465717937769713, 0.4533688013641923, 0.43222185841313115, 0.4446604572843622, 0.463224181510784, 0.4619538465694145, 0.46958074786044934, 0.41952847511680036, 0.46557215129887614, 0.45804945994306495, 0.41297905281738, 0.44327226947855064, 0.45367481708526614, 0.44368069701724583, 0.465689353413052, 0.45921772480010986, 0.4530447174884655, 0.46263747533162436, 0.4475169348716736, 0.45522248780285873, 0.45254292452776873, 0.4230453301800622, 0.4345386498062699, 0.4301635992968524, 0.44100206225006666, 0.4386422113577525, 0.4481150015636727, 0.43872628768285116, 0.4539447885530966, 0.42946221934424506, 0.4171651069764738, 0.431709137890074, 0.4306964118392379, 0.44186250377584385, 0.45787988486113373, 0.4254229966799418, 0.40699926760461597, 0.4530752302540673, 0.42970193650987415, 0.42534053489013957, 0.44007375165268225, 0.4216153904243752, 0.45631396284809816, 0.4185643019058086, 0.42944867814028703, 0.4346567330537019, 0.4268580909128542, 0.43277610438841363, 0.4400578741232554, 0.40314323491520354, 0.42177675203040793, 0.44269311101348313, 0.4432588709725274, 0.401412281371929, 0.41409758426524973, 0.4247236440799854, 0.44604422008549727, 0.4375670348714899, 0.4429461235028726, 0.40686881767378913, 0.4266128510457498, 0.42314517899795817, 0.42264234586998267, 0.4135208194344132, 0.43461269497871396, 0.4116431102929292, 0.4294503915309906, 0.4435025185567361, 0.4295275052388509, 0.41917418515240706, 0.47531498608765776, 0.4484277749944616, 0.44334027352156463, 0.41008598071557506, 0.43906391342480977, 0.44601432252813267, 0.44431293213808976, 0.4618117382349791, 0.4204064397458677, 0.4402011963173195, 0.4269104823359737, 0.4235080060252437, 0.4259584762431957, 0.42021162002174944, 0.4455206353576095, 0.42446956943582603, 0.41453278064727783, 0.41939235506234346, 0.42371108377421346, 0.4105461080869039, 0.42080847784324926, 0.41668923046853806, 0.39880889570271527, 0.41036383235896073, 0.39881414501755325, 0.4016723253550353, 0.40551457581696687, 0.4063864616111473, 0.41403611227318093, 0.43270990362873785, 0.4136877561940087, 0.3954861118175365, 0.4376939708215219, 0.4047082322615164, 0.44738702239813627, 0.38024286676336216, 0.43066351855242696, 0.40586741111896657, 0.383524402335838, 0.37334918057477035, 0.4127867274814182, 0.45043277749308835, 0.4064308100718039, 0.44054385944649027, 0.4027482683128781, 0.36961546655054445, 0.41514099973219415, 0.3971890364752875, 0.40030661900838216, 0.38515412864861664, 0.4034264557449906, 0.3861187222710362, 0.39263226248599864, 0.4108376509613461, 0.36101855013105605, 0.40210351762948215, 0.39452832888673856, 0.4270155922130302, 0.41682635691430836, 0.36919562688580265, 0.3934548669391208, 0.40018009539003724, 0.4070112229276586, 0.42495444439075614, 0.4101855724829215, 0.4145288194991924, 0.3859198863418014, 0.4015614423486922, 0.4013538133215021, 0.4292501185116944, 0.3868084781258195, 0.38315748938807737, 0.37679400267424407, 0.3934726756148868, 0.41541923231548733, 0.4067760697338316, 0.3904849382241567, 0.42157960653305054, 0.39178788591314245, 0.3923843509179574, 0.3802785381122872, 0.3924254077893716, 0.39766160395410327, 0.3749073592821757, 0.39725686011490996, 0.38741382479667663, 0.41131307557777125, 0.38653885814878675, 0.3972721713119083, 0.3492037553257412, 0.38040794686034873, 0.41168047512019124, 0.38562924866323117, 0.4071999309239564, 0.3954856790436639, 0.40744894442734897, 0.38295604639583164, 0.3708718468083276, 0.393822685568421, 0.3939459362736455, 0.38514118203410397, 0.37561497688293455, 0.36940356294314064, 0.3687330090116572, 0.38521294320071187, 0.409981205507561, 0.3785297388059122, 0.38247733597402217, 0.39974125221923545, 0.3837462752395206, 0.3927794859144423, 0.39216157343652513, 0.3816131963994768, 0.41109842812573466, 0.3855152659062986, 0.3749961660526417, 0.3830581263701121, 0.38606856081220836, 0.4302208894270438, 0.4110077025272228, 0.39047127436708523, 0.3973050148398788, 0.33598028286739634, 0.3748117490167971, 0.37629634102185566, 0.3800546737511953, 0.3781199817304258, 0.4129367251749392, 0.401195520957311, 0.38460938184349625, 0.35801385482152304, 0.3747487340591572, 0.41901095619908085, 0.40342686891555785, 0.3929825938189471, 0.4094306857497604, 0.4034190341719875, 0.36407948573430376, 0.36818456636534797, 0.3618151456779904, 0.40297439248473554, 0.3547323817676968, 0.3351727029570827, 0.35740703225135806, 0.3655762035758407, 0.3816145721629814, 0.3814635686962693, 0.32013789671438714, 0.3438247451517317, 0.35850713862313166, 0.4191300998352192, 0.41926501477206196, 0.3999997106967149, 0.3620909540741532, 0.35857835699010776, 0.36903542116836263, 0.3728909428031356, 0.3757355574325279, 0.3838716102529455, 0.3592405587214011, 0.3887839325710579, 0.38889162959875884, 0.38617600319562134, 0.3539055640609176, 0.36467294812202455, 0.3885912397172716, 0.3840935437767594, 0.3957391533145198, 0.3770746355145066, 0.3491647794953099, 0.3738609814202344, 0.38002929552837655, 0.36787580993440416, 0.3721579693423377, 0.3813928336125833, 0.34333604062045064, 0.3921463574303521, 0.3723210762165211, 0.34566184216075474, 0.36167557844409237, 0.3562377933219627, 0.3716158229333383, 0.3700718330453943, 0.36622120521686696, 0.35165640336495857, 0.39853402543950966, 0.3949870901637607, 0.33372635457250804, 0.35203935309692663, 0.41033228079477946, 0.355820397845021, 0.39408173437471744, 0.3376735430072855, 0.37573630761217186, 0.35049588596379316, 0.388994850759153, 0.4118708891338772, 0.33787696489581354, 0.38473027728222037, 0.38252454139568187, 0.3628384558801298, 0.35854458729426064, 0.38278819040015893, 0.37026699838814914, 0.3632566300586418, 0.3340208116725639, 0.3828499995337592, 0.3715480344825321, 0.3945935846258093, 0.33351098669899837, 0.3921310356369725, 0.3489883367220561, 0.36352244240266307, 0.36135909142317596, 0.3687497679392497, 0.35781847962626706, 0.39121766258169105, 0.36968242353863184, 0.3539108848130261, 0.3655786498387655, 0.35774096263779537, 0.3634086717499627, 0.353962973886066, 0.3746305294831594, 0.37669372964788367, 0.39411314262284175, 0.36796271955525434, 0.34493538781448646, 0.3717447105160466, 0.3767194617236102, 0.3428974485838855, 0.35190889314368917, 0.3500089305418509, 0.39378425187534755, 0.3853816677022863, 0.36210852366906626, 0.36961880348346854, 0.3812539510373716, 0.36280976246904445, 0.3472445892404627, 0.3525164951218499, 0.36638917406400046, 0.3889204711384243, 0.350694967508316, 0.37936635114528516, 0.383209397704513, 0.333665699561437, 0.34893619537353515, 0.3570878495551922, 0.32247468963817316, 0.34287292237635014, 0.34005913668208654, 0.3481875401073032, 0.3571511495554889, 0.3861896748012967, 0.3487136809031169], 'acc': [0.45185185264658045, 0.5333333333774849, 0.5333333334657882, 0.5288888890213437, 0.5333333333774849, 0.5333333347461842, 0.533333334657881, 0.5333333347461842, 0.5333333348344873, 0.5333333333774849, 0.533333334657881, 0.5333333347461842, 0.5318518532647027, 0.5333333347461842, 0.5333333348344873, 0.5333333335540913, 0.5333333347461842, 0.533333334657881, 0.5333333347461842, 0.5333333335540913, 0.5333333343929715, 0.5333333348344873, 0.5333333333774849, 0.5333333345695778, 0.5333333347461842, 0.533333334657881, 0.5333333334657882, 0.5333333335540913, 0.5318518530880963, 0.5333333335540913, 0.5333333334657882, 0.5333333348344873, 0.5333333345695778, 0.533333334657881, 0.5333333343929715, 0.5333333344812746, 0.5333333333774849, 0.5348148162276657, 0.5348148148589664, 0.533333334657881, 0.5333333336423944, 0.5333333335540913, 0.533333334657881, 0.5333333345695778, 0.5348148158744529, 0.5333333348344873, 0.5333333334657882, 0.5392592604072005, 0.533333334657881, 0.5333333347461842, 0.5348148163159688, 0.5362962975325408, 0.5333333334657882, 0.534814815123876, 0.5303703718715244, 0.5392592605838069, 0.5348148162276657, 0.5362962970910249, 0.5348148157861498, 0.536296297620844, 0.5348148159627562, 0.5362962964287511, 0.5348148163159688, 0.5362962977091471, 0.5318518532647027, 0.5377777779102325, 0.5422222237233763, 0.5437037050282514, 0.5348148163159688, 0.5392592593034109, 0.5437037037478553, 0.5348148158744529, 0.5407407407848923, 0.5362962963404478, 0.5422222235467699, 0.5437037038361585, 0.5422222234584667, 0.5481481492960895, 0.5392592593034109, 0.5422222237233763, 0.5496296310424804, 0.5481481492960895, 0.5481481494726959, 0.5585185195781567, 0.5422222237233763, 0.5525925940054435, 0.5481481496493021, 0.5511111126122651, 0.5600000010596381, 0.5540740741182256, 0.5688888890213436, 0.5525925939171402, 0.5585185199313694, 0.5614814826294228, 0.5540740751337122, 0.5525925936522307, 0.5585185198430662, 0.5688888900368302, 0.5600000014128509, 0.5600000012362445, 0.5718518529997931, 0.5644444453274762, 0.5674074079372264, 0.5511111126122651, 0.5629629638459948, 0.5688888903900429, 0.5688888889330405, 0.567407408731955, 0.5792592604955037, 0.5748148149472696, 0.5837037052048577, 0.5822222232818604, 0.5748148149472696, 0.5614814825411196, 0.5703703714300085, 0.5777777791906287, 0.5748148163159689, 0.582222223105254, 0.5659259270738672, 0.5822222233701635, 0.5733333348344873, 0.586666667814608, 0.5733333345695778, 0.5925925938288371, 0.5985185198430661, 0.5940740748688027, 0.5896296309541773, 0.6000000008830317, 0.5837037047633419, 0.5896296309541773, 0.6044444458572953, 0.5911111123473556, 0.5911111124356587, 0.6148148156095434, 0.5925925935639276, 0.5703703716066149, 0.586666667814608, 0.600000000971335, 0.5896296307775709, 0.5851851864214297, 0.5970370383615847, 0.5822222234584667, 0.5955555569684063, 0.6118518533530058, 0.6148148162276657, 0.6044444458572953, 0.6088888903017398, 0.5985185196664599, 0.6088888903900429, 0.628148149560999, 0.6207407421535915, 0.6103703715183116, 0.6133333348344874, 0.6222222230169508, 0.6133333345695778, 0.6177777785725064, 0.6103703714300085, 0.6029629638459948, 0.6133333338631524, 0.6192592602305942, 0.6133333341280619, 0.6207407416237725, 0.6118518530880963, 0.6192592605838069, 0.6444444455040825, 0.6370370378317657, 0.6385185194015502, 0.6355555564385873, 0.6325925937405339, 0.6474074086436519, 0.6400000012362445, 0.637037038008372, 0.6340740752220154, 0.6577777790140222, 0.6444444457689921, 0.6325925939171403, 0.6681481492077863, 0.6503703713417053, 0.6592592600539878, 0.669629630777571, 0.6637037037478553, 0.6755555565268905, 0.669629630336055, 0.6385185198430662, 0.6696296306009646, 0.6622222232818603, 0.6548148155212402, 0.6696296305126614, 0.6725925938288371, 0.6874074083787424, 0.6711111124356588, 0.6725925933873211, 0.6800000010596381, 0.6711111121707493, 0.6666666679912143, 0.6681481493843926, 0.684444445239173, 0.6800000000441516, 0.6918518532647027, 0.6888888898602238, 0.6770370380966751, 0.6844444455040826, 0.6829629639342979, 0.7140740749571058, 0.6903703711650989, 0.6859259271621704, 0.6740740753986217, 0.6755555567918, 0.7051851858033075, 0.6948148156095434, 0.6844444455040826, 0.681481482717726, 0.7155555566151937, 0.7066666675496984, 0.688888889948527, 0.7140740753986218, 0.6977777791023254, 0.6844444455923857, 0.7185185194015503, 0.7214814823645133, 0.6874074084670455, 0.7274074080255296, 0.7111111121707493, 0.7096296305126614, 0.7111111117292334, 0.7244444448859604, 0.7140740751337122, 0.7214814823645133, 0.7244444454157793, 0.7081481492960895, 0.7362962972676312, 0.7362962974442376, 0.7185185194898535, 0.7037037045867355, 0.7288888896836175, 0.7229629640226011, 0.7303703709884927, 0.7155555563502841, 0.7244444453274762, 0.730370371165099, 0.7392592600539879, 0.7229629636693884, 0.7422222229286476, 0.717037038008372, 0.7200000007947286, 0.734814815874453, 0.7407407415354693, 0.7407407416237726, 0.7348148156978466, 0.7451851864214297, 0.7466666674613953, 0.7244444454157793, 0.7392592597890784, 0.737777778925719, 0.7496296305126614, 0.7600000005298191, 0.751111111552627, 0.7362962971793281, 0.7407407416237726, 0.7570370379200688, 0.7600000003532127, 0.7422222230169508, 0.7525925931224117, 0.7525925934756243, 0.7600000007947286, 0.7437037046750387, 0.7733333340397588, 0.7555555560853746, 0.7451851860682169, 0.7333333341280619, 0.7377777785725064, 0.7570370380966752, 0.7629629636693884, 0.7733333343929715, 0.7259259267206545, 0.749629630336055, 0.7718518525582773, 0.7511111120824461, 0.7659259267206545, 0.7674074079372265, 0.7570370379200688, 0.7481481488545736, 0.7540740755752281, 0.7555555559970715, 0.7688888897719207, 0.7525925931224117, 0.7570370377434624, 0.7570370375668561, 0.7629629637576916, 0.7748148156095435, 0.7585185194898535, 0.7940740746921963, 0.7718518527348837, 0.7762962967378122, 0.7896296305126614, 0.7748148153446339, 0.7762962974442376, 0.7881481488545735, 0.7851851859799138, 0.7703703710767957, 0.7540740753103186, 0.770370371165099, 0.7644444451508698, 0.777777778837416, 0.7733333338631524, 0.76148148227621, 0.7629629636693884, 0.7792592600539878, 0.8000000007064254, 0.7866666671081826, 0.7881481486779672, 0.779259259612472, 0.7881481488545735, 0.7748148152563307, 0.7837037045867354, 0.7866666673730921, 0.7881481489428768, 0.7970370375668561, 0.7614814819229974, 0.7985185191366407, 0.7955555562619809, 0.7837037041452196, 0.7866666673730921, 0.7807407410939534, 0.7703703709001894, 0.7807407414471662, 0.7792592598773815, 0.7925925930341085, 0.7970370376551593, 0.7762962973559344, 0.788148149119483, 0.7955555567034969, 0.8162962968261154, 0.7955555559970714, 0.7792592601422911, 0.8207407412705598, 0.8000000006181223, 0.7985185191366407, 0.8311111116409302, 0.800000000529819, 0.7881481486779672, 0.7866666671081826, 0.7866666673730921, 0.7925925930341085, 0.8014814818346941, 0.7985185189600345, 0.8014814820996037, 0.8118518523816709, 0.8029629633161757, 0.7881481492077863, 0.7851851859799138, 0.7866666675496985, 0.8162962968261154, 0.822222222575435, 0.7911111117292333, 0.8162962970910249, 0.7851851857150043, 0.8162962971793281, 0.780740741358863, 0.788148149119483, 0.8103703709884926, 0.8148148156095434, 0.8000000004415159, 0.8074074080255297, 0.808888889418708, 0.79407407451559, 0.8059259265440482, 0.8118518526465804, 0.7985185191366407, 0.8281481489428767, 0.8029629635810852, 0.7881481490311799, 0.8177777779543841, 0.8014814821879069, 0.804444444709354, 0.8251851857150042, 0.7970370373019465, 0.8088888895953144, 0.8177777783075968, 0.8029629638459947, 0.7881481488545735, 0.8074074079372264, 0.8118518522933678, 0.808888889418708, 0.8074074080255297, 0.8281481488545736, 0.8355555560853746, 0.8296296300711455, 0.8133333340397587, 0.832592593299018, 0.8162962967378122, 0.8088888892421016, 0.820740741358863, 0.7807407417120757, 0.8296296304243582, 0.8385185191366408, 0.8281481488545736, 0.832592593299018, 0.797037038008372, 0.8133333340397587, 0.8311111116409302, 0.8000000004415159, 0.8177777780426874, 0.808888889418708, 0.8222222229286477, 0.8296296300711455, 0.8192592603188974, 0.8177777783075968, 0.828148148589664, 0.8162962969144185, 0.8148148153446338, 0.8311111118175365, 0.8503703708118863, 0.8000000006181223, 0.8088888895953144, 0.8251851858033075, 0.8103703709884926, 0.8207407411822566, 0.844444444709354, 0.8266666669315762, 0.8222222226637381, 0.8237037041452195, 0.8400000003532128, 0.8133333339514556, 0.8074074077606201, 0.8207407411822566, 0.8237037041452195, 0.8266666671081826, 0.8340740742506805, 0.8325925930341085, 0.8385185192249439, 0.8177777784842032, 0.8340740744272868, 0.8162962968261154, 0.808888889418708, 0.8222222223988286, 0.8385185195781566, 0.8192592596124719, 0.8296296299828423, 0.8459259263674418, 0.8444444448859604, 0.8296296300711455, 0.8177777783075968, 0.8340740746921963, 0.8148148151680276, 0.8088888896836175, 0.8325925931224116, 0.8370370373019466, 0.8414814815697846, 0.8281481484130577, 0.8266666671964857, 0.8340740747804994, 0.8311111118175365, 0.8400000002649095, 0.8325925932107149, 0.831111111552627, 0.8622222225754349, 0.8266666671081826, 0.8459259265440482, 0.8325925931224116, 0.8414814820113006, 0.8325925930341085, 0.8311111113760207, 0.8488888896836175, 0.8503703706352799, 0.8533333338631524, 0.8414814818346942, 0.8385185188717312, 0.8266666675496984, 0.8148148158744529, 0.8414814825411197, 0.8311111121707493, 0.8503703709884927, 0.8340740746921963, 0.8370370373019466, 0.8237037040569164, 0.8370370373902497, 0.8548148151680275, 0.8266666670198793, 0.8207407414471661, 0.8429629635810852, 0.8340740747804994, 0.8400000003532128, 0.8400000006181222, 0.8207407416237725, 0.8400000004415159, 0.8340740746038932, 0.8429629633161757, 0.8414814820113006, 0.8192592594358656, 0.8355555560853746, 0.8533333337748492, 0.8311111117292334, 0.8592592597890784, 0.8118518523816709, 0.8311111114643238, 0.848888889418708, 0.8133333339514556, 0.841481481746391, 0.8385185188717312, 0.8340740746038932, 0.8548148151680275, 0.8518518522933677, 0.8518518522050645, 0.8548148151680275, 0.8562962962962963, 0.8385185188717312, 0.8311111113760207, 0.8311111113760207, 0.854814815432937, 0.8325925929458053, 0.8414814818346942, 0.8533333335099397, 0.8562962969144186, 0.8444444448859604, 0.8651851855383978, 0.832592592769199, 0.8311111116409302, 0.8385185189600344, 0.8400000004415159, 0.8414814818346942, 0.8474074082904391, 0.8474074079372265, 0.8488888893304047, 0.8548148149914212, 0.8518518522050645, 0.8355555559970714, 0.8562962969144186, 0.8488888893304047, 0.8459259266323513, 0.8444444447976571, 0.850370371165099, 0.8637037039686133, 0.8459259264557449, 0.8429629633161757, 0.8370370375668561, 0.8414814820113006, 0.842962963492782, 0.8711111113760206, 0.8503703706352799, 0.8607407410939535, 0.8414814820113006, 0.8400000003532128, 0.8533333339514556, 0.8429629637576915, 0.8637037041452196, 0.8681481486779672, 0.8459259261908355, 0.8577777784842032, 0.8414814819229973, 0.8311111113760207, 0.856296296649509, 0.8474074077606201, 0.837037037478553, 0.8444444449742635, 0.8577777783959, 0.8681481485013609, 0.8548148149914212, 0.8370370375668561, 0.8400000004415159, 0.8266666671964857, 0.8577777783959, 0.8518518520284583, 0.842962963492782, 0.8459259264557449, 0.8548148155212403, 0.8400000006181222, 0.8577777783959, 0.8429629633161757, 0.8533333337748492, 0.8429629634044788, 0.8518518524699741, 0.8325925929458053, 0.8429629633161757, 0.832592593299018, 0.8666666671964858, 0.8444444449742635, 0.8488888893304047, 0.8518518526465805, 0.8414814818346942, 0.8385185186951248, 0.8518518523816709, 0.8444444451508699, 0.8577777781309905, 0.8548148149914212, 0.869629629806236, 0.8355555560853746, 0.8385185189600344, 0.8592592594358656, 0.8577777782192937, 0.8488888895953143, 0.8607407410056502, 0.8503703709001894, 0.856296296649509, 0.8651851855383978, 0.8622222223988286, 0.8607407410939535, 0.8637037042335227, 0.8518518522933677, 0.8607407411822566, 0.8592592597007751, 0.8400000001766064, 0.8607407410939535, 0.8666666669315762, 0.8592592597890784, 0.8577777783959, 0.841481481746391, 0.8785185191366408, 0.8503703709001894, 0.854814815432937, 0.8637037041452196, 0.8800000001766064, 0.8355555559087683, 0.8311111122590524, 0.8696296298945392, 0.8474074082904391, 0.8770370373902497, 0.8800000002649095, 0.8577777779543841, 0.8666666670198794, 0.8666666672847889, 0.8755555557321619, 0.8725925933873212, 0.8740740744272868, 0.8622222223988286, 0.8607407411822566, 0.882962963492782, 0.8562962970027217, 0.8548148151680275, 0.8533333337748492, 0.8414814818346942, 0.8651851857150042, 0.8666666669315762, 0.8548148150797243, 0.8577777784842032, 0.8474074078489233, 0.8548148149914212, 0.860740741358863, 0.8814814815697847, 0.8651851857150042, 0.8637037042335227, 0.8311111114643238, 0.8711111117292334, 0.8725925929458053, 0.8800000002649095, 0.8637037040569164, 0.8725925926808957, 0.8562962963845995, 0.866666666843273, 0.8503703708118863, 0.8725925928575021, 0.8518518525582772, 0.8740740743389837, 0.8592592594358656, 0.8474074081138329, 0.8651851855383978, 0.8622222223988286, 0.8548148155212403, 0.860740741358863, 0.8711111112877175, 0.8607407410939535, 0.8814814819229974, 0.8711111113760206, 0.8518518522933677, 0.8711111114643239, 0.8651851856267011, 0.8548148152563307, 0.8518518524699741, 0.8770370374785529, 0.868148148589664, 0.8725925928575021, 0.8681481485013609, 0.8637037044984323, 0.8888888891537984, 0.8844444446210508, 0.884444444709354, 0.8637037041452196, 0.8459259265440482, 0.8740740744272868, 0.8622222225754349, 0.8459259263674418, 0.863703704321826, 0.8725925928575021, 0.8548148153446339, 0.8681481485013609, 0.8814814818346942, 0.8607407412705598, 0.890370371165099, 0.8770370372136433, 0.8651851855383978, 0.8474074082904391, 0.8577777783075968, 0.8607407411822566, 0.8622222224871318, 0.8918518520284582, 0.868148148589664, 0.8844444447976572, 0.8681481485013609, 0.8725925930341085, 0.8444444450625667, 0.8666666671081825, 0.8711111111994143, 0.8755555559087682, 0.8592592595241688, 0.8311111119058398, 0.8548148152563307, 0.8607407410939535, 0.8548148152563307, 0.8592592597007751, 0.8800000002649095, 0.8651851856267011, 0.8814814820113005, 0.8562962968261154, 0.8681481483247545, 0.8918518520284582, 0.8637037040569164, 0.8548148153446339, 0.859259259612472, 0.8696296304243583, 0.8962962969144186, 0.8785185189600344, 0.8933333337748492, 0.8414814815697846, 0.8459259264557449, 0.8533333334216365, 0.8725925932990181, 0.9022222225754349, 0.8740740745155899, 0.8800000006181222, 0.8740740745155899, 0.8533333342163651, 0.8755555561736778, 0.8711111114643239, 0.8533333336865461, 0.8592592592592593, 0.8770370376551593, 0.8844444447976572, 0.8488888892421016, 0.8696296301594487, 0.8503703710767958, 0.8651851856267011, 0.8829629632278725, 0.8755555559087682, 0.8488888891537985, 0.8740740744272868, 0.8666666671081825, 0.8696296299828423, 0.8785185189600344, 0.8637037039686133, 0.8592592598773815, 0.8770370375668561, 0.8977777781309905, 0.8651851853617916, 0.8637037042335227, 0.8725925932107148, 0.8770370375668561, 0.8918518520284582, 0.8562962970910248, 0.8548148153446339, 0.8814814818346942, 0.8785185188717313, 0.8429629637576915, 0.8755555558204651, 0.8592592597890784, 0.8977777779543842, 0.8755555558204651, 0.8755555558204651, 0.8696296299828423, 0.8518518522050645, 0.8800000004415159, 0.8770370372136433, 0.8725925930341085, 0.8874074078489232, 0.8829629635810852, 0.8622222223988286, 0.8711111114643239, 0.8696296300711455, 0.8770370371253402, 0.8770370374785529, 0.8755555559087682, 0.8577777783959, 0.8829629634044789, 0.8577777780426873, 0.8814814818346942, 0.8696296301594487, 0.8800000004415159, 0.8592592595241688, 0.8888888891537984, 0.8666666672847889, 0.8696296299828423, 0.8755555559087682, 0.86370370388031, 0.8725925929458053, 0.8829629634044789, 0.8785185189600344, 0.8592592598773815, 0.8696296302477519, 0.8503703707235831, 0.8696296300711455, 0.8725925930341085, 0.8681481485013609, 0.8711111114643239, 0.8785185186951249, 0.8696296298945392, 0.8859259262791387, 0.8488888893304047, 0.8637037042335227, 0.878518518783428, 0.8829629631395693, 0.8725925929458053, 0.8651851857150042, 0.8770370373902497, 0.8829629635810852, 0.8681481486779672, 0.8740740746038932, 0.8651851854500947, 0.8607407409173471, 0.8562962970027217, 0.8977777781309905, 0.8681481484130577, 0.8666666671081825, 0.8770370372136433, 0.8725925930341085, 0.8814814821879069, 0.8859259266323514, 0.8844444447976572, 0.8533333335982428, 0.8666666670198794]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1a333ca470>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1a336769b0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1a336a0f28>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1a336d34e0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The history attribute from history object is a record of training loss values and metrics values at \n",
    "# successive epochs, as well as validation loss values and validation metrics values. It can be used to plot graphs\n",
    "# which let us understand how good the training was performed over the epochs.\n",
    "\n",
    "\n",
    "print(history.history)\n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot(subplots=True, grid=True, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = blue>Testing Accuracy [Software 2.0]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeLabel(encodedLabel):\n",
    "    if encodedLabel == 0:\n",
    "        return \"Other\"\n",
    "    elif encodedLabel == 1:\n",
    "        return \"Fizz\"\n",
    "    elif encodedLabel == 2:\n",
    "        return \"Buzz\"\n",
    "    elif encodedLabel == 3:\n",
    "        return \"FizzBuzz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 18  Correct :82\n",
      "Testing Accuracy: 82.0\n"
     ]
    }
   ],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "testData = pd.read_csv('testing.csv')\n",
    "\n",
    "processedTestData  = encodeData(testData['input'].values)\n",
    "processedTestLabel = encodeLabel(testData['label'].values)\n",
    "predictedTestLabel = []\n",
    "\n",
    "for i,j in zip(processedTestData,processedTestLabel):\n",
    "    y = model.predict(np.array(i).reshape(-1,10))\n",
    "    predictedTestLabel.append(decodeLabel(y.argmax()))\n",
    "    \n",
    "    if j.argmax() == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))\n",
    "\n",
    "# Please input your UBID and personNumber \n",
    "testDataInput = testData['input'].tolist()\n",
    "testDataLabel = testData['label'].tolist()\n",
    "\n",
    "testDataInput.insert(0, \"UBID\")\n",
    "testDataLabel.insert(0, \"amlangup\")\n",
    "\n",
    "testDataInput.insert(1, \"personNumber\")\n",
    "testDataLabel.insert(1, \"50288686\")\n",
    "\n",
    "predictedTestLabel.insert(0, \"\")\n",
    "predictedTestLabel.insert(1, \"\")\n",
    "\n",
    "output = {}\n",
    "output[\"input\"] = testDataInput\n",
    "output[\"label\"] = testDataLabel\n",
    "\n",
    "output[\"predicted_label\"] = predictedTestLabel\n",
    "\n",
    "opdf = pd.DataFrame(output)\n",
    "opdf.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAcc = 0.0\n",
    "maxIter = 0\n",
    "C_Lambda = 0.03\n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "M = 5\n",
    "PHI = []\n",
    "IsSynthetic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n",
    "\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
    "\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        \n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            \n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "def generate_phis(_RawData, _TrainingData, _ValData, _TestData):\n",
    "    kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(_TrainingData))\n",
    "    Mu = kmeans.cluster_centers_\n",
    "\n",
    "    _BigSigma     = GenerateBigSigma(_RawData, Mu, TrainingPercent,IsSynthetic)\n",
    "    print(\"BigSigma: \" + str(_BigSigma.shape))\n",
    "    \n",
    "    _TRAINING_PHI = GetPhiMatrix(_RawData, Mu, _BigSigma, TrainingPercent)\n",
    "    print(\"Training Phi: \" + str(_TRAINING_PHI.shape))\n",
    "    \n",
    "    _TEST_PHI     = GetPhiMatrix(_TestData, Mu, _BigSigma, 100) \n",
    "    print(\"Testing Phi: \" + str(_TEST_PHI.shape))\n",
    "    \n",
    "    _VAL_PHI      = GetPhiMatrix(_ValData, Mu, _BigSigma, 100)\n",
    "    print(\"Validation Phi: \" + str(_VAL_PHI.shape))\n",
    "    \n",
    "    \n",
    "    return _TRAINING_PHI, _TEST_PHI, _VAL_PHI\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes dataset containing pairs and \n",
    "# returns a dataframe of n*21 dimention containing their image specifications \n",
    "# It performs feature concatination, i.e. It takes two sets of 9 features for each image\n",
    "# and concats them into 18 features\n",
    "\n",
    "def merge_data_set_feature_concatenation(image_specs, pair_set):\n",
    "    df3 = pd.merge(pair_set, image_specs ,left_on = \"img_id_A\", right_on = \"img_id\",how=\"inner\")\n",
    "    df4 = pd.merge(df3, image_specs ,left_on = 'img_id_B', right_on = 'img_id',how=\"inner\")\n",
    "    \n",
    "    targets = df4['target']\n",
    "\n",
    "    _RawTarget = targets.values\n",
    "    datasetStriped = df4.drop(['img_id_A', 'img_id_B', 'target','img_id_x', 'img_id_y'], axis=1)\n",
    "    \n",
    "    uniques = datasetStriped.apply(lambda x: x.nunique())\n",
    "    datasetStriped = datasetStriped.drop(uniques[uniques==1].index, axis=1)\n",
    "    \n",
    "    _RawData = np.transpose(datasetStriped.values)\n",
    "    \n",
    "    return _RawTarget, _RawData\n",
    "\n",
    "# This function takes dataset containing pairs and \n",
    "# returns a dataframe of n*12 dimention containing their image specifications \n",
    "# It performs feature subtraction, i.e. It takes two sets of 9 features for each image\n",
    "# and subtracts them and return 9 features\n",
    "\n",
    "def merge_data_set_feature_subtraction(image_specs, pair_set):\n",
    "    targets = pair_set['target']\n",
    "    \n",
    "    df3 = pd.merge(pair_set, image_specs ,left_on = \"img_id_A\", right_on = \"img_id\",how=\"inner\")\n",
    "    df3 = df3.drop(['img_id_A', 'img_id_B', 'target','img_id'], axis=1)\n",
    "    \n",
    "\n",
    "    df4 = pd.merge(pair_set, image_specs ,left_on = 'img_id_B', right_on = 'img_id',how=\"inner\")\n",
    "    df4 = df4.drop(['img_id_A', 'img_id_B', 'target','img_id'], axis=1)\n",
    "    \n",
    "    df5 = df3.sub(df4)\n",
    "    df5 = df5.abs()\n",
    "    \n",
    "    uniques = df5.apply(lambda x: x.nunique())\n",
    "    df5 = df5.drop(uniques[uniques==1].index, axis=1)\n",
    "    \n",
    "    temp_np = df5.values\n",
    "    \n",
    "#     temp_np = np.absolute(np.subtract(df3.values, df4.values))\n",
    "\n",
    "    _RawTarget = targets.values\n",
    "    _RawData = np.transpose(temp_np)\n",
    "    \n",
    "    return _RawTarget, _RawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_validation_testing_ds(RawTarget, RawData):\n",
    "    _TrainingTarget = GenerateTrainingTarget(RawTarget,TrainingPercent)\n",
    "    _TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent)\n",
    "    print(\"Training Target: \"+ str(_TrainingTarget.shape))\n",
    "    print(\"Training Data: \"+ str(_TrainingData.shape))\n",
    "\n",
    "    _ValDataAct = GenerateValTargetVector(RawTarget,ValidationPercent, (len(_TrainingTarget)))\n",
    "    _ValData    = GenerateValData(RawData,ValidationPercent, (len(_TrainingTarget)))\n",
    "    print(\"Validation Target: \"+ str(_ValDataAct.shape))\n",
    "    print(\"Validation Data: \"+ str(_ValData.shape))\n",
    "\n",
    "    _TestDataAct = GenerateValTargetVector(RawTarget,TestPercent, (len(_TrainingTarget)+len(_ValDataAct)))\n",
    "    _TestData = GenerateValData(RawData,TestPercent, (len(_TrainingTarget)+len(_ValDataAct)))\n",
    "    print(\"Testing Target: \"+ str(_TestDataAct.shape))\n",
    "    print(\"Testing Data: \"+ str(_TestData.shape))\n",
    "    \n",
    "    return _TrainingTarget, _TrainingData, _ValDataAct, _ValData, _TestDataAct, _TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(_loop_range, _TrainingTarget, _TRAINING_PHI, _ValDataAct, _VAL_PHI, _TestDataAct, _TEST_PHI):\n",
    "\n",
    "\n",
    "    W_Now        = np.ones(M)\n",
    "\n",
    "    La           = 2\n",
    "    learningRate = 0.01\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    L_Acc_Val   = []\n",
    "    L_Acc_TR    = []\n",
    "    L_Acc_Test  = []\n",
    "    W_Mat        = []\n",
    "\n",
    "    for i in range(0,_loop_range):\n",
    "\n",
    "        # here we are calculating the weight changes we have to get a more optimized solution\n",
    "        # for that we have to calculate the error changes first and combining with learning rate\n",
    "        # we decide what will be the new weights for next iteration.\n",
    "\n",
    "        Delta_E_D     = -np.dot((_TrainingTarget[i] - np.dot(np.transpose(W_Now), _TRAINING_PHI[i])), _TRAINING_PHI[i])\n",
    "        La_Delta_E_W  = np.dot(La,W_Now)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "        W_T_Next      = W_Now + Delta_W\n",
    "        W_Now         = W_T_Next\n",
    "\n",
    "\n",
    "\n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest( _TRAINING_PHI, W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT, _TrainingTarget)\n",
    "        L_Acc_TR.append(float(Erms_TR.split(',')[0]))\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest( _VAL_PHI, W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT, _ValDataAct)\n",
    "        L_Acc_Val.append(float(Erms_Val.split(',')[0]))\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest( _TEST_PHI, W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT, _TestDataAct)\n",
    "        L_Acc_Test.append(float(Erms_Test.split(',')[0]))\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "        \n",
    "    print ('----------Gradient Descent Solution--------------------')\n",
    "    print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "    print (\"Accuracy Training   = \" + str(np.around(max(L_Acc_TR),5)))\n",
    "    print (\"Accuracy Validation = \" + str(np.around(max(L_Acc_Val),5)))\n",
    "    print (\"Accuracy Testing    = \" + str(np.around(max(L_Acc_Test),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging_gradient_descent_hod(RawTarget, RawData):  \n",
    "    TrainingTarget, TrainingData, ValDataAct, ValData, TestDataAct, TestData = generate_training_validation_testing_ds(RawTarget, RawData)\n",
    "    TRAINING_PHI, TEST_PHI, VAL_PHI = generate_phis(RawData, TrainingData, ValData, TestData)\n",
    "    stochastic_gradient_descent(TrainingTarget.shape[0], TrainingTarget, TRAINING_PHI, ValDataAct, VAL_PHI, TestDataAct, TEST_PHI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_specs_df = pd.read_csv(\"HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\", index_col=0)\n",
    "\n",
    "same_pair_df = pd.read_csv(\"HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\")\n",
    "\n",
    "diff_pair_df = pd.read_csv(\"HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\")\n",
    "\n",
    "\n",
    "# create a dataframe by taking a subset from samples of different writers\n",
    "# Since unmatched dataset is huge we are randomly creating a sample with same data size (791 rows)\n",
    "# because if the amount of unmatched data overwhelms the matched data, the model may overfit\n",
    "\n",
    "diff_pair_df_sample = diff_pair_df.sample(n=same_pair_df.shape[0], replace=True)\n",
    "\n",
    "#Merging same and different writer's data set into one. (1582 rows)\n",
    "dataset_pairs = pd.concat([diff_pair_df_sample,same_pair_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "== Human Observed Data ==\n",
      "==== Concatenation ======\n",
      "=========================\n",
      "RawData : (18, 1582)\n",
      "Training Target: (1266,)\n",
      "Training Data: (18, 1266)\n",
      "Validation Target: (158,)\n",
      "Validation Data: (18, 158)\n",
      "Testing Target: (157,)\n",
      "Testing Data: (18, 157)\n",
      "BigSigma: (18, 18)\n",
      "Training Phi: (1266, 5)\n",
      "Testing Phi: (157, 5)\n",
      "Validation Phi: (158, 5)\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms Training   = 0.49964\n",
      "E_rms Validation = 0.49857\n",
      "E_rms Testing    = 0.48941\n",
      "Accuracy Training   = 55.13428\n",
      "Accuracy Validation = 57.59494\n",
      "Accuracy Testing    = 61.78344\n"
     ]
    }
   ],
   "source": [
    "# Now we are merging the image specifications with the dataset, we will use this dataset to train\n",
    "# test or validate data.\n",
    "\n",
    "RawTarget, RawData = merge_data_set_feature_concatenation(image_specs_df, dataset_pairs)\n",
    "print(\"=========================\")\n",
    "print(\"== Human Observed Data ==\")\n",
    "print(\"==== Concatenation ======\")\n",
    "print(\"=========================\")\n",
    "print(\"RawData : \" + str(RawData.shape))\n",
    "\n",
    "staging_gradient_descent_hod(RawTarget, RawData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "== Human Observed Data ==\n",
      "==== Subtraction ======\n",
      "=========================\n",
      "RawData : (9, 1582)\n",
      "Training Target: (1266,)\n",
      "Training Data: (9, 1266)\n",
      "Validation Target: (158,)\n",
      "Validation Data: (9, 158)\n",
      "Testing Target: (157,)\n",
      "Testing Data: (9, 157)\n",
      "BigSigma: (9, 9)\n",
      "Training Phi: (1266, 5)\n",
      "Testing Phi: (157, 5)\n",
      "Validation Phi: (158, 5)\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms Training   = 0.49992\n",
      "E_rms Validation = 0.49988\n",
      "E_rms Testing    = 0.49116\n",
      "Accuracy Training   = 51.34281\n",
      "Accuracy Validation = 53.79747\n",
      "Accuracy Testing    = 62.42038\n"
     ]
    }
   ],
   "source": [
    "targets, RawData = merge_data_set_feature_subtraction(image_specs_df, dataset_pairs)\n",
    "print(\"=========================\")\n",
    "print(\"== Human Observed Data ==\")\n",
    "print(\"==== Subtraction ======\")\n",
    "print(\"=========================\")\n",
    "print(\"RawData : \" + str(RawData.shape))\n",
    "\n",
    "staging_gradient_descent_hod(RawTarget, RawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Structural Concavity (GSC) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging_gradient_descent_gscd(RawTarget, RawData):  \n",
    "    TrainingTarget, TrainingData, ValDataAct, ValData, TestDataAct, TestData = generate_training_validation_testing_ds(RawTarget, RawData)\n",
    "    TRAINING_PHI, TEST_PHI, VAL_PHI = generate_phis(RawData, TrainingData, ValData, TestData)\n",
    "    stochastic_gradient_descent(800, TrainingTarget, TRAINING_PHI, ValDataAct, VAL_PHI, TestDataAct, TEST_PHI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_specs_df = pd.read_csv(\"GSC-Dataset/GSC-Features-Data/GSC-Features.csv\")\n",
    "\n",
    "same_pair_df = pd.read_csv(\"GSC-Dataset/GSC-Features-Data/same_pairs.csv\")\n",
    "\n",
    "diff_pair_df = pd.read_csv(\"GSC-Dataset/GSC-Features-Data/diffn_pairs.csv\")\n",
    "\n",
    "\n",
    "# create a dataframe by taking a subset from samples of different writers\n",
    "# Since unmatched dataset is huge we are randomly creating a sample with same data size (791 rows)\n",
    "# because if the amount of unmatched data overwhelms the matched data, the model may overfit\n",
    "\n",
    "diff_pair_df_sample = diff_pair_df.sample(n=same_pair_df.shape[0], replace=True)\n",
    "\n",
    "#Merging same and different writer's data set into one. (1582 rows)\n",
    "dataset_pairs = pd.concat([diff_pair_df_sample,same_pair_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "====== GSC Data =========\n",
      "==== Concatenation ======\n",
      "=========================\n",
      "RawData : (1017, 143062)\n",
      "Training Target: (114450,)\n",
      "Training Data: (1017, 114450)\n",
      "Validation Target: (14306,)\n",
      "Validation Data: (1017, 14306)\n",
      "Testing Target: (14305,)\n",
      "Testing Data: (1017, 14305)\n",
      "BigSigma: (1017, 1017)\n",
      "Training Phi: (114450, 5)\n",
      "Testing Phi: (14305, 5)\n",
      "Validation Phi: (14306, 5)\n"
     ]
    }
   ],
   "source": [
    "# Now we are merging the image specifications with the dataset, we will use this dataset to train\n",
    "# test or validate data.\n",
    "\n",
    "RawTarget, RawData = merge_data_set_feature_concatenation(image_specs_df, dataset_pairs)\n",
    "\n",
    "print(\"=========================\")\n",
    "print(\"====== GSC Data =========\")\n",
    "print(\"==== Concatenation ======\")\n",
    "print(\"=========================\")\n",
    "print(\"RawData : \" + str(RawData.shape))\n",
    "\n",
    "staging_gradient_descent_gscd(RawTarget, RawData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "====== GSC Data =========\n",
      "==== Subtraction ======\n",
      "=========================\n",
      "RawData : (509, 143062)\n",
      "Training Target: (114450,)\n",
      "Training Data: (509, 114450)\n",
      "Validation Target: (14306,)\n",
      "Validation Data: (509, 14306)\n",
      "Testing Target: (14305,)\n",
      "Testing Data: (509, 14305)\n",
      "BigSigma: (509, 509)\n",
      "Training Phi: (114450, 5)\n",
      "Testing Phi: (14305, 5)\n",
      "Validation Phi: (14306, 5)\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms Training   = 0.52341\n",
      "E_rms Validation = 0.52097\n",
      "E_rms Testing    = 0.52378\n",
      "Accuracy Training   = 50.07427\n",
      "Accuracy Validation = 50.71299\n",
      "Accuracy Testing    = 50.42992\n"
     ]
    }
   ],
   "source": [
    "# Now we are merging the image specifications with the dataset, we will use this dataset to train\n",
    "# test or validate data.\n",
    "\n",
    "RawTarget, RawData = merge_data_set_feature_subtraction(image_specs_df, dataset_pairs)\n",
    "\n",
    "print(\"=========================\")\n",
    "print(\"====== GSC Data =========\")\n",
    "print(\"==== Subtraction ======\")\n",
    "print(\"=========================\")\n",
    "print(\"RawData : \" + str(RawData.shape))\n",
    "\n",
    "staging_gradient_descent_gscd(RawTarget, RawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

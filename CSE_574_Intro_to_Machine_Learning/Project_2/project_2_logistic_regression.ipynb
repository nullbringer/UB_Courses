{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "\n",
    "# This function takes dataset containing pairs and \n",
    "# returns a dataframe of n*21 dimention containing their image specifications \n",
    "# It performs feature concatination, i.e. It takes two sets of 9 features for each image\n",
    "# and concats them into 18 features\n",
    "\n",
    "def merge_data_set_feature_concatenation(image_specs, pair_set):\n",
    "    df3 = pd.merge(pair_set, image_specs ,left_on = \"img_id_A\", right_on = \"img_id\",how=\"inner\")\n",
    "    df4 = pd.merge(df3, image_specs ,left_on = 'img_id_B', right_on = 'img_id',how=\"inner\")\n",
    "    \n",
    "    targets = df4['target']\n",
    "\n",
    "    _RawTarget = targets.values\n",
    "    datasetStriped = df4.drop(['img_id_A', 'img_id_B', 'target','img_id_x', 'img_id_y'], axis=1)\n",
    "    \n",
    "    uniques = datasetStriped.apply(lambda x: x.nunique())\n",
    "    datasetStriped = datasetStriped.drop(uniques[uniques==1].index, axis=1)\n",
    "    \n",
    "    _RawData = np.transpose(datasetStriped.values)\n",
    "    \n",
    "    return _RawTarget, _RawData\n",
    "\n",
    "# This function takes dataset containing pairs and \n",
    "# returns a dataframe of n*12 dimention containing their image specifications \n",
    "# It performs feature subtraction, i.e. It takes two sets of 9 features for each image\n",
    "# and subtracts them and return 9 features\n",
    "\n",
    "def merge_data_set_feature_subtraction(image_specs, pair_set):\n",
    "    targets = pair_set['target']\n",
    "    \n",
    "    df3 = pd.merge(pair_set, image_specs ,left_on = \"img_id_A\", right_on = \"img_id\",how=\"inner\")\n",
    "    df3 = df3.drop(['img_id_A', 'img_id_B', 'target','img_id'], axis=1)\n",
    "    \n",
    "\n",
    "    df4 = pd.merge(pair_set, image_specs ,left_on = 'img_id_B', right_on = 'img_id',how=\"inner\")\n",
    "    df4 = df4.drop(['img_id_A', 'img_id_B', 'target','img_id'], axis=1)\n",
    "    \n",
    "    df5 = df3.sub(df4)\n",
    "    df5 = df5.abs()\n",
    "    \n",
    "    uniques = df5.apply(lambda x: x.nunique())\n",
    "    df5 = df5.drop(uniques[uniques==1].index, axis=1)\n",
    "    \n",
    "    temp_np = df5.values\n",
    "    \n",
    "\n",
    "    _RawTarget = targets.values\n",
    "    _RawData = np.transpose(temp_np)\n",
    "    \n",
    "    return _RawTarget, _RawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_validation_testing_ds(RawTarget, RawData):\n",
    "    _TrainingTarget = GenerateTrainingTarget(RawTarget,TrainingPercent)\n",
    "    _TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent)\n",
    "    print(\"Training Target: \"+ str(_TrainingTarget.shape))\n",
    "    print(\"Training Data: \"+ str(_TrainingData.shape))\n",
    "\n",
    "    _ValDataAct = GenerateValTargetVector(RawTarget,ValidationPercent, (len(_TrainingTarget)))\n",
    "    _ValData    = GenerateValData(RawData,ValidationPercent, (len(_TrainingTarget)))\n",
    "    print(\"Validation Target: \"+ str(_ValDataAct.shape))\n",
    "    print(\"Validation Data: \"+ str(_ValData.shape))\n",
    "\n",
    "    _TestDataAct = GenerateValTargetVector(RawTarget,TestPercent, (len(_TrainingTarget)+len(_ValDataAct)))\n",
    "    _TestData = GenerateValData(RawData,TestPercent, (len(_TrainingTarget)+len(_ValDataAct)))\n",
    "    print(\"Testing Target: \"+ str(_TestDataAct.shape))\n",
    "    print(\"Testing Data: \"+ str(_TestData.shape))\n",
    "    \n",
    "    return _TrainingTarget, _TrainingData, _ValDataAct, _ValData, _TestDataAct, _TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_logistic_regression(_loop_range, _TrainingTarget, _TrainingData, _ValDataAct, _ValData, _TestDataAct, _TestData):\n",
    "    W = np.ones(_TrainingData.shape[0])\n",
    "    W_NOW = np.transpose(W)\n",
    "    learning_rate = 0.01\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    L_Acc_Val   = []\n",
    "    L_Acc_TR    = []\n",
    "    L_Acc_Test  = []\n",
    "\n",
    "    for i in range(_loop_range):\n",
    "\n",
    "        z = np.dot(W_NOW, _TrainingData)\n",
    "        predicted_values = sigmoid(z)\n",
    "\n",
    "        diff = np.subtract(predicted_values, _TrainingTarget) #need to check np.substract\n",
    "\n",
    "        Delta_W = np.dot(_TrainingData, diff)/_TrainingTarget.shape[0]\n",
    "\n",
    "        W_NOW = W_NOW - (learning_rate*Delta_W)\n",
    "\n",
    "            #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = sigmoid(np.dot(W_NOW, _TrainingData))\n",
    "        Erms_TR       = GetErms(TR_TEST_OUT, _TrainingTarget)\n",
    "        L_Acc_TR.append(float(Erms_TR.split(',')[0]))\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = sigmoid(np.dot(W_NOW, _ValData))\n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT, _ValDataAct)\n",
    "        L_Acc_Val.append(float(Erms_Val.split(',')[0]))\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = sigmoid(np.dot(W_NOW, _TestData))\n",
    "        Erms_Test = GetErms(TEST_OUT, _TestDataAct)\n",
    "        L_Acc_Test.append(float(Erms_Test.split(',')[0]))\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "\n",
    "    print ('----------Gradient Descent Solution--------------------')\n",
    "    print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "    print (\"Accuracy Training   = \" + str(np.around(max(L_Acc_TR),5)))\n",
    "    print (\"Accuracy Validation = \" + str(np.around(max(L_Acc_Val),5)))\n",
    "    print (\"Accuracy Testing    = \" + str(np.around(max(L_Acc_Test),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_specs_df = pd.read_csv(\"HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\", index_col=0)\n",
    "\n",
    "same_pair_df = pd.read_csv(\"HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\")\n",
    "\n",
    "diff_pair_df = pd.read_csv(\"HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\")\n",
    "\n",
    "\n",
    "# create a dataframe by taking a subset from samples of different writers\n",
    "# Since unmatched dataset is huge we are randomly creating a sample with same data size (791 rows)\n",
    "# because if the amount of unmatched data overwhelms the matched data, the model may overfit\n",
    "\n",
    "diff_pair_df_sample = diff_pair_df.sample(n=same_pair_df.shape[0], replace=True)\n",
    "\n",
    "#Merging same and different writer's data set into one. (1582 rows)\n",
    "dataset_pairs = pd.concat([diff_pair_df_sample,same_pair_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "== Human Observed Data ==\n",
      "==== Concatenation ======\n",
      "=========================\n",
      "RawData : (18, 1582)\n",
      "RawTarget : (1582,)\n",
      "Training Target: (1266,)\n",
      "Training Data: (18, 1266)\n",
      "Validation Target: (158,)\n",
      "Validation Data: (18, 158)\n",
      "Testing Target: (157,)\n",
      "Testing Data: (18, 157)\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms Training   = 0.49421\n",
      "E_rms Validation = 0.50435\n",
      "E_rms Testing    = 0.49684\n",
      "Accuracy Training   = 55.52923\n",
      "Accuracy Validation = 60.75949\n",
      "Accuracy Testing    = 58.59873\n"
     ]
    }
   ],
   "source": [
    "# Now we are merging the image specifications with the dataset, we will use this dataset to train\n",
    "# test or validate data.\n",
    "\n",
    "RawTarget, RawData = merge_data_set_feature_concatenation(image_specs_df, dataset_pairs)\n",
    "print(\"=========================\")\n",
    "print(\"== Human Observed Data ==\")\n",
    "print(\"==== Concatenation ======\")\n",
    "print(\"=========================\")\n",
    "print(\"RawData : \" + str(RawData.shape))\n",
    "print(\"RawTarget : \" + str(RawTarget.shape))\n",
    "\n",
    "TrainingTarget, TrainingData, ValDataAct, ValData, TestDataAct, TestData = generate_training_validation_testing_ds(\n",
    "                                                                            RawTarget, RawData)\n",
    "\n",
    "execute_logistic_regression(1000, TrainingTarget, TrainingData, ValDataAct, ValData, TestDataAct, TestData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "== Human Observed Data ==\n",
      "==== Subtraction ======\n",
      "=========================\n",
      "RawData : (9, 1582)\n",
      "RawTarget : (1582,)\n",
      "Training Target: (1266,)\n",
      "Training Data: (9, 1266)\n",
      "Validation Target: (158,)\n",
      "Validation Data: (9, 158)\n",
      "Testing Target: (157,)\n",
      "Testing Data: (9, 157)\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms Training   = 0.50591\n",
      "E_rms Validation = 0.49828\n",
      "E_rms Testing    = 0.50724\n",
      "Accuracy Training   = 51.18483\n",
      "Accuracy Validation = 58.86076\n",
      "Accuracy Testing    = 56.05096\n"
     ]
    }
   ],
   "source": [
    "# Now we are merging the image specifications with the dataset, we will use this dataset to train\n",
    "# test or validate data.\n",
    "\n",
    "RawTarget, RawData = merge_data_set_feature_subtraction(image_specs_df, dataset_pairs)\n",
    "print(\"=========================\")\n",
    "print(\"== Human Observed Data ==\")\n",
    "print(\"==== Subtraction ======\")\n",
    "print(\"=========================\")\n",
    "print(\"RawData : \" + str(RawData.shape))\n",
    "print(\"RawTarget : \" + str(RawTarget.shape))\n",
    "\n",
    "TrainingTarget, TrainingData, ValDataAct, ValData, TestDataAct, TestData = generate_training_validation_testing_ds(\n",
    "                                                                            RawTarget, RawData)\n",
    "\n",
    "execute_logistic_regression(1000, TrainingTarget, TrainingData, ValDataAct, ValData, TestDataAct, TestData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
